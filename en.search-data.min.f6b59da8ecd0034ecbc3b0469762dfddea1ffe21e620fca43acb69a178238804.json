[{"id":0,"href":"/posts/mysql/ch08/","title":"Ch08-MySQL 之 锁","section":"Blog","content":" latch # 是内存结构中的一种轻量级的锁，在 MySQL 数据库中，latch 是用于保护内存中 List Page 完整性的锁结构，latch 可以分为有 mutex、SX rw-lock（spin lock），SX rw-lock 是 MySQL 5.7 的新特性，针对 Page 粒度加的内存锁，有助于提升索引访问效率（针对索引更新的模式）。\n参考文献 # MySQL 之事务隔离级别和锁简介 MySQL 之锁详解（一）：MDL（元数据锁）、latch（闩锁） MySQL 之锁详解（二）：InnoDB 的 S 锁、X 锁、IS 锁和 IX 锁 MySQL 之锁详解（三）：InnoDB 的 Record 锁、Gap 锁和 Next-Key 锁 "},{"id":1,"href":"/posts/leveldb/ch09/","title":"Ch09-LevelDB 之 Open","section":"Blog","content":"Open 流程\n1. 接口定义 # // include/leveldb/db.h class LEVELDB_EXPORT DB { static Status Open(const Options\u0026amp; options, const std::string\u0026amp; name, DB** dbptr); } // db/db_impl.h Status DB::Open(const Options\u0026amp; options, const std::string\u0026amp; dbname, DB** dbptr) {} 2. Env 初始化 # 这里会根据操作系统选择初始化哪种 Env，不过应该注意点的是，代码中并没有这样的判断逻辑，这里的选择逻辑实际上实在 CMakeLists.txt 中完成的。\nif (WIN32) target_sources(leveldb PRIVATE \u0026#34;util/env_windows.cc\u0026#34; \u0026#34;util/windows_logger.h\u0026#34;) else (WIN32) target_sources(leveldb PRIVATE \u0026#34;util/env_posix.cc\u0026#34; \u0026#34;util/posix_logger.h\u0026#34; ) endif (WIN32) 3. DB 初始化 # 初始化 DB 的各种属性，比如 Comparator，Memtable, ImMemtable 等； 加数据库锁，锁住整个数据库。 4. DB Recover # 如果数据库目录不存在，创建目录； 加文件锁，锁住整个数据库，因为 LevelDB 是单进程多线程的，必须保证只有一个进程打开数据库。 读取 MANIFEST 文件，恢复系统关闭时的元数据，也就是版本信息，或者新建 MAINFEST 文件； 根据版本信息，搜索数据库目录，找到关闭时没有写入到 SSTable 的日志，按日志写入顺序逐个恢复日志数据。DBImpl::RecoverLogFile 会创建一个 MemTable，开始读取日志信息，将日志的数据插入到 MemTable，并根据需要调用 DBImpl::WriteLevel0Table 将 MemTable 写入到 SSTable 中 创建数据库相关的内存数据结构，如 Version、VersionSet 等。 5. DB 初始化结束 # 释放数据库锁 返回指向新建的 DB 的指针（这里是通过参数的形式返回的） "},{"id":2,"href":"/posts/leveldb/ch08/","title":"Ch08-LevelDB 之 编码","section":"Blog","content":"Endian-neutral encoding: * Fixed-length numbers are encoded with least-significant byte first * In addition we support variable length \u0026#34;varint\u0026#34; encoding * Strings are encoded prefixed by their length in varint format -- util/coding.h 1. 定长整数 # 1.1 定义 # // 接口 void PutFixed32(std::string* dst, uint32_t value); void PutFixed64(std::string* dst, uint64_t value); // 编码 inline void EncodeFixed32(char* dst, uint32_t value); inline uint32_t DecodeFixed32(const char* ptr); inline void EncodeFixed64(char* dst, uint64_t value); inline uint64_t DecodeFixed64(const char* ptr) 1.2 说明 # 将整数的 0-7 位放到第一个地址，将 8-15 放到第二个地址，其他的依次类推。整个字节序按照小端序排列。\n举个例子，uint64 类型的数字 1026 转成 二进制是 10000000010，其中高字节是 100，低字节是 00000010，然后将 00000010 存到 dst 指向的第一个地址，100 指向 dst 指向的第二个地址。剩下的 6 字节依次类推（若没有数字则补零）。\n2. 变长整数 # 2.1 定义 # // 接口 void PutVarint32(std::string* dst, uint32_t value); bool GetVarint32(Slice* input, uint32_t* value); void PutVarint64(std::string* dst, uint64_t value); bool GetVarint64(Slice* input, uint64_t* value); // 编码 char* EncodeVarint32(char* dst, uint32_t v); char* EncodeVarint64(char* dst, uint64_t v); 2.2 说明 # 跟定长整数编码方式类似，将每个数字按照字节拆分，然后存储到一段连续的地址中。不同的地方是每个地址留出最高位作为标记位，如果标记为 1 或者 0，剩余的比特位用于存数字。 暂时无法在飞书文档外展示此内容\n比如 uint64 类型的数字 1026 转成二进制是 10000000010，低字节的 7 位存储到低地址中，低字节的第 8 位置 1，高字节的 7 位存储到高地址中，高字节的第 8 位置 1。\n3. 字符串 # 3.1 定义 # // 接口 void PutLengthPrefixedSlice(std::string* dst, const Slice\u0026amp; value); bool GetLengthPrefixedSlice(Slice* input, Slice* result); // 编码 3.2 说明 # 将字符串长度先保留下来，然后将字符串追加在该长度后面。这里的字符串长度采用上面的变长整数（PutVarint32）编码方式编码。\n"},{"id":3,"href":"/posts/leveldb/ch07/","title":"Ch07-LevelDB 之 WAL","section":"Blog","content":"WAL 一般用于故障恢复，其内容就是内存里 MemTable 内容的持久化，当一个 MemTable 写满后，开启一个新的 MemTable 时，也同时会开启一个新的 WAL，当 MemTable 被 Dump 到磁盘后，相应的 WAL 可以被删除。所以说控制每次 WAL 写入磁盘的方式，便可以控制最多可能丢失的数据量。\n1. 数据结构 # WAL 由若干个 LogBlock 组成，而 LogBlock 又由多个 LogRecord 组成。LogRecord 有 7 Byte 的固定开头，其中 4 Byte 是后面所有部分的 checksum，2 Byte 是数据的长度，1 Byte 该 LogRecord 的类型，剩余的 data 无固定大小。 （不过应该注意，在 LevelDB 并没有 LogBlock 和 LogRecord 这样的数据结构定义。\n2. 接口定义 # // log_writer.h class Writer { public: Status AddRecord(const Slice\u0026amp; slice); private: Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length); } // log_reader.h class Reader { public: bool ReadRecord(Slice* record, std::string* scratch); } } 3. RecordType 说明 # LogRecord 中的 type 是为了解决写入数据时，LogBlock 里的空间不足以容纳数据的情况。\n// db/log_format.h enum RecordType { kZeroType = 0, kFullType = 1, kFirstType = 2, kMiddleType = 3, kLastType = 4 }; 举个例子，假如写入下述长度的数据。\nA: length 1000 B: length 97270 C: length 8000 当前 LogBlock 里的空间足以容纳写入的数据，type 为 kFullType，表示当前 LogRecord 里包含所有的数据； 当前的 LogBlock 里的空间不足以容纳写入的数据时，将写入的数据拆分，用前面部分将当前 LogBlock 填满，这时候 type 就是 KFirstType，表示当前的 LogRecord 是数据的第一个部分； 接下来开始一个新的 LogBlock，如果这个 LogBlock 依然不能容纳所有的数据，这时候 type 就是 kMiddleType，表示这个 LogRecord 保存了中间部分的数据，后面还有数据； 当剩余的数据可以容纳到新的 LogBlock 时，这时候 type 就是 kLastType，表示这个记录的数据结束了，可以和前面的数据组合起来； kZeroType 是为了兼容 mmap 相关的代码，这种方式会先将数据分配好，置 0，所以当读取日志的文件读取这些 0 时，就可以跳过这些数据，我们不会写入这种类型的日志记录。 4. 参考文献 # [LevelDB] 存储3：有备无患——WAL\n"},{"id":4,"href":"/posts/leveldb/ch05/","title":"Ch05-LevelDB 之 SSTable","section":"Blog","content":"SSTable\nSSTable 不是一个内存数据结构，准确来说按照指定格式对数据组织后存储的文件。在 doc/table_format.md 中可以看到部分格式说明。 SSTable 由若干种不同的 Block 组成。\n1. 基本结构 # SSTable 由若干个 Block 组成，而 Block 则由若干个 Entry 组成，Entry 则是对 KV 的编码形成的一串字符串，如上图所示。\nBlock 分类 说明 Data Block 由 BlockBuilder 完成构建，用于保存实际的 KV 数据。 Meta Block 包含 Filter 和 Stats。Filter 由 FilterBlockBuilder 完成构建， MetaIndex Block 由 BlockBuilder 完成构建，保存了所有 Key 组成的 Bloom Filter。 Index Block 由 BlockBuilder 完成构建，用于指向对应的 Data Block。 2. 生成流程 # // table/table_builder.cc Status TableBuilder::Finish() { // 1. data_block Flush(); // WriteBlock(\u0026amp;r-\u0026gt;data_block, \u0026amp;r-\u0026gt;pending_handle); // 2. meta_blcok | filter WriteRawBlock(r-\u0026gt;filter_block-\u0026gt;Finish(), kNoCompression, \u0026amp;filter_block_handle); // 3. meta_index_block WriteBlock(\u0026amp;meta_index_block, \u0026amp;metaindex_block_handle); // 4. index_block WriteBlock(\u0026amp;r-\u0026gt;index_block, \u0026amp;index_block_handle); // 5. footer r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Append(footer_encoding); } 在调用 TableBuilder::Finish()的时候，会按照 SSTable 定义的结构，将数据依次写入到指定的文件中，这样就完成了 SSTable 的生成。\n"},{"id":5,"href":"/posts/leveldb/ch04/","title":"Ch04-LevelDB 之 MemTable","section":"Blog","content":"MemTable\n1. Immutable VS Mutable # 本质上都是 MemTable，只是其定义的变量扮演的角色不同罢了，有点类似于 double buffer 的两个 buffer 之间的关系。\nnamespace leveldb { class DBImpl : public DB { public: private: MemTable* mem_; MemTable* imm_ GUARDED_BY(mutex_); } } 2. 数据结构定义 # // db/memtable.h namespace leveldb { class MemTable { public: void Add(SequenceNumber seq, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value); bool Get(const LookupKey\u0026amp; key, std::string* value, Status* s); private: KeyComparator comparator_; typedef SkipList\u0026lt;const char*, KeyComparator\u0026gt; Table; Table table_; }; } // db/skiplist.h namespace leveldb { template \u0026lt;typename Key, class Comparator\u0026gt; class SkipList { public: void Insert(const Key\u0026amp; key); bool Contains(const Key\u0026amp; key) const; class Iterator { void Next(); void Prev(); void Seek(const Key\u0026amp; target); } private: Comparator const compare_; Node* const head_; }; template \u0026lt;typename Key, class Comparator\u0026gt; struct SkipList\u0026lt;Key, Comparator\u0026gt;::Node { Key const key; Node* Next(int n)； private: std::atomic\u0026lt;Node*\u0026gt; next_[1]; } 3. 原理说明 # 本质上是对 SkipList 的再封装，所以它的 Add(...) 和 Get(...) 操作本质上就是对 SkipList 的操作。SkipList 的原理，这里不去详细说明，网上有很多，自行搜索便是。\n4. 内存分配 # 借由 Arena 完成，且不会独自回收内存。\n5. 构造说明 # 插入数据的时候，会首先将 key 和 value 编码成下述所示的字符串，再构造成 SkipList::Node 插入到 SkipList 中。写操作的核心部分由 SkipList::Insert(...) 完成。\n读取数据的时候，Node 的 key 构造规则会略有不同，仅仅会构造前半部分，如下所示。读取操作的核心部分由 SkipList::Iterator::Seek(\u0026hellip;) 完成。\n"},{"id":6,"href":"/posts/leveldb/ch03/","title":"Ch03-LevelDB 之 Arena 内存管理","section":"Blog","content":"Arena 内存管理\n1. 内存管理 # 整个内存空间实际上借由 std::vector\u0026lt;char*\u0026gt; blocks 进行管理，其中每个元素都保存了一个指向堆内存（默认 4KB）的指针。如果剩余的内存不足的时候，那么重新分配一段新的空间，并将指向该空间的指针存入到 blocks 这个 vector 中。\n2. 分配策略 # 当 block 有足够剩余的内存时，直接用剩余的内存分配 当 block 没有足够的剩余内存时： 当需求内存大于 1KB 时 4096/4，则单独分配需要的内存 当需求内存小于 1KB 时，重新申请一块默认大小的内存块，再分配需要的内存 3. 释放策略 # Arena 不支持释放部分内存，只能一次性释放所有分配的内存。\n4. 提供的接口 # 接口 说明 char* Allocate(size_t bytes); char* AllocateAligned(size_t bytes); 内存对齐 "},{"id":7,"href":"/posts/leveldb/ch02/","title":"Ch02-LevelDB 架构","section":"Blog","content":"LevelDB 架构\n1. 原理架构 # 组件名称 说明 MemTable SSTable 全称 Sorted String Table，按照 key 值将数据顺序存储在磁盘上。 WAL 保证可靠写入以及异常恢复。 MANIFEST 保存各种元数据，比如当前数据有哪些 SSTable，这些 SSTable 属于哪一层，每一个 SSTable 的键范围和文件大小等信息。由 CURRENT 来指向目前使用的是哪个 MANIFEST 文件。 TableCache 用于缓存 SSTable 的文件描述符、索引和 filter BlockCache 用于缓存 SSTable Block 的数据 2. 常见功能 # 功能名称 配置项 说明 Comparator options.comparator 比较 Key 的大小 Sync options.sync 同步写入 SSTable Snapshot options.snapshot 压缩 options.compression 缓存 options.block_cache 默认使用 LRU 算法 布隆过滤器 options.filter_policy 数据校验 options.paranoid_checks 3. 代码组织 # . ├── db │ ├── ··· │ ├── db_impl.h │ └── db_impl.cc // db 相关接口实现 ├── table // table 相关操作 │ ├── ··· │ ├── table_builder.cc │ └── table.cc ├── include // LevelDB 相关接口接口 │ └── leveldb │ ├── ··· │ ├── c.h │ ├── comparator.h │ └── db.h ├── port // OS 相关接口 │ ├── port.h │ └── port_stdcxx.h ├── util // 工具库 │ ├── bloom.cc │ ├── arena.cc │ └── arena.h ├── helpers │ └── memenv ├── doc ├── issues ├── benchmarks ├── cmake └── third_pary 4. 文件组织 # . ├── 000016.ldb ├── 000018.ldb // SSTABLE 文件 ├── 000019.log // WAL 文件 ├── LOCK // 进程锁文件，防止其他进程使用 ├── LOG // 日志文件 ├── LOG.old ├── CURRENT // 保存当前正在使用的 MANIFEST 文件名称 └── MANIFEST-000017 // 保存元数据信息 5. 参考文献 # LevelDB 完全解析（0）：基本原理和整体架构 "},{"id":8,"href":"/posts/leveldb/ch01/","title":"Ch01-LevelDB 初识","section":"Blog","content":"leveldb是一个key/value型的单机存储引擎，由google开发，并宣布在BSD许可下开放源代码。它是leveling+分区实现的LSM典型代表。\n1. 特性 # Key、Value 支持任意的 byte 类型数组，不单单支持字符串 LevelDB 是一个持久化存储的 KV 系统，将大部分数据存储到磁盘上 按照记录 key 值顺序存储数据，并且 LevelDB 支持按照用户定义的比较函数进行排序 操作接口简单，包括写/读记录以及删除记录，也支持针对多条操作的原子批量操作。 支持数据快照 (snapshot) 功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。 支持数据压缩 (snappy 压缩) 操作，有效减小存储空间、并增快 IO 效率。 LSM 典型实现，适合写多读少。 2. 基本信息 # 条目 说明 项目地址 https://github.com/google/leveldb 最新版本 1.23 "},{"id":9,"href":"/posts/mysql/ch06/","title":"Ch06-MySQL 之 复制技术","section":"Blog","content":"我们习惯把 MySQL 复制中的角色叫做 Master/Slave，MySQL 8.0 后，术语和命令上都会统一到 Source/Replica。MySQL 采用 log shipping 的复制技术，在 source 节点上生成 binlog，通过 dump 线程把 binlog 推到 replica 节点上，replica 节点首先把 binlog 存储到 relay log 中，然后通过 MTS(Multi-Threaded Slave) 技术 replay 到 replica 节点上的表空间。\n复制主要的作用体现为：提升可用性；提升读扩展能力；提升数据持久性保证；提升事务提交性能。\n在 MySQL 的演进迭代过程中，有三种复制模式和五种 MTS 技术。实际应用中，一般是一种复制模式和一种或者多种 MTS 技术组合在一起的。\n1. 五种 MTS 技术 # MTS Remark Single Thread Slave (STS) 在 replica 上从 replay log 回放到 replica 表空间只有一个线程，主要存在于 MySQL 5.6.2 之前。这个时期复制延迟通常很大，因为主/从数据库的并发度差距很大。 Schema based MTS (SB-MTS) MySQL 5.6.3 引入。多个 schema(MySQL 中等同于 database) 之间可以并发的回放 replay log 到 replica 表空间。 Logical Clock based MTS (LC-MTS) MySQL 5.7.2 引入。MySQL 主库上为了提升 I/O 的利用率，采用了 group commit 的技术，简单理解就是把多个小 I/O 聚合成一个大 I/O，去分担存储设备寻道的开销（SSD 没有磁道的概念，但小 I/O 据称的收益依然非常可观）。一批可以 group commit 的事务，读写的数据集是没有冲突的，LC-MTS 正是利用了这个特点，在 repliac 上同一个 group commit 中多个事务可以并行的回放。 Lock Interval based MTS (LI-MTS) MySQL 5.7.6 引入，在 LC-MTS 的基础上进行了进一步的优化，将同一个 group 中补充的事务并行化。推导逻辑是：只要两个事务可以同时获得锁，说明他们读写的资源是不冲突的，因此就可以并行回放。 Writeset based MTS (WS-MTS) MySQL 5.7.22 引入，之前 MTS 技术都依赖 source 节点上运行态的情况实施并发策略，但 WS-MTS 把并发回放的策略细类度到数据本身，两个事务读写的数据集没有交集，就可以并行回放。 2. 三种复制模式 # MTS Remark Single Thread Slave (STS) 在 replica 上从 replay log 回放到 replica 表空间只有一个线程，主要存在于 MySQL 5.6.2 之前。这个时期复制延迟通常很大，因为主/从数据库的并发度差距很大。 Schema based MTS (SB-MTS) MySQL 5.6.3 引入。多个 schema(MySQL 中等同于 database) 之间可以并发的回放 replay log 到 replica 表空间。 Logical Clock based MTS (LC-MTS) MySQL 5.7.2 引入。MySQL 主库上为了提升 I/O 的利用率，采用了 group commit 的技术，简单理解就是把多个小 I/O 聚合成一个大 I/O，去分担存储设备寻道的开销（SSD 没有磁道的概念，但小 I/O 据称的收益依然非常可观）。一批可以 group commit 的事务，读写的数据集是没有冲突的，LC-MTS 正是利用了这个特点，在 repliac 上同一个 group commit 中多个事务可以并行的回放。 Lock Interval based MTS (LI-MTS) MySQL 5.7.6 引入，在 LC-MTS 的基础上进行了进一步的优化，将同一个 group 中补充的事务并行化。推导逻辑是：只要两个事务可以同时获得锁，说明他们读写的资源是不冲突的，因此就可以并行回放。 Writeset based MTS (WS-MTS) MySQL 5.7.22 引入，之前 MTS 技术都依赖 source 节点上运行态的情况实施并发策略，但 WS-MTS 把并发回放的策略细类度到数据本身，两个事务读写的数据集没有交集，就可以并行回放。 3. 参考文献 # MySQL 并行复制技术进化史 MySQL 新特性 MTS "},{"id":10,"href":"/posts/database/ch04/","title":"Ch04-数据库理论 之 执行模型","section":"Blog","content":" 1. 迭代模型/火山模型（Iterator Model） # 又称 Volcano Model 或者 Pipeline Model。该计算模型将关系代数中每一种操作抽象为一个 Operator，将整个 SQL 构建成一个 Operator 树，查询树自顶向下的调用 next() 接口，数据则自底向上的被拉取处理。这种处理方式也称为拉取执行模型 (Pull Based)。\n2. 物化模型（Materialization Model） # 物化模型的处理方式是每个 operator 一次处理所有的输入，处理完之后将所有结果一次性输出。物化模型更适合 OLTP 负载，这些查询每次只访问小规模的数据，只需要少量的函数调用。\n3. 向量化/批处理模型（Vectorized / Batch Model） # 向量化模型 和 火山模型 类似，每个 operator 需要实现一个 next() 函数，但是每次调用 next() 函数会返回一批的元组（tuples），而不是一个元组，所以向量化模型也可称为批处理模型。向量化模型是火山模型和物化模型的折衷。\n向量化模型比较适合 OLAP 查询，因为其大大减少了每个 operator 的调用次数，也就简单减少了虚函数的调用。\n4. 参考文献 # 三种常见的数据库查询引擎执行模型 SQL 查询优化原理与 Volcano Optimizer 介绍 "},{"id":11,"href":"/posts/database/ch03/","title":"Ch03-数据库理论 之 CBO","section":"Blog","content":"Cost-Based Optimization 基于代价的优化器\n直方图 # 等宽直方图 # 等高直方图 # "},{"id":12,"href":"/posts/mysql/ch04/","title":"Ch04-MySQL 之 存储结构","section":"Blog","content":"\nPage # 索引页\n组成 说明 File Header 表示页的一些通用信息，占固定的 38 字节。 Page Header 表示数据页专有的一些信息，占固定的 56 个字节。 Infimum + Supremum 两个虚拟的伪记录，分别表示页中的最小和最大记录，占固定的 26 个字节。 User Records 真实存储我们插入的记录的部分，大小不固定。 Free Space 页中尚未使用的部分，大小不确定。 Page Directory 页中的某些记录相对位置，也就是各个槽在页面中的地址偏移量，大小不固定，插入的记录越多，这个部分占用的空间越多。 File Trailer 用于检验页是否完整的部分，占用固定的 8 个字节。 Row # 格式 说明 compact redundant dynamic 与 compact 行格式挺像，但它不会在记录的真实数据处存储字段真实数据的前 768 个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址 compressed 会采用压缩算法对页面进行压缩，以节省空间 参考文献 # MySQL 之数据页结构 MySQL 之 InnoDB 表空间 MySQL 之 InnoDB 存储结构总结 附件 # "},{"id":13,"href":"/posts/mysql/ch03/","title":"Ch03-MySQL 之 内存结构","section":"Blog","content":"Buffer Pool\n1. Buffer Pool # Buffer Pool 是内存中比较重要的一块区域，主要用来缓存被访问的表和索引数据。Buffer Pool 允许直接从从内存中获取经常使用的数据，在专用的服务器上面，最高有 80% 的物理内存被分配给 Buffer Pool。\n为了提高大容量读取操作的效率，Buffer Pool 被划分成了可以容纳多行的页。为了提高缓存管理的效率，Buffer Pool 采用链接页表的方式实现，对于较少使用的数据，采用 LRU 算法的变体进行淘汰。\nBuffer Pool 不仅仅缓存索引，还会缓存行的数据，自适应哈希索引，插入缓冲 (Insert Buffer)，锁，以及其他内部数据结构，InnoDB 还利用 Buffer Pool 来帮助实现延迟写入（合并多个写入操作，一起顺序写回）。因此选择合适的 Buffer Pool 可以有效提升 MySQL 的效率，当然，过大的 Buffer Pool 也会导致预热和关闭时间的延长 (比如有很多脏页在 Buffer Pool 中)。\n条目 说明 LRU 链表 维护 Clean Page（此 Page 已被使用且未被修改）和 Dirty Page Flush 链表 维护 Dirty Page（此 Page 已被使用且被修改） Free 链表 维护 Free Page（此 Page 未被使用） 对于更多 Buffer Pool 的细节可以参考 Buffer Pool\n2. Change Buffer # Change Buffer 是一种特殊的数据结构，当修改的二级索引页不在 Buffer Pool 中时，那么 Change Buffer 会缓存二级索引页的变更部分（可能是由 INSERT, UPDATE, DELETE 等 DML 语句的操作导致）。当页面因为其他的读操作被载入 Buffer Pool 的时候，这些更改稍后也会被一起合并。\n与 clustered index(InnoDB 的专业术语，指 primary key index) 不同，二级索引通常是非唯一的，而且插入顺序也相对比较随机，比如删除和更新影响的二级索引页可能在索引树上面并不相邻。当影响到的页因为其他操作而被读入 Buffer Pool 时，便会合并更改的缓存。\n当系统处于空闲时间，或者在关机前的时间，清理操作会将更改的索引页写入到磁盘。与每次将修改的值立即写入磁盘相比，清理操作可能会更有效率。对于更多 Change Buffer 的细节可以参考 Change Buffer\n3. Adaptive Hash Index # InnoDB 存储引擎会监控对表上索引的查找，如果观察到建立 Hash Index 可以带来速度的提升，则建立 Hash Index，所以被称为自适应 (Adaptive) 的。Adaptive Hash Index 通过 Buffer Pool 的 B+树构造而来，因此建立的速度非常快，而且也不需要将整张表都建立 Hash Index，InnoDB 存储引擎会自动根据访问的频率和模式来为某些页建立 Hash Index。\n对于更多 Adaptive Hash Index 的细节可以参考 Adaptive Hash Index\n4. Log Buffer # Log Buffer 是内存中的一块区域，该区域主要暂存将被写入到的磁盘的 Log File。一个比较大的 Log Buffer 可以使得比较大的事务在提交之间不需要将 Redo Log 写入到磁盘，因此，如果有 UPDATE, INSERT, DELETE 等大体量的事务操作，可以考虑适当的增加 Log Buffer以节约磁盘I/O。\n对于更多 Log Buffer 的细节可以参考 Log Buffer\n5. 参考文献 # 缓冲池 (buffer pool)，这次彻底懂了！！！ 写缓冲 (change buffer)，这次彻底懂了！！！ 自适应哈希索引 (Adaptive Hash Index, AHI) MySQL 各种“Buffer”之 InnoDB Buffer Pool MySQL 各种“Buffer”之 Change Buffer MySQL 各种“Buffer”之 Adaptive Hash Index MySQL 各种“Buffer”之 Log Buffer MySQL 各种“Buffer”之 Doublewrite Buffer "},{"id":14,"href":"/posts/database/ch02/","title":"Ch02-数据库理论 之 RBO","section":"Blog","content":"Rule-Based Optimization 基于规则的优化器\nRBO 规则 说明 谓词重写 LIKE 规则，BETWEEN-AND 规则，IN 转 OR 规则，IN 转 ANY 规则，OR 转 ANY 规则，ALL/ANY 转聚合，NOT 规则，OR 转 UNION 规则 谓词预处理 1 \u0026lt; a =\u0026gt; a \u0026gt; 1 , 10 != id =\u0026gt; id != 10 谓词下推 t1.id = 1 t1 left join t2 on t1.id = t2.id =\u0026gt; t1 left join t2 on t1.id = t2.id and t1.id = 1 列裁剪优化 常量传递 表达式计算 参考文献 # 数据库查询优化器，RBO 优化规则介绍及示例 "},{"id":15,"href":"/posts/elasticsearch/ch10/","title":"Ch10-Elasticsearch 之 选举","section":"Blog","content":"Elasticsearch 之 选举\n1. 节点选主 # 7.x 之前采用的 Bully 选主算法；7.x 之后采用 Raft 算法。\n2. Shard 选主 # primary shard 的数量在创建索引的时候就固定了，replica shard 的数量可以随时修改。primary shard 宕机后，master 将某个 replica shard 提升为 primary shard。重启宕机 node，master copy replica 到该 node，使用原有的 shard 并同步宕机后的修改。原有的 primary shard 降级为 replica shard。\n3. 参考文献 # ElasticSearch 7.x 之前选主流程 ElasticSearch 7.x 之后选主流程 "},{"id":16,"href":"/posts/elasticsearch/ch09/","title":"Ch09-Elasticsearch 之 threadpool","section":"Blog","content":"Elasticsearch 之线程池。\n1. 实现原理 # 线程池的实现还是比较简单的，本质上都是对 Java 各种线程池的再封装，然后再将这些封装后的线程池放入到一个 Map 中维护起来。如果有任务需要的话，那么再从 Map 中取出/创建指定类型的线程池便可。线程池的初始化是在 Node 刚刚启动的时候，就完成的。在 org.elasticsearch.threadpool.ThreadPool 中有个变量 private final Map\u0026lt;String, ExecutorHolder\u0026gt; executors，而 executors 的构建方式如下所示。\npublic class ThreadPool implements ReportingService\u0026lt;ThreadPoolInfo\u0026gt;, Scheduler { private final Map\u0026lt;String, ExecutorHolder\u0026gt; executors; private final Map\u0026lt;String, ExecutorBuilder\u0026gt; builders; public ThreadPool(final Settings settings, final ExecutorBuilder\u0026lt;?\u0026gt;... customBuilders) { final Map\u0026lt;String, ExecutorBuilder\u0026gt; builders = new HashMap\u0026lt;\u0026gt;(); builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000)); builders.put(Names.FLUSH, new ScalingExecutorBuilder(Names.FLUSH, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); ... this.builders = Collections.unmodifiableMap(builders); final Map\u0026lt;String, ExecutorHolder\u0026gt; executors = new HashMap\u0026lt;\u0026gt;(); for (final Map.Entry\u0026lt;String, ExecutorBuilder\u0026gt; entry : builders.entrySet()) { final ExecutorBuilder.ExecutorSettings executorSettings = entry.getValue().getSettings(settings); // 这里会将 ExecutorBuilder 通过 build 方法构建为 ExecutorHolder final ExecutorHolder executorHolder = entry.getValue().build(executorSettings, threadContext); if (executors.containsKey(executorHolder.info.getName())) { throw new IllegalStateException(\u0026#34;duplicate executors with name [\u0026#34; + executorHolder.info.getName() + \u0026#34;] registered\u0026#34;); } logger.debug(\u0026#34;created thread pool: {}\u0026#34;, entry.getValue().formatInfo(executorHolder.info)); executors.put(entry.getKey(), executorHolder); } this.executors = unmodifiableMap(executors); } } 比如创建一个 fixed 类型的线程池，其流程如下所示。\n2. 分类 # org.elasticsearch.threadpool.ThreadPool 2.1 按照用途 # 名称 说明 index.merge.policy.floor_segment 默认 2MB，小于这个大小的 segment，优先被归并。 same 在调用者线程执行，不转移到新的线程池。 generic 用于通用的操作 (例如，节点发现)，线程池类型为 scaling。 listener 主要用于 Java 客户端线程监听器被设置为 true 时执行动作。线程池类型为 scaling，最大线程数为 min(10, (处理器数量)/2)。 get 用于 get 操作。线程池类型为 fixed, 大小为处理器的数量，队列大小为 1000。 analyze write search 用于count/search/suggest操作。线程池类型为fixed, 大小为 int((处理器数量 3)/2)+1，队列大小为 1000。 snapshot_throttled management 管理工作的线程池，例如，Node info、Node tats、List tasks 等。 flush 用于索引数据的 flush 操作。 refresh 用于 refresh 操作。线程池类型为 scaling, 线程空闲保持存活时间为 5min，最大线程数为 min(10, (处理器数量)/2)。 warme 用于 segment warm-up 操作。线程池类型为 scaling，线程保持存活时间为 5min，最大线程数为 min(5, (处理器数量)/2)。 snapshot 用于 snaphostrestore 操作。线程池类型为 scaling，线程保持存活时间为 5min，最大线程数为 min(5, (处理器数量)/2)。 force_merge 顾名思义，用于 Lucene 分段的 force merge。 fetch_shard_started 用于 TransportNodesAction. fetch_shard_store 用于 TransportNodesListShardStoreMetaData。 system_read system_write 2.2 按照种类来分 # 名称 说明 direct 对用户并不可见，当某个任务不需要在独立的线程执行，又想被线程池管理时，于是诞生了这种特殊类型的线程池：在调用者线程中执行任务 fixed 拥有固定数量的线程来处理请求，当线程空闲时不会销毁，当所有线程都繁忙时，请求被添加到队列中 fixed_auto_queue_size fixed 类型的线程池相似，该线程池的线程数量为固定值，但是队列类型不一样。其队列大小根据利特尔法则 ( Little’s Law) 自动调整大小。 scaling 线程数量是动态的，介于 core 和 max 参数之间变化。线程池的最小线程数为配置的 core 大小，随着请求的增加，当 core 数量的线程全都繁忙时，线程数逐渐增大到 max 数量。max 是线程池可拥有的线程数。上限。当线程空闲时，线程数从 max 大小逐渐降低到 core 大小。 "},{"id":17,"href":"/posts/elasticsearch/ch08/","title":"Ch08-Elasticsearch 之 Http","section":"Blog","content":"Elasticsearch 之 Http\n以最基本的 curl -X GET http://localhost:9200 请求为例，如下图所示。\n1. RestHandler 路由 # RestController 这个类里面维护了一个 handlers，在 Node 启动的时候，会将所有 ES 对外暴露的 URL 注册到这个 handlers 里面，组织形式有点类似于 URL 的分层结构（多叉树）。\nPathTrie\u0026lt;MethodHandlers\u0026gt; handlers = new PathTrie\u0026lt;\u0026gt;(RestUtils.REST_DECODER) 在 RestController 中通过 getAllHandlers（实际上就是遍历前面生成的多叉树）获取到实际的 MethodHandlers，对于/的访问可以对应的获取到 RestMainAction 处理逻辑，简单来说，这里仅仅处理 HTTP 请求，不会对实际的业务逻辑处理。\npublic class RestMainAction extends BaseRestHandler { @Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { return channel -\u0026gt; client.execute(MainAction.INSTANCE, new MainRequest(), new RestBuilderListener\u0026lt;MainResponse\u0026gt;(channel) { @Override public RestResponse buildResponse(MainResponse mainResponse, XContentBuilder builder) throws Exception { return convertMainResponse(mainResponse, request, builder); } }); } } 2. TransportAction 路由 # 上图中的 actions 在 Node 启动的时候就完成了初始化。所以当 RestMainAction 的 client 使用 MainAction.INSTANCE 的时候，就可以正确的映射到对应的 TransportAction，即 TransportMainAction。\npublic class TransportMainAction extends HandledTransportAction\u0026lt;MainRequest, MainResponse\u0026gt; { protected void doExecute(Task task, MainRequest request, ActionListener\u0026lt;MainResponse\u0026gt; listener) { ClusterState clusterState = clusterService.state(); listener.onResponse( new MainResponse(nodeName, Version.CURRENT, clusterState.getClusterName(), clusterState.metadata().clusterUUID(), Build.CURRENT)); } } TransportAction 中定义了具体的处理逻辑，借由 doExecute 方法实现。比如在 TransportMainAction##doExecute 就定义了集群的基本信息的获取方式和返回的内容。\n"},{"id":18,"href":"/posts/elasticsearch/ch06/","title":"Ch06-Elasticsearch 之 Update","section":"Blog","content":"更新操作也是写操作。Elasticsearch 在创建新文档时，Elasticsearch 将为该文档分配一个版本号。对文档的每次更改都会产生一个新的版本号。当执行更新时，旧版本在.del 文件中被标记为已删除，并且新版本在新的分段中编入索引。旧版本可能仍然与搜索查询匹配，但是从结果中将其过滤掉。\n1. Update 流程 # 收到 Update 请求后，从 Segment 或者 TransLog 中读取同 id 的完整 Doc，记录版本号为 V1。 将版本 V1 的全量 Doc 和请求中的部分字段 Doc 合并为一个完整的 Doc，同时更新内存中的 VersionMap。获取到完整 Doc 后，Update 请求就变成了 Index 请求。 加锁。 再次从 versionMap 中读取该 id 的最大版本号 V2，如果 versionMap 中没有，则从 Segment 或者 TransLog 中读取，这里基本都会从 versionMap 中获取到。 检查版本是否冲突 (V1==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的 Add 请求。 在 Index Doc 阶段，首先将 Version + 1 得到 V3，再将 Doc 加入到 Lucene 中去，Lucene 中会先删同 id 下的已存在 doc id，然后再增加新 Doc。写入 Lucene 成功后，将当前 V3 更新到 versionMap 中。 释放锁，部分更新的流程就结束了。 "},{"id":19,"href":"/posts/elasticsearch/ch07/","title":"Ch07-Elasticsearch 之 Segment Merge","section":"Blog","content":"在 Elasticsearch 中，为了让插入的让数据更快的被检索使用。用一句话来概括就是”开新文件”。但是从另一个方面看，开新文件也会给服务器带来负载压力。因为默认每 1 秒，都会有一个新文件产生，每个文件都需要有文件句柄，内存，CPU 使用等各种资源。一天有 86400 秒，设想一下，每次请求要扫描一遍 86400 个文件，这个响应性能绝对好不了。为了解决这个问题，Elasticsearch 引入了 Merge 操作。\n1. segment merge 的流程 # segment merge 的流程还是比较容易理解的，简单来说就是每个小 segment 的数据 copy 到一个比较大的 segment 里面，然后删除掉之前比较小的 segment，并且重新修改 commit point。\nElasticsearch 会不断在后台运行任务，主动将前面 flush 到磁盘的零散的 segment 做数据归并，尽量让索引内只保有少量的，每个都比较大的 segment 文件。这个过程是由独立的线程来进行的，并不影响新 segment 的产生。归并过程中，尚未完成的较大的 segment 是被排除在检索可见范围之外的。此时，索引状态如下图所示。\n当归并完成，较大的这个 segment 刷到磁盘后，commit 文件做出相应变更，删除之前几个小 segment，改成新的大 segment。等检索请求都从小 segment 转到大 segment 上以后，删除没用的小 segment。这时候，索引里 segment 数量就下降了。此时，索引状态如下图所示。\n2. segment merge 配置 # segment 归并的过程，需要先读取 segment，归并计算，再写一遍 segment，最后还要保证刷到磁盘。可以说，这是一个非常消耗磁盘 IO 和 CPU 的任务。所以，ES 提供了对归并线程的限速机制，确保这个任务不会过分影响到其他任务。 在 5.0 之前，归并线程的限速配置 indices.store.throttle.max_bytes_per_sec 是 20MB。5.0 开始，ES 对此作了大幅度改进，使用了 Lucene 的 CMS(ConcurrentMergeScheduler) 的 auto throttle 机制，正常情况下已经不再需要手动配置 indices.store.throttle.max_bytes_per_sec 了。 归并线程的数目，ES 也是有所控制的。默认数目的计算公式是：Math.min(3, Runtime.getRuntime().availableProcessors() / 2)。即服务器 CPU 核数的一半大于 3 时，启动 3 个归并线程；否则启动跟 CPU 核数的一半相等的线程数。相信一般做 Elastic Stack 的服务器 CPU 核数都会在 6 个以上。所以一般来说就是 3 个归并线程。如果你确定自己磁盘性能跟不上，可以降低 index.merge.scheduler.max_thread_count 配置，免得 IO 情况更加恶化。\n3. segment merge 策略 # segment merge 线程是按照一定的运行策略来挑选 segment 进行归并的。主要有以下几条：\n名称 说明 index.merge.policy.floor_segment 默认 2MB，小于这个大小的 segment，优先被归并。 index.merge.policy.max_merge_at_once 默认一次最多归并 10 个 segment index.merge.policy.max_merge_at_once_explicit 默认 forcemerge 时一次最多归并 30 个 segment。 index.merge.policy.max_merged_segment 默认 5 GB，大于这个大小的 segment，不用参与归并。forcemerge 除外。 根据这段策略，其实我们也可以从另一个角度考虑如何减少 segment 归并的消耗以及提高响应的办法——加大 flush 间隔，尽量让每次新生成的 segment 本身大小就比较大。\n4. forcemerge # 既然默认的最大 segment 大小是 5GB。那么一个比较庞大的数据索引，就必然会有为数不少的 segment 永远存在，这对文件句柄，内存等资源都是极大的浪费。但是由于归并任务太消耗资源，所以一般不太选择加大 index.merge.policy.max_merged_segment 配置，而是在负载较低的时间段，通过 forcemerge 接口，强制归并 segment。\ncurl -XPOST http://127.0.0.1:9200/logstash-2015-06.10/_forcemerge?max_num_segments=1 由于 forcemerge 线程对资源的消耗比普通的归并线程大得多，所以，绝对不建议对还在写入数据的热索引执行这个操作。这个问题对于 Elastic Stack 来说非常好办，一般索引都是按天分割的。\n"},{"id":20,"href":"/posts/elasticsearch/ch05/","title":"Ch05-Elasticsearch 之 Delete","section":"Blog","content":"删除操作是也是写操作。\n磁盘上的每个分段 (segment) 都有一个.del 文件与它相关联。当发送删除请求时，该文档未被真正删除，而是在.del 文件中标记为已删除。此文档可能仍然能被搜索到，但会从结果中过滤掉。当执行 segment merge 操作的时候，在.del 文件中标记为已删除的文档不会被包括在新的合并段中。\n"},{"id":21,"href":"/posts/elasticsearch/ch04/","title":"Ch04-Elasticsearch 之 Search","section":"Blog","content":"Elasticsearch 的 Search 操作包含两个阶段，一个是 QueryPhase，另一个是 FetchPhase。QueryPhase 在初始查询阶段，查询会广播到索引中每一个分片副本 (主分片或副分片)。每个分片在本地执行搜索并构建一个匹配文档的优先队列（优先队列是一个存有 topN 匹配文档的有序列表，即 doc id。优先队列大小为分页参数 from + size），注意该队列中并没有取具体的数据。FetchPhase 会根据获取到的 doc id 向对应的节点依次发送 GET 请求，获取结果，然后合并，聚合，排序最终返回结果。\n1. Search 流程 # Elasticsearch 目前有两种搜索类型 DFS_QUERY_THEN_FETCH 和 QUERY_THEN_FETCH。两种不同的搜索类型的区别在于查询阶段，DFS 查询阶段的流程要多一些，它使用全局信息来获取更准确的评分。\n1.1 查询阶段 (Query Phase) # 在此阶段，协调节点将搜索请求路由到索引 (index) 中的所有分片 (shards)（包括：主要或副本）。分片独立执行搜索，并根据相关性分数创建一个优先级排序结果。所有分片将匹配的文档和相关分数的文档 ID 返回给协调节点。协调节点创建一个新的优先级队列，并对全局结果进行排序。可以有很多文档匹配结果，但默认情况下，每个分片将前 10 个结果发送到协调节点，协调创建优先级队列，从所有分片中分选结果并返回前 10 个匹配。\n注意：\n这里只是返回了匹配文档的 ID 和 score，没有返回完整的 doc\n1.2 获取阶段 (Fetch Phase) # 在协调节点对所有结果进行排序，已生成全局排序的文档列表后，它将从所有分片请求原始文档。所有的分片都会丰富文档并将其返回到协调节点。\n注意：\n这里会重新请求原始 doc，并返回完整的 doc\n2. 概念介绍 # 搜索相关性 (Search Relevance)\n相关性由 Elasticsearch 给予搜索结果中返回的每个文档的分数确定。用于评分的默认算法为 tf/idf（术语频率/逆文档频率）。该术语频率测量术语出现在文档中的次数（更高频率=更高的相关性），逆文档频率测量术语在整个索引中出现的频率占索引中文档总数的百分比（更高的频率=较少的相关性）。最终得分是 tf/idf 分数与其他因素（如词语邻近度（短语查询）），术语相似度（用于模糊查询）等的组合。\n"},{"id":22,"href":"/posts/elasticsearch/ch03/","title":"Ch03-Elasticsearch 之 Put","section":"Blog","content":"Elasticsearch 之 Put，不过与其说是 Elasticsearch 的操作流程，倒不如说是单个 Lucene 索引操作流程。\n1. Put 流程 # 当我们发送索引一个新文档的请求到协调节点后，将发生如下一组操作。\nElasticsearch 集群中的每个节点都包含了该节点上分片的元数据信息。协调节点 (默认) 使用文档 ID 参与计算，以便为路由提供合适的分片。Elasticsearch 使用 MurMurHash3 函数对文档 ID 进行哈希，其结果再对分片数量取模，得到的结果即是索引文档的分片。 shard = hash(document_id) % (num_of_primary_shards) 当分片所在的节点接收到来自协调节点的请求后，会将该请求写入 translog，并将文档加入内存缓冲。如果请求在主分片上成功处理，该请求会并行发送到该分片的副本上。当 translog 被同步到全部的主分片及其副本上后，客户端才会收到确认通知。 Memory buffer 中的内容会被周期性 (默认是 1 秒) 写入到 filesystem cache 上的新 segment 上 (该过程叫做 refresh)。虽然这个 segment 并没有被同步写入磁盘 (fsync)，但它是开放的，内容可以被搜索到。 每 30 分钟，或者当 translog 很大的时候，translog 会被清空，文件系统缓存会被同步。这个过程在 Elasticsearch 中称为 flush。在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新 segment。segment 的 fsync 将创建一个新的提交点，并将内容刷新到磁盘。旧的 translog 将被删除并开始一个新的 translog。 注意：\nCommit to disk with tranlog flush 应该是 30 分钟 fsync 一次\n由于取余这个计算，完全依赖于分母，所以导致 ES 索引有一个限制，索引的主分片数，不可以随意修改。因为一旦主分片数不一样，所以数据的存储位置计算结果都会发生改变，索引数据就完全不可读了。\n2. 概念说明 # 名词 说明 fsync 涉及到 fsync 操作有两种。内存 buffer 中的内容会被写到 filesystem cache 的 segment 上面，然后这个 segment 会被 fsync 写入到磁盘；translog 会每隔 5 秒或者在一个变更请求完成之后执行一次 fsync 操作，将 translog 从缓存刷入磁盘。 refresh es 接收数据请求时先存入内存中，默认每隔一秒会从内存 buffer 中将数据写入 filesystem cache，这个过程叫做 refresh flush es 默认每隔一段时间会将 filesystem cache 中的数据刷入磁盘同时清空 translog 日志文件，这个过程叫做 flush。对于 flush 操作，Elasticsearch 默认设置为：每 30 分钟主动进行一次 flush，或者当 translog 文件大小大于 512MB (老版本是 200MB) 时，主动进行一次 flush。 translog 保证在 filesystem cache 中的数据不会因为 elasticsearch 重启或是发生意外故障的时候丢失。当系统重启时会从 translog 中恢复之前记录的操作。当对 elasticsearch 进行 CRUD 操作的时候，会先到 translog 之中进行查找，因为 tranlog 之中保存的是最新的数据。translog 的清除时间时进行 flush 操作之后（将数据从 filesystem cache 刷入 disk 之中）。 3. 参考文献 # 图解 Elestricsearch 写入流程 《Elasticsearch 源码解析与优化实战》第 7 章：写流程 "},{"id":23,"href":"/posts/elasticsearch/ch02/","title":"Ch02-Elasticsearch 之 Shard","section":"Blog","content":"Shard 即分片，它是 ES 分布式存储的基石，是底层的基本读写单元。ES 集群的核心就是对所有分片的分布、索引、负载、路由等进行各种操作。\n分片分为主分片和副本分片，一般情况一个主分片有多个副本分片。主分片负责处理写入请求和存储数据，副本分片只负责存储数据，是主分片的拷贝，文档会存储在具体的某个主分片和副本分片上。\n1. Shard 分配控制 # 某个 shard 分配在哪个节点上，一般来说，是由 Elasticsearch 自动决定的。通常新索引生成，索引的删除，新增副本分片，节点增减引发的数据均衡等操作会触发 Shard 的分配操作，也可以通过 cluster.routing.allocation.* 等参数进行精细控制。此外还提供了部分手动参数，用户可以通过其对 Shard 进行设置，allocate_replica，allocate_stale_primary，allocate_empty_primary，move,cancel。\n2. Shard 路由计算 # 作为一个没有额外依赖的简单的分布式方案，ES 在这个问题上同样选择了一个非常简洁的处理方式，对任一条数据计算其对应分片的方式如下：\nshard = hash(routing) % number_of_primary_shards 每个数据都有一个 routing 参数，默认情况下，就使用其 _id 值。将其 _id 值计算哈希后，对索引的主分片数取余，就是数据实际应该存储到的分片 ID。 由于取余这个计算，完全依赖于分母，所以导致 ES 索引有一个限制，索引的主分片数，不可以随意修改。因为一旦主分片数不一样，所以数据的存储位置计算结果都会发生改变，索引数据就完全不可读了。\n3. Shard 设置 # PUT indexName { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5 } } 4. Shard 查询 # 参数 说明 randomizeacross shards 随机选择分片查询数据，es 的默认方式 _local 优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有 IO 问题但有可能造成负载不均问题。数据量是完整的。 _primary 只在主分片中查询不去副本查，一般数据完整。 _primary_first 优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。 _only_node 只在指定 id 的节点中的分片中查询，数据可能不完整。 _prefer_node 优先在指定你给节点中查询，一般数据完整。 _shards 在指定分片中查询，数据可能不完整。 _only_nodes 可以自定义去指定的多个节点查询，es 不提供此方式需要改 5. Replica # 在有副本配置的情况下，数据从发向 ES 节点，到接到 ES 节点响应返回，流向如下图所示。\n客户端请求发送给 Node 1 节点，注意图中 Node 1 是 Master 节点，实际完全可以不是。 Node 1 用数据的 _id 取余计算得到应该讲数据存储到 shard 0 上。通过 cluster state 信息发现 shard 0 的主分片已经分配到了 Node 3 上。Node 1 转发请求数据给 Node 3。 Node 3 完成请求数据的索引过程，存入主分片 0。然后并行转发数据给分配有 shard 0 的副本分片的 Node 1 和 Node 2。当收到任一节点汇报副本分片数据写入成功，Node 3 即返回给初始的接收节点 Node 1，宣布数据写入成功。Node 1 返回成功响应给客户端。 "},{"id":24,"href":"/posts/elasticsearch/ch01/","title":"Ch01-Elasticsearch 介绍","section":"Blog","content":"Elasticsearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。\n1. 基本信息 # 条目 说明 官网 https://www.elastic.co 下载地址 https://hadoop.apache.org/releases.html 2. 架构介绍 # ElasticSearch 节点类型分为 Master（主节点）和 DataNode（数据节点）。每个 DataNode 上面包含若干个分片，分片（Shard）分为 Primary Shard（主分片）和 Replica Shard（副分片）。分片（Shard）可以支撑海量数据，解决单机磁盘容量问题，副本（Replica）可以保证数据不丢失。\n2.1 术语介绍 # 类型 说明 Cluster 包含若干个 Node。 Node 是一个 Elasticsearch 的实例，本质上是一个 Java 进程。每个节点上面都保存着集群的状态信息，包括所有的节点信息、所有的索引和相关的 Mapping 于 Setting 信息和分片的路由信息等。节点按照角色可以划分为 MasterNode、DataNode、IngestNode 等。 Shard 是 Lucene 的一个实例，Shard 分为 Primary Shard 和 Replica Shard，一般情况，一个 Primary Shard 有多个 Replica Shard。Primary Shard 负责处理写入请求和存储数据，Replica Shard 只负责存储数据，是 Primary Shard 的拷贝，Index 会存储在具体的某个 Primary Shard 和 Replica Shard 上。 Index 类似于 MySQL 的 Table。 Document 类似于 MySQL 的 Row。 Field 类似于 MySQL 的 Column。 Mapping 类似于 MySQL 的 Schema。 DSL 类似于 MySQL 的 SQL。 2.2 节点介绍 # 类型 配置 说明 MasterNode node.master = true 管理索引（创建/删除/分配）；维护元数据；管理集群节点状态；不负责数据写入和查询。 DataNode node.data = true 数据写入；数据检索。 IngestNode node.ingest = true 执行预处理管道，不负责数据和集群相关事宜。 CoordinatingNode all node 负责接收客户端的请求，将请求路由到到合适的节点，并将结果汇集到一起。事实上所有的节点都是 CoordinatingNode，但是当其他节点类型设置为 false 的时候，该节点才会扮演单纯的 CoordinatingNode。 3. 倒排索引 # 正排索引指的是从文档 id 到文档内容、单词的关联关系。例如每本书的目录，通过目录可以很快找到某个标题的具体内容在书中的那一页。而倒排索引指的文档内容或者单词到文档 id 的关联关系。还是以书的例子，倒排索引指的是从具体内容到文章标题的索引。\n4. 易混概念 # 数据分片技术是指分布式存储系统按照一定的规则将数据存储到对应的存储节点中，或者到对应的存储节点中获取想要的数据。 数据复制技术是指将数据进行备份，使得多个节点都存储该数据，提高系统可用性和可靠性。 5. 参考文献 # ElasticSearch 架构原理入门篇 Elasticsearch 核心技术（四）：分布式存储架构与索引原理分析 "},{"id":25,"href":"/posts/apache-kylin/ch08/","title":"Ch08-Kylin 之 部分细节","section":"Blog","content":"Kylin 部分细节\n1. KylinConfig # KylinConfig 除过承载kylin.properties和kylin-defaults.properties中的所有配置项之外，还保留了一个transient Map\u0026lt;Class, Object\u0026gt; managersCache = new ConcurrentHashMap\u0026lt;\u0026gt;();，这个 CHM 采用懒加载的方式，将 Kylin 所有的 manager 加载进来，这样当某个逻辑需要指定的 Manager 时，就可以直接通过这个 map 获取到（如果获取不到的话，会将对应的 manager 构建起来，然后放到这个 map 里面，同时返回）。\n目前整个 Kylin-4.0.0 中总共有 23 种 manager：BadQueryHistoryManager, Broadcaster, CubeDescManager, CubeManager, CuboidManager, DataModelManager, DictionaryManager, DistributedScheduler, DraftManager, ExecutableDao, ExecutableManager, HybridManager, KafkaConfigManager, KylinUserManager, ProjectManager, RealizationRegistry, SnapshotManager, SourceManager, StreamingManager, StreamingSourceConfigManager, TableACLManager, TableMetadataManager, TempStatementManager。这些 manager 全部通过 KylinConfig 中的 getManager 方法初始化并获取。\npublic \u0026lt;T\u0026gt; T getManager(Class\u0026lt;T\u0026gt; clz) { KylinConfig base = base(); if (base != this) return base.getManager(clz); if (managersCache == null) { managersCache = new ConcurrentHashMap\u0026lt;\u0026gt;(); } Object mgr = managersCache.get(clz); if (mgr != null) return (T) mgr; synchronized (clz) { mgr = managersCache.get(clz); if (mgr != null) return (T) mgr; try { logger.info(\u0026#34;Creating new manager instance of {}\u0026#34;, clz); // new manager via static Manager.newInstance() Method method = clz.getDeclaredMethod(\u0026#34;newInstance\u0026#34;, KylinConfig.class); method.setAccessible(true); // override accessibility mgr = method.invoke(null, this); } catch (Exception e) { throw new RuntimeException(e); } managersCache.put(clz, mgr); } return (T) mgr; } 2. olap_model.json 是每次查询都会生成吗？ # 这个 json 文件出现在每次查询，调用 calcite 的时候，传入 schema 信息时用到。\npublic class QueryConnection { private static Boolean isRegister = false; public static Connection getConnection(String project) throws SQLException { if (!isRegister) { try { Class\u0026lt;?\u0026gt; aClass = Thread.currentThread().getContextClassLoader() .loadClass(\u0026#34;org.apache.calcite.jdbc.Driver\u0026#34;); Driver o = (Driver) aClass.getDeclaredConstructor().newInstance(); DriverManager.registerDriver(o); } catch (ClassNotFoundException | InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) { e.printStackTrace(); } isRegister = true; } File olapTmp = OLAPSchemaFactory.createTempOLAPJson(project, KylinConfig.getInstanceFromEnv()); Properties info = new Properties(); info.putAll(KylinConfig.getInstanceFromEnv().getCalciteExtrasProperties()); // Import calcite props from jdbc client(override the kylin.properties) info.putAll(BackdoorToggles.getJdbcDriverClientCalciteProps()); info.put(\u0026#34;model\u0026#34;, olapTmp.getAbsolutePath()); info.put(\u0026#34;typeSystem\u0026#34;, \u0026#34;org.apache.kylin.query.calcite.KylinRelDataTypeSystem\u0026#34;); return DriverManager.getConnection(\u0026#34;jdbc:calcite:\u0026#34;, info); } } 可以看到 QueryConnection 不是单例的实现，所以 Kylin Web 界面每发起一次查询时，这里都会借助 Calcite 建立一次链接。因此 info 中的model.json的内容在每次查询时都会生成，但是最终写入磁盘的文件只会在第一次查询的时候生成。在OLAPSchemaFactory内部有一个缓存Map\u0026lt;String, File\u0026gt; cachedJsons = Maps.newConcurrentMap()，这个 key 值是 json 的内容，value 是 json 的路径。因此只有这个 map 还存在，那么这个文件是不会再生成的（注意，这里没有检测文件是否存在，所以在 kylin 启动起来后，删掉这个文件，那查询就只能报错了）。\n\u0026#34;caseSensitive\u0026#34; -\u0026gt; \u0026#34;true\u0026#34; \u0026#34;unquotedCasing\u0026#34; -\u0026gt; \u0026#34;TO_UPPER\u0026#34; \u0026#34;model\u0026#34; -\u0026gt; \u0026#34;/home/li/Software/apache-kylin-4.0.0-beta-bin/bin/../tomcat/temp/olap_model_277471611618725080.json\u0026#34; \u0026#34;typeSystem\u0026#34; -\u0026gt; \u0026#34;org.apache.kylin.query.calcite.KylinRelDataTypeSystem\u0026#34; \u0026#34;conformance\u0026#34; -\u0026gt; \u0026#34;LENIENT\u0026#34; \u0026#34;quoting\u0026#34; -\u0026gt; \u0026#34;DOUBLE_QUOTE\u0026#34; { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;defaultSchema\u0026#34;: \u0026#34;DEMO\u0026#34;, \u0026#34;schemas\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;DEMO\u0026#34;, \u0026#34;factory\u0026#34;: \u0026#34;org.apache.kylin.query.schema.OLAPSchemaFactory\u0026#34;, \u0026#34;operand\u0026#34;: { \u0026#34;project\u0026#34;: \u0026#34;project_demo\u0026#34; }, \u0026#34;functions\u0026#34;: [ { name: \u0026#39;MASSIN\u0026#39;, className: \u0026#39;org.apache.kylin.query.udf.MassInUDF\u0026#39; }, { name: \u0026#39;INTERSECT_VALUE\u0026#39;, className: \u0026#39;org.apache.kylin.measure.bitmap.BitmapIntersectValueAggFunc\u0026#39; }, { name: \u0026#39;PERCENTILE\u0026#39;, className: \u0026#39;org.apache.kylin.measure.percentile.PercentileAggFunc\u0026#39; }, { name: \u0026#39;CONCAT\u0026#39;, className: \u0026#39;org.apache.kylin.query.udf.ConcatUDF\u0026#39; }, { name: \u0026#39;INTERSECT_COUNT\u0026#39;, className: \u0026#39;org.apache.kylin.measure.bitmap.BitmapIntersectDistinctCountAggFunc\u0026#39; }, { name: \u0026#39;VERSION\u0026#39;, className: \u0026#39;org.apache.kylin.query.udf.VersionUDF\u0026#39; }, { name: \u0026#39;PERCENTILE_APPROX\u0026#39;, className: \u0026#39;org.apache.kylin.measure.percentile.PercentileAggFunc\u0026#39; } ] } ] } 3. 参考文献 # Apache Kylin 概览 Kylin on Parquet 介绍和快速上手 "},{"id":26,"href":"/posts/apache-kylin/ch07/","title":"Ch07-Kylin 之 DISTINCT COUNT","section":"Blog","content":"distinct count\n1. 去重算法 # 1.1 介绍 # Apache Kylin 使用 RoaringBitmap 算法实现了去重功能，RoaringBitmap 算是 bitmap 的一种改进版，能提供更优秀的压缩性能和更快的查询效率。\n1.2 算法实现 # 将 32 位无符号整数按照高 16 位分桶，即最多可能有 216=65536 个桶，论文内称为 container。存储数据时，按照数据的高 16 位找到 container（找不到就会新建一个），再将低 16 位放入 container 中。也就是说，一个 RBM 就是很多 container 的集合。\n0x00020032（十进制 131122）放入一个 RBM 的过程如上图所示，0x00020032 的前 16 位是 0002，找到对应的桶 0x0002。在桶对应的 Container 中存储低 16 位，因为 Container 元素个数不足 4096，因此是一个 Array Container。低 16 位为 0032（十进制为 50）, 在 Array Container 中二分查找找到相应的位置插入即可（如上图 50 的位置）。\n相较于原始的 Bitmap 需要占用 16K (131122/8/1024) 内存来存储这个数，而这种存储实际只占用了 4B（桶中占 2 B，Container 中占 2 B，不考虑数组的初始容量）。\n2. 近似 distinct_count # 2.1 介绍 # Apache Kylin 使用 HyperLogLog 算法实现了近似 Count Distinct，提供了错误率从 9.75% 到 1.22% 几种精度供选择；算法计算后的 Count Distinct 指标，理论上，结果最大只有 64KB，最低的错误率是 1.22%；这种实现方式用在需要快速计算、节省存储空间，并且能接受错误率的 Count Distinct 指标计算。\n2.2 算法实现 # 该算法的核心是基于伯努利实验的分析结果进行推测。用网上最常见的例子来说明下\n第一次试验：抛掷 1 次出现正面，此时 k=1,n=1; 第二次实验：抛掷 3 次出现正面，此时 k=3,n=2; 第三次实验：抛掷 6 次出现正面，此时 k=6,n=3; 第 n 次试验：抛掷 10 次出现正面，此时 k=10,n=n，通过估算关系计算，n=2^10 上述的公式也可以理解为为了得到得到 k 次正面，一共进行了 2^k 次实验，此时可以简单认为 2^k 就是估计出来的 distinct_count 值。在 HyperLogLog 算法中，将所有的数字放入到 BitMap 中（完成了去重），然后从后向前取值为 0 的数字个数，然后该个数就是上述公式中的 k。\n如上图所示，估算出来的 distinct_count 值为 2^4=16。此时可以发现正确的 distinct_cout 值应该是 8，偏差有点大。为了解决估值偏差较大的问题，可以采用如下方式结合来缩小误差。\n增加测试的轮数，取平均值。假设三次伯努利试验为 1 轮测试，我们取出这一轮试验中最大的的 kmax 作为本轮测试的数据，同时我们将测试的轮数定位 100 轮，这样我们在 100 轮实验中，将会得到 100 个 kmax，此时平均数就是 (k_max_1 + … + k_max_m)/m，这里 m 为试验的轮数，此处为 100. 增加修正因子，修正因子是一个不固定的值，会根据实际情况来进行值的调整。 3. 精确 distinct_count # 3.1 介绍 # 从 1.5.3 版本开始，Kylin 中实现了基于 bitmap 的精确 Count Distinct 计算方式。当数据类型为 tiny int(byte)、small int(short) 以及 int，会直接将数据值映射到 bitmap 中；当数据类型为 long,string 或者其他，则需要将数据值以字符串形式编码成 dict(字典)，再将字典 ID 映射到 bitmap。指标计算后的结果，并不是计数后的值，而是包含了序列化值的 bitmap，这样才能确保在任意维度上的 Count Distinct 结果是正确的。这种实现方式提供了精确的无错误的 distinct_count 结果，但是需要更多的存储资源，如果数据中的不重复值超过百万，结果所占的存储应该会达到几百 MB。\nKylin v3.0.0 引入了第一版的 Hive global dictionary(KYLIN-3841)。这个功能使用 Hive 的分布式 SQL 引擎来构建全局字典。为了进一步提升性能，kylin v3.1.0 引入了第二版的 Hive global dictionary v2(KYLIN-4342), 这个版本在某些步骤使用 MapReduce 代替 HQL 进行全局字典的构建。\nKylin v4.0.0 又基于 spark 实现了另外一种分布式构建全局字典的方式。\n3.2 算法实现 # 可以看到不管是哪个版本 distinct_count 功能都是基于 dict 和 bitmap 实现的，先将 string 的等类型的数据存储到 dict 中形成 \u0026lt;hash(string), offset\u0026gt;，然后将 offset 放入到 bitmap 中完成 distinct，最后遍历 bitmap 中的元素计算出 count。\n这里简单说明下 kylin v4.0.0 中 dict 的构建方式。这个 dict 也称为全局字典，即所有 Server 都可见的一个 dict。\n3.2.1 全局字典 # 一份字典包括一份 meta 数据文件和多个字典文件，每个字典文件称之为桶 (Bucket)。\n第一次构建\nrelative 表示本次本 Bucket 已有的数据，Absolute 表示将要构建出来的数据。\n以 Bucket2#Jack-1 举例，此时已知 Bucket1 中有 2 个元素，那么可以推断出此时 offset 为 2，那么在构建全局字典的时候，将 \u0026lt;Jack,1\u0026gt; 改成 \u0026lt;Jack1, 1 + 2\u0026gt; 便可。其他依次类推。\n第二次构建\nrelative 表示本次本 Bucket 已有的数据，Absolute 表示将要构建出来的数据（但是此时还有上次构建的数据）。\n以 Bucket2#May1-1 举例，此时已知第一次构建完成后中有 6 个元素，而本次 Bucket1 有 2 个元素，那么可以推断出此时 offset 为 8，那么在构建全局字典的时候，将 \u0026lt;May,1\u0026gt; 改成 \u0026lt;May, 1 + 8\u0026gt; 便可。其他依次类推。\n4. 参考文献 # Roaring Bitmap 更好的位图压缩算法 再谈基数估计之 HyperLogLog 算法 高效压缩位图 RoaringBitmap 的原理与应用 Apache Kylin 4.0 精确去重的全局字典原理 Kylin 4.0 全局字典设计和实现 "},{"id":27,"href":"/posts/apache-kylin/ch06/","title":"Ch06-Kylin 之 剪枝优化","section":"Blog","content":"Kylin 剪枝优化\n如图所示，构建一个 4 个维度（A，B，C, D）的 Cube，需要生成 16 个 Cuboid。\n随着维度数目的增加 Cuboid 的数量会爆炸式地增长，不仅占用大量的存储空间还会延长 Cube 的构建时间。为了缓解 Cube 的构建压力，减少生成的 Cuboid 数目，占用存储空间，同时提高查询性能，Apache Kylin 引入了一系列的高级设置，帮助用户筛选出真正需要的 Cuboid。这些高级设置包括聚合组（Aggregation Group）、联合维度（Joint Dimension）、层级维度（Hierachy Dimension）和必要维度（Mandatory Dimension）等。\n1. 聚合组 (Aggregation group) # 用户根据自己关注的维度组合，可以划分出自己关注的组合大类，这些大类在 Apache Kylin 里面被称为聚合组。上面的例子如果用户仅仅关注维度 AB 组合和维度 CD 组合，那么该 Cube 则可以被分化成两个聚合组，分别是聚合组 AB 和聚合组 CD。如图所示，生成的 Cuboid 数目从 16 个缩减成了 8 个。\n2. 必要维度 (mandatory dimension) # 用户有时会对某一个或几个维度特别感兴趣，所有的查询请求中都存在 group by 这个维度，那么这个维度就被称为必要维度，只有包含此维度的 Cuboid 会被生成。假设维度 A 是必要维度，那么生成的 Cube 如图所示，维度数目从 16 变为 9。即必须同时都出现。\n3. 层级维度 (hierarchy dimension) # 基于 ABCD 四个维度的场景，假设维度 A 代表国家，维度 B 代表省份，维度 C 代表城市，那么 ABC 三个维度可以被设置为层级维度，生成的 Cube 如图所示，Cuboid 数目从 16 减小到 8。即可同时出现，也可不同时出现，若出现那么出现的顺序必须跟层级维度的顺序保持一直。\n4. 联合维度 (Joint dimension) # 将维度 A、B 和 C 定义为联合维度，Apache Kylin 就仅仅会构建 Cuboid ABC，而 Cuboid AB、BC、A 等等 Cuboid 都不会被生成。即要么都不出现，要么都出现。\n5. 参考文献 # Kylin 维度高级设置 "},{"id":28,"href":"/posts/apache-kylin/ch05/","title":"Ch05-Kylin 之 Cube 构建算法","section":"Blog","content":"Kylin Cube 构建算法\n1. 逐层算法 (Layer Cubing) # 在逐层算法中，按维度数逐层减少来计算，每个层级的计算（除了第一层，它是从原始数据聚合而来），是基于它上一层级的结果来计算的。比如，[Group by A, B]的结果，可以基于[Group by A, B, C]的结果，通过去掉C后聚合得来的；这样可以减少重复计算；\n以上图为例，实际的构建过程是从上往下构建。\n2. 增量构建 (Increment) # Segment 在增量构建中，将 Cube 划分为多个 Segment，每个 Segment 用起始时间和结束时间标志。Segment 代表一段时间内源数据的预计算结果。一个 Segment 的起始时间等于它之前那个 Segment 的结束时间（前闭后开） 对于每个 Segment 的构建，实际上还是按照逐层算法进行的。\n3. 参考文献 # 5000 字带你快速入门 Apache Kylin "},{"id":29,"href":"/posts/apache-kylin/ch04/","title":"Ch04-Kylin 之 Cube 构建流程","section":"Blog","content":"Kylin Cube 执行流程\n1. Cube 构建调度流程 # Cube 的构建流程如下所示：\nNSparkCubingJob 中有个非常重要的方法create，该方法将构建 cube 的两个阶段NResourceDetectStep和NSparkCubingStep封装成 Job，并添加到了 kylin 维护的线程池中，依次执行。\n注意 上图中的addTask()和下图中的getTasks()这两个方法操作的是同一个 queue，因为NSparkCubingJob是DefaultChainedExecutable的子类，并且NSparkCubingJob并没有单独覆盖其方法。\n为了更加清晰的阐述这件事情，这里使用流程图来描述，如下图所示：\n在项目启动的时候，会初始化一个定时线程池 (fetchPool)，接着该线程池又会添加一个定长 (kylin.job.max-concurrent-jobs) 的线程池 (jobPool)。jobPool 中可以执行多个NSparkCubingJob，每个NSparkCubingJob包含了NResourceDetectStep和NSparkCubingStep两个阶段。\n至此，也就能够明白 cube 整体调度流程了，fetchPool 周期性调度 jobPool，jobPool 中如果有 job，那么就会被调起来执行。\n2. ResourceDetectBeforeCubingJob # 这一步的目的对 cube 的配置预处理，方便生成cubing_detect_items.json，resource_paths.json，count_distinct.json这三个文件，以供实际构建 cube 时使用。\n2.2 文件目录 # bin/hdfs dfs -ls /kylin/kylin_metadata/project_demo/job_tmp/5f824eb0-3124-4d02-a722-2ddaddd9eefd/share Found 3 items -rw-r--r-- 1 li supergroup 12 2021-07-05 22:27 /kylin/kylin_metadata/project_demo/job_tmp/5f824eb0-3124-4d02-a722-2ddaddd9eefd/share/72023e32-05cd-9c65-c6d5-19f106f4dfbb_cubing_detect_items.json -rw-r--r-- 1 li supergroup 133 2021-07-05 22:27 /kylin/kylin_metadata/project_demo/job_tmp/5f824eb0-3124-4d02-a722-2ddaddd9eefd/share/72023e32-05cd-9c65-c6d5-19f106f4dfbb_resource_paths.json -rw-r--r-- 1 li supergroup 9 2021-07-05 22:27 /kylin/kylin_metadata/project_demo/job_tmp/5f824eb0-3124-4d02-a722-2ddaddd9eefd/share/count_distinct.json 2.2.1 cubing_detect_items.json # key 值是 layout id，value 是 task 总数（根据 rdd 的依赖关系统计而来）\n{ \u0026#34;-1\u0026#34;: 2 } 2.2.2 resource_paths.json # key 值是 layout id，value 是表数据地址\n{ \u0026#34;-1\u0026#34;: [ \u0026#34;hdfs://localhost:9000/user/hive/warehouse/demo.db/t_student\u0026#34;, \u0026#34;hdfs://localhost:9000/user/hive/warehouse/demo.db/t_part\u0026#34; ] } 2.2.3 count_distinct.json # 度量中是否有选择 count distinct\nfalse 2.3 Spark 命令 # #1 Step Name: Detect Resource job.NSparkExecutable:377 : spark submit cmd: export HADOOP_CONF_DIR=/home/li/Software/apache-kylin-4.0.0-beta-bin/hadoop_conf \u0026amp;\u0026amp; /home/li/Software/apache-kylin-4.0.0-beta-bin/spark/bin/spark-submit --class org.apache.kylin.engine.spark.application.SparkEntry --conf \u0026#39;spark.yarn.queue=default\u0026#39; --conf \u0026#39;spark.history.fs.logDirectory=hdfs:///kylin/spark-history\u0026#39; --conf \u0026#39;spark.master=local\u0026#39; --conf \u0026#39;spark.hadoop.yarn.timeline-service.enabled=false\u0026#39; --conf \u0026#39;spark.driver.cores=1\u0026#39; --conf \u0026#39;spark.eventLog.enabled=true\u0026#39; --conf \u0026#39;spark.eventLog.dir=hdfs:///kylin/spark-history\u0026#39; --conf \u0026#39;spark.driver.memory=1G\u0026#39; --conf \u0026#39;spark.shuffle.service.enabled=true\u0026#39; --conf \u0026#39;spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/li/Software/apache-kylin-4.0.0-beta-bin/conf/spark-driver-log4j.properties -Dkylin.kerberos.enabled=false -Dkylin.hdfs.working.dir=hdfs://localhost:9000/kylin/kylin_metadata/ -Dspark.driver.log4j.appender.hdfs.File=hdfs://localhost:9000/kylin/kylin_metadata/project_demo/spark_logs/driver/5f824eb0-3124-4d02-a722-2ddaddd9eefd-00/execute_output.json.1625495237514.log -Dlog4j.debug=true -Dspark.driver.rest.server.ip=127.0.1.1 -Dspark.driver.rest.server.port=7070 -Dspark.driver.param.taskId=5f824eb0-3124-4d02-a722-2ddaddd9eefd-00 -Dspark.driver.local.logDir=/home/li/Software/apache-kylin-4.0.0-beta-bin/logs/spark\u0026#39; --conf \u0026#39;spark.sql.autoBroadcastJoinThreshold=-1\u0026#39; --conf \u0026#39;spark.sql.adaptive.enabled=false\u0026#39; --conf \u0026#39;spark.driver.extraClassPath=/home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar\u0026#39; --name job_step_5f824eb0-3124-4d02-a722-2ddaddd9eefd-00 --jars /home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar /home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar -className -className org.apache.kylin.engine.spark.job.ResourceDetectBeforeCubingJob /home/li/Software/apache-kylin-4.0.0-beta-bin/tomcat/temp/segmentIds4550979510488136528 --conf 'spark.master=local'这里很神奇的一点是，无论配置参数配成什么样，最后提交的时候都会被覆盖成 local 模式。\n3. CubeBuildJob # Cube 的整体构建流程如下图所示。\n整个流程看起来比较复杂，但是主要的流程如下：\n初始化 CubeInstance 从 CubeInstance 取出满足条件的 CubeSegment 通过 CubeInstance 和 CubeSegment 构造 SegmentInfo 将 SegmentInfo 中的所有 Cuboid 构造成生成树 构建大宽表 从该大宽表和生成树中构造 Cuboid 将构造出来的 Cuboid 写入到 HDFS 中 保存 SegmentInfo 的信息 Cube 是逻辑上由 Cuboid 组成，在实际存储的时候，Cube 由 Segment 组成 (有多少个 build 任务就会有多少个 Segment)，Segment 又由 Cuboid 的实体 parquet 组成，因此 Cube 的构建，本质上就是构建若干 parquet 文件。\n这里要说明下，在 Kylin 中，Cuboid 和 Segment 具体信息在系统启动初始化 Cube 的时候就初始化好了，上图中getSegmentInfo所做的工作只是从已经初始化好的 Cube 中提取相关信息罢了。\n3.1 Cube 初始化流程 # Cube 的初始化流程如下图所示：\n3.2 Cuboid 构建流程 # 3.2.1 AggregationGroup#init() # 在生成 cuboidId 之前会先生成聚合组的 mask，其中用到了大量的移位和位或运算去生成 Mask。 AggregationGroup 在初始化时会生成所有包含维度、必须维度、联合维度、层级维度的 Mask\npublic class AggregationGroup implements Serializable { public void init(CubeDesc cubeDesc, RowKeyDesc rowKeyDesc) { this.cubeDesc = cubeDesc; this.isMandatoryOnlyValid = cubeDesc.getConfig().getCubeAggrGroupIsMandatoryOnlyValid(); if (this.includes == null || this.includes.length == 0 || this.selectRule == null) { throw new IllegalStateException(\u0026#34;AggregationGroup incomplete\u0026#34;); } normalizeColumnNames(); // 所有包含维度的 fullMask buildPartialCubeFullMask(rowKeyDesc); // 强制维度 buildMandatoryColumnMask(rowKeyDesc); // 生成每组联合维度的 mask buildJointColumnMask(rowKeyDesc); // 所有联合维度的 fullMask buildJointDimsMask(); // 每组层级维度的 mask buildHierarchyMasks(rowKeyDesc); // 所有层级维度的 fullMask buildHierarchyDimsMask(); // 普通维度的掩码，不在强制、联合、层级维度中 buildNormalDimsMask(); } } 3.3 SegmentInfo 构建流程 # object MetadataConverter { def getSegmentInfo(cubeInstance: CubeInstance, segmentId: String, segmentName: String, identifier: String): SegmentInfo = { val allColumnDesc = extractAllColumnDesc(cubeInstance) // 根据 cube 信息生成所有的 layout val (layoutEntities, measure) = extractEntityAndMeasures(cubeInstance) // 提取出 dataType 为 bitmap 的度量，count distinct 使用 val dictColumn = measure.values.filter(_.returnType.dataType.equals(\u0026#34;bitmap\u0026#34;)) .map(_.pra.head).toSet // 根据获取到的信息，构造 SegmentInfo SegmentInfo(segmentId, segmentName, identifier, cubeInstance.getProject, cubeInstance.getConfig, // 提取 factTableDesc extractFactTable(cubeInstance), // 提取 lookupTables:List[TableDesc] extractLookupTable(cubeInstance), // 快照 snapshotTables extractLookupTable(cubeInstance), // 表连接信息 joinDescs:Array[JoinDesc] extractJoinTable(cubeInstance), // 所有字段信息，维度、度量 allColumnDesc.asScala.values.toList, // cube 层级信息 layoutEntities, // cube 层级信息，toBuildLayouts mutable.Set[LayoutEntity](layoutEntities: _*), // 字典列。returnType 为 bitmap 的 dictColumn, // 字典列。returnType 为 bitmap 的 dictColumn, // 分区信息，会把点 (.) 转为 0_DOT_0。如 TEST_KYLIN_FACT0_DOT_0CAL_DT \u0026gt;= \u0026#39;1970-01- 01\u0026#39; AND TEST_KYLIN_FACT0_DOT_0CAL_DT \u0026lt; \u0026#39;2012-06-01\u0026#39; extractPartitionExp(cubeInstance.getSegmentById(segmentId)), // 过滤条件 extractFilterCondition(cubeInstance.getSegmentById(segmentId))) } } 3.3.1 extractAllColumnDesc(cubeInstance) # def extractAllColumnDesc(cubeInstance: CubeInstance): java.util.LinkedHashMap[Integer, ColumnDesc] = { // 存放所有列，key 是 index,value 是对应的列。最后返回的集合中包含所有的维度和度量 index val dimensionIndex = new util.LinkedHashMap[Integer, ColumnDesc]() // 所有 rowkey，不包含衍生维度 val columns = cubeInstance.getDescriptor .getRowkey .getRowKeyColumns // 使用 bitIndex 作为 index，提取 rowKeyColumn val dimensionMapping = columns .map(co =\u0026gt; (co.getColRef, co.getBitIndex)) // 单独提取出列的 ColRef 为 Set 集合 val set = dimensionMapping.map(_._1).toSet // 1. 获取所有字段，维度 + 度量 // 2. 和 Rowkeys 取差集，相当于提取出度量列、衍生维度列，然后使用 zipWithIndex 自动生成序号 // 3. 为了避免与维度列的 index 混淆，index 加上所有维度的数量 val refs = cubeInstance.getAllColumns.asScala.diff(set) .zipWithIndex .map(tp =\u0026gt; (tp._1, tp._2 + dimensionMapping.length)) // 将维度和度量列统一方放入 dimensionIndex map 中 val columnIDTuples = dimensionMapping ++ refs val colToIndex = columnIDTuples.toMap columnIDTuples .foreach { co =\u0026gt; dimensionIndex.put(co._2, toColumnDesc(co._1, co._2, set.contains(co._1))) } dimensionIndex } 3.3.2 extractEntityAndMeasures(cubeInstance) # def extractEntityAndMeasures(cubeInstance: CubeInstance): (List[LayoutEntity], Map[Integer, FunctionDesc]) = { // 获取所有的列 index 信息，根据 bitIndex 生成 val (columnIndexes, shardByColumnsId, idToColumnMap, measureId) = genIDToColumnMap(cubeInstance) (cubeInstance.getCuboidScheduler .getAllCuboidIds // 获取所有 cuboidId，根据 cuboidId 生成 layouts .asScala .map { long =\u0026gt; // 遍历每个 cuboidId，生成 layoutEntity genLayoutEntity(columnIndexes, shardByColumnsId, idToColumnMap, measureId, long) }.toList, measureId.asScala.toMap) } private def genIDToColumnMap(cubeInstance: CubeInstance): (List[Integer], List[Integer], java.util.Map[Integer, ColumnDesc], java.util.Map[Integer, FunctionDesc]) = { val dimensionIndex = new util.LinkedHashMap[Integer, ColumnDesc]() ... val shardByColumnsId = shardByColumns.asScala.toList .map(column =\u0026gt; dimensionMap.get(column)) .filter(v =\u0026gt; v != null) .map(column =\u0026gt; Integer.valueOf(column.get)) ... val idToColumnMap = dimensionMapping.map(tp =\u0026gt; Integer.valueOf(tp._2)).toList val measureIndex = new util.LinkedHashMap[Integer, FunctionDesc]() ... (idToColumnMap, shardByColumnsId, dimensionIndex, measureIndex) } private def genLayoutEntity( columnIndexes: List[Integer], shardByColumnsId: List[Integer], idToColumnMap: java.util.Map[Integer, ColumnDesc], measureId: java.util.Map[Integer, FunctionDesc], long: lang.Long) = { // 根据 cuboidId 计算出该 cuboid 包含的哪些维度 val dimension = BitUtils.tailor(columnIndexes.asJava, long) // 存放维度的 bitIndex val integerToDesc = new util.LinkedHashMap[Integer, ColumnDesc]() // 把 cuboid 的维度信息放入 map 中 dimension.asScala.foreach(index =\u0026gt; integerToDesc.put(index, idToColumnMap.get(index))) val entity = new LayoutEntity() entity.setId(long) entity.setOrderedDimensions(integerToDesc) entity.setOrderedMeasures(measureId) val shards = shardByColumnsId.filter(column =\u0026gt; dimension.contains(column)) entity.setShardByColumns(shards.asJava) entity } } 3.4 SpanningTree # 在 SegmentInfo 中的会生成 toBuildLayouts，但是这些 layoutEntities 是无序的，但是在 Cube 中这些 layout 是具有父子级关系的，例如 ABCD-\u0026gt;ABC-\u0026gt;AB，那么就需要有一个数据结构来生成并保存这种顺序。 在 ForestSpanningTree 中会先将传入的 layoutEntities 进行排序，排序的优先级为维度数-\u0026gt;度量数-\u0026gt;cuboidId\nfor (String segId : segmentIds) { ... seg = ManagerHub.getSegmentInfo(config, cubeName, segId); spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(seg.toBuildLayouts())); ... } 3.5 ParentSourceChooser # ParentSourceChooser是整个构建 cube 的逻辑中最复杂的一部分，这里依次完成了大宽表，SnapShot 等的构建。\nfor (String segId : segmentIds) { ... sourceChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, true); sourceChooser.decideSources(); NBuildSourceInfo buildFromFlatTable = sourceChooser.flatTableSource(); Map\u0026lt;Long, NBuildSourceInfo\u0026gt; buildFromLayouts = sourceChooser.reuseSources(); ... } // String flatTablePath = sourceChooser.persistFlatTableIfNecessary(); 3.6 build # 层级构建会基于上面生成的宽表进行构建，优先构建 baseCuboid\n3.6.1 buildCuboid # // 按照 rowkeys(维度) 的顺序进行分区排序 Dataset\u0026lt;Row\u0026gt; afterSort = afterAgg .select(NSparkCubingUtil.getColumns(rowKeys, layoutEntity.getOrderedMeasures().keySet())) .sortWithinPartitions(NSparkCubingUtil.getColumns(rowKeys)); saveAndUpdateLayout(afterSort, seg, layoutEntity, parentId); 3.6.2 saveAndUpdateLayout # 将结果保存到 temp 目录\nString tempPath = path + TEMP_DIR_SUFFIX; // save to temp path logger.info(\u0026#34;Cuboids are saved to temp path : \u0026#34; + tempPath); storage.saveTo(tempPath, dataset, ss); 将结果重分区，然后写入到最终的 path 下\n// 进行重分区，默认会按文件大小 (128M)，行数 (2500000，如果存在 count distinct 则是 1000000), 将结果重分区写到 path 路径下 int shardNum = BuildUtils.repartitionIfNeed(layout, storage, path, tempPath, cubeInstance.getConfig(), ss); layout.setShardNum(shardNum); cuboidShardNum.put(layoutId, (short) shardNum); ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), null); QueryExecutionCache.removeQueryExecution(queryExecutionId); // 记录文件数量，文件大小 BuildUtils.fillCuboidInfo(layout, path); 3.7 Spark 命令 # #2 Step Name: Build Cube with Spark job.NSparkExecutable:377 : spark submit cmd: export HADOOP_CONF_DIR=/home/li/Software/apache-kylin-4.0.0-beta-bin/hadoop_conf \u0026amp;\u0026amp; /home/li/Software/apache-kylin-4.0.0-beta-bin/spark/bin/spark-submit --class org.apache.kylin.engine.spark.application.SparkEntry --conf \u0026#39;spark.yarn.queue=default\u0026#39; --conf \u0026#39;spark.history.fs.logDirectory=hdfs:///kylin/spark-history\u0026#39; --conf \u0026#39;spark.master=yarn\u0026#39; --conf \u0026#39;spark.hadoop.yarn.timeline-service.enabled=false\u0026#39; --conf \u0026#39;spark.driver.cores=1\u0026#39; --conf \u0026#39;spark.eventLog.enabled=true\u0026#39; --conf \u0026#39;spark.eventLog.dir=hdfs:///kylin/spark-history\u0026#39; --conf \u0026#39;spark.driver.memory=1G\u0026#39; --conf \u0026#39;spark.shuffle.service.enabled=true\u0026#39; --conf \u0026#39;spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/li/Software/apache-kylin-4.0.0-beta-bin/conf/spark-driver-log4j.properties -Dkylin.kerberos.enabled=false -Dkylin.hdfs.working.dir=hdfs://localhost:9000/kylin/kylin_metadata/ -Dspark.driver.log4j.appender.hdfs.File=hdfs://localhost:9000/kylin/kylin_metadata/project_demo/spark_logs/driver/caecf214-ec24-4f30-baa0-9a242719e108-01/execute_output.json.1625311763246.log -Dlog4j.debug=true -Dspark.driver.rest.server.ip=127.0.1.1 -Dspark.driver.rest.server.port=7070 -Dspark.driver.param.taskId=caecf214-ec24-4f30-baa0-9a242719e108-01 -Dspark.driver.local.logDir=/home/li/Software/apache-kylin-4.0.0-beta-bin/logs/spark\u0026#39; --conf \u0026#39;spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkylin.hdfs.working.dir=hdfs://localhost:9000/kylin/kylin_metadata/ -Dkylin.metadata.identifier=kylin_metadata -Dkylin.spark.category=job -Dkylin.spark.project=project_demo -Dkylin.spark.identifier=5f824eb0-3124-4d02-a722-2ddaddd9eefd -Dkylin.spark.jobName=5f824eb0-3124-4d02-a722-2ddaddd9eefd-01 -Duser.timezone=Asia/Shanghai\u0026#39; --conf \u0026#39;spark.driver.extraClassPath=/home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar\u0026#39; --files /home/li/Software/apache-kylin-4.0.0-beta-bin/conf/spark-executor-log4j.properties --name job_step_caecf214-ec24-4f30-baa0-9a242719e108-01 --jars /home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar /home/li/Software/apache-kylin-4.0.0-beta-bin/lib/kylin-parquet-job-4.0.0-beta.jar -className -className org.apache.kylin.engine.spark.job.CubeBuildJob /home/li/Software/apache-kylin-4.0.0-beta-bin/tomcat/temp/segmentIds4902579912004494679 4. 参考文献 # Kylin on Parquet Kylin 源码解析-kylin 构建流程总览 Kylin 源码解析 - 构建层级分析 Kylin 官方案例详细剖析及剪枝优化-OLAP 商业环境实战 Kylin 如何实现基数统计 "},{"id":30,"href":"/posts/apache-kylin/ch03/","title":"Ch03-Kylin 之 Query","section":"Blog","content":"query 的整体执行流程如下图所示。可以看到整个 query 的逻辑实际上是由 QueryService 处理的，这里会做重要的路由处理，是执行 cube 的查询，还是下推到其他查询引擎，还是下推到 hive。这个路由逻辑简单来说，如下图所示：\n完整的处理逻辑如下图所示：\nquery 和 update 的逻辑如下所示：\n1. Query # 在上图的 query 流程中，其实可以看到所有的处理过程最后会交给 calcite，而 calcite 的实现类 OLAPTableScan 会将 OLAPToEnumerableConverter 注册进来，最终在implement方法中完成 RelNode 到 Cube 处理逻辑的转换。\n1.1 Cube 选取流程 # 整个过程封装在 RealizationChooser#selectRealization 中，大体上可以分为两步，第一部分是makeOrderedModelMap()，这一步会选出所有满足的 cuboid，第二步是selectRealization()，这一步骤会选出唯一一适合的 cuboid(如果所有的都合适的话，就选 set 里面遍历到的第一个)。详细可以分成下述几步来讲\n1.1.1 对 model 及对应的 realizations 进行过滤及排序 # 获取属于该 project 下 factTableName 与查询中事实表相等的所有 realizations，factTableName 即 context.firstTableScan.getTableName 对 realizations 执行过滤，得到 filteredRealizations NOT READY cube 会被过滤 黑名单中的 cube 会被过滤 cube.allColumns 必须与 OLAPContext.allColumns 相等或是其父集。cube.allColumns 包含了事实表的外键列、维度表的主键列、所有度量涉及的列、所有维度列 OLAPContext.allColumns 在 OLAPRel#implementOLAP 方法中被添加 OLAPContext.filterColumns 在 OLAPFilterRel#implementOLAP 中被添加 project 包含的列（即 agg 参数列即 group by 列），在 OLAPProjectRel#implementOLAP 添加 遍历 filteredRealizations，对于每个 realization，获取其 model，并记录每个 mode 对应的最小的 realization cost 及 model 对应的 Set 根据各个 model 对应的最小 realization cost，对各个 \u0026lt;model, Set\u0026lt;IRealization\u0026gt;\u0026gt; 进行排序，得到 modelMap: Map\u0026lt;DataModelDesc, Set\u0026lt;IRealization\u0026gt;\u0026gt; 如果 modelMap 为空，则抛 No model found for \u0026hellip; 异常 1.1.2 从 modelMap 中选择最终的 realization # public class RealizationChooser { private static void attemptSelectRealization(OLAPContext context) { Map\u0026lt;DataModelDesc, Set\u0026lt;IRealization\u0026gt;\u0026gt; modelMap = makeOrderedModelMap(context); for (Map.Entry\u0026lt;DataModelDesc, Set\u0026lt;IRealization\u0026gt;\u0026gt; entry : modelMap.entrySet()) { ... IRealization realization = QueryRouter.selectRealization(context, entry.getValue()); if (realization == null) { logger.info(\u0026#34;Give up on model {} because no suitable realization is found\u0026#34;, model); context.unfixModel(); continue; } context.realization = realization; return } throw new NoRealizationFoundException(\u0026#34;No realization found for \u0026#34; + toErrorMsg(context)); } } 1.1.2.1 modelMap 遍历逻辑 # IRealization realization = QueryRouter.selectRealization(context, entry.getValue())\n若 realization 不为 null，则 realization 就是选中的 realization，设置为 context.realization，选择过程结束；否则，continue，对下一个 entry 进行同样的调用 若遍历完所有的 entry，依然没有符合要求的 realization，则抛异常 NoRealizationFoundException 1.1.2.2 selectRealization 执行逻辑 # 对候选的 realizations 应用 3 条规则，以进行过滤和重新排序：\n移除 (RemoveBlackoutRealizationsRule) 黑名单中的和移除被配置 (kylin.query.realization-filter) 过滤的 移除 (RemoveUncapableRealizationsRule) 不适用的（逻辑封装在 CubeCapabilityChecker#check 中），以下几种情况不适用： OLAPContext 维度列（其 groupByColumns（在 OLAPAggregateRel#implementOLAP 中添加） + filterColumns（在 OLAPFilterRel#implementOLAP 中添加））中存在不在 cube 维度列中的情况 OLAPContext.aggregations（在 OLAPAggregateRel#implementOLAP 中添加）中存在不在 cube aggregations 中的情况 limit 在 agg 之前（使用 OLAPContext#limitPrecedesAggr 判断，在 OLAPAggregateRel#implementOLAP 中进行判断），会导致 cube 的度量结果与查询不一致 对剩下的进行排序 (RealizationSortRule)，优先级最高、cost 最小的胜出 1.2 Cuboid 定位流程 # 1.2.1 Kylin 是怎么去掉 join 过程 # Kylin 最终下推到 Cubiod 的时候，应该是个点查的操作。比如 SQL 里面包含若干 left join，但是最终执行的时候，绝对不会有 left join，那么这个去掉 join 的过程是如何完成的？\n这一块逻辑可以参考CalciteToSparkPlaner.scala的逻辑。\nclass CalciteToSparkPlaner(dataContext: DataContext) extends RelVisitor with Logging { private val stack = new util.Stack[DataFrame]() private val unionStack = new util.Stack[Int]() stack.push(node match { ... case rel: OLAPJoinRel =\u0026gt; if (!rel.isRuntimeJoin) { logTime(\u0026#34;join with table scan\u0026#34;) { TableScanPlan.createOLAPTable(rel, dataContext) } } else { val right = stack.pop() val left = stack.pop() logTime(\u0026#34;join\u0026#34;) { plans.JoinPlan.join(Lists.newArrayList(left, right), rel) } } case rel: OLAPUnionRel =\u0026gt; val size = unionStack.pop() val java = Range(0, stack.size() - size).map(a =\u0026gt; stack.pop()).asJava logTime(\u0026#34;union\u0026#34;) { plans.UnionPlan.union(Lists.newArrayList(java), rel, dataContext) } ... }) } 可以看到对于查询是的 join 操作，最终会走到TableScanPlan.createOLAPTable(rel, dataContext)这个逻辑，而这个逻辑本质上是查找并读取 cuboid 的逻辑。\nobject TableScanPlan extends LogEx { def createOLAPTable(rel: OLAPRel, dataContext: DataContext): DataFrame = logTime(\u0026#34;table scan\u0026#34;, info = true) { ... val request = query.getStorageQueryRequest( olapContext.storageContext, olapContext.getSQLDigest, returnTupleInfo) request.getGroups val cuboid = request.getCuboid ... var df = SparderContext.getSparkSession.kylin .format(\u0026#34;parquet\u0026#34;) .cuboidTable(cubeInstance, cuboid) .toDF(schemaNames: _*) ... val columns = RuntimeHelper.gtSchemaToCalciteSchema( cuboid.getCuboidToGridTableMapping.getPrimaryKey, tuple._2, factTableAlias, rel.getColumnRowType.getAllColumns.asScala.toList, df.schema, columnIndex, tupleIdx, topNMapping, topNMeasureIndexes) df.select(columns: _*) } } 而 cuboid 的定位逻辑其实并不复杂。\n比如 SQL 中表示的维度是 A, B, C，用 3 位的 2 进制可以表示为100(A), 010(B), 001(C)。那么 AB 就可以表示为110，BC 可以表示为011，AC 可以表示为101，同理 ABC 就可以表示为111。\n上图路径hdfs://localhost:9000/kylin/kylin_metadata/project_demo/parquet/cube_demo/FULL_BUILD_UC0/3/part-00000-77d5c00c-95b0-4b35-b77d-c7f9acbbb694-c000.snappy.parquet中的3表示的就是对维度 BC 建立的 cuboid。\n而文件内容中 schema 信息 (10, 11, 12) 则表示的是度量COUNT(1), MIN(D)，SUM(E)。而（1，0）表示的是 A, B 两个维度。\n2. 参考文献 # Kylin on Parquet 介绍和快速上手 可能是全网最深度的 Apache Kylin 查询剖析 "},{"id":31,"href":"/posts/apache-calcite/ch02/","title":"Ch02-Calcite 执行流程","section":"Blog","content":"Apache Calcite 是一种提供了标准的 SQL 语言、多种查询优化和连接各种数据源基础框架，可以让用户轻松的接入各种数据，并实现使用 SQL 查询。此外，Calcite 还提供了 OLAP 和流处理的查询引擎。\n1. 处理流程 # 但这里为了讲述方便，把 SQL 的执行分为下面五个阶段（跟上面比比又独立出了一个阶段）：\n解析 SQL，把 SQL 转换成为 AST（抽象语法树），在 Calcite 中用 SqlNode 来表示； 语法检查，根据数据库的元数据信息进行语法验证，验证之后还是用 SqlNode 表示 AST 语法树； 语义分析，根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）； 逻辑计划优化，优化器的核心，根据前面生成的逻辑计划按照相应的规则（Rule）进行优化； 物理执行，生成物理计划，物理执行计划执行。 这里只关注前四步的内容，会配合源码实现以及一个示例来讲解。\nid | name | sex :---|:-----|:--- 0 | bob | male 1 | tom | male 2 | tom | female 2. SQL 解析阶段（SQL–\u0026gt;SqlNode） # Calcite 使用 JavaCC 做 SQL 解析，JavaCC 根据 Calcite 中定义的 Parser.jj 文件，生成一系列的 java 代码，生成的 Java 代码会把 SQL 转换成 AST 的数据结构（这里是 SqlNode 类型）。\n3. SqlNode 验证（SqlNode–\u0026gt;SqlNode） # 4. 语义分析（SqlNode–\u0026gt;RelNode/RexNode） # 经过第二步之后，这里的 SqlNode 就是经过语法校验的 SqlNode 树，接下来这一步就是将 SqlNode 转换成 RelNode/RexNode，也就是生成相应的逻辑计划（Logical Plan）。\n2021-05-21T23:54:16,187 DEBUG [main] calcite.sql2rel: Plan after converting SqlNode to RelNode\rLogicalSort(sort0=[$0], dir0=[ASC])\rLogicalFilter(condition=[=($0, \u0026#39;male\u0026#39;)])\rLogicalAggregate(group=[{0}], CNT=[COUNT()])\rLogicalProject(SEX=[$2])\rLogicalFilter(condition=[=($1, \u0026#39;tom\u0026#39;)])\rLogicalTableScan(table=[[default, T_MEM_TABLE]]) 5. 优化阶段（RelNode–\u0026gt;RelNode） # Calcite 中关于优化器提供了两种实现：\n优化器 说明 HepPlanner RBO 的实现，它是一个启发式的优化器，按照规则进行匹配，直到达到次数限制（match 次数限制）或者遍历一遍后不再出现 rule match 的情况才算完成； VolcanoPlanner CBO 的实现，它会一直迭代 rules，直到找到 cost 最小的 paln。 5.1 HepPlanner # HepPlanner 会先将所有 relNode tree 转化为 HepRelVertex，这时就构建了一个 Graph：将所有的 elNode 节点使用 Vertex 表示，Gragh 会记录每个 HepRelVertex 的 input 信息，这样就是构成了一张 graph。在真正的实现时，递归逐渐将每个 relNode 转换为 HepRelVertex，并在 graph 中记录相关的信息。\n5.2 VolcannoPlanner # VolcanoPlanner 在优化过程中，rule 的触发会产⽣新的 RelNode, VolcanoPlanner 会把新的 RelNode 和 旧的 RelNode 都保存起来，通过⽐较代价 (cost) 的⼤⼩，选择最合适的 RelNode。新旧的 RelNode 在语义上等价，组成搜索空间，在这个搜索空间中，来⽐较 RelNode 的代价。\n6. 执行阶段 # 7. 总结 # 8. 参考文献 # Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources Apache Calcite 处理流程详解（一） SQL 形式化语言——关系代数 "},{"id":32,"href":"/posts/apache-calcite/ch01/","title":"Ch01-Calcite 介绍","section":"Blog","content":"Apache Calcite 是一种提供了标准的 SQL 语言、多种查询优化和连接各种数据源基础框架，可以让用户轻松的接入各种数据，并实现使用 SQL 查询。此外，Calcite 还提供了 OLAP 和流处理的查询引擎。\n1. Calcite 是什么 # Calcite 之前的名称叫做 optiq，optiq 起初在 Hive 项目中，为 Hive 提供基于成本模型的优化，即 CBO（Cost Based Optimizatio）。2014 年 5 月 optiq 独立出来，成为 Apache 社区的孵化项目，2014 年 9 月正式更名为 Calcite。Calcite 项目的创建者是 Julian Hyde，他在数据平台上有非常多的工作经历，曾经是 Oracle、Broadbase 公司 SQL 引擎的主要开发者、SQLStream 公司的创始人和主架构师、Pentaho BI 套件中 OLAP 部分的架构师和主要开发者。现在他在 Hortonworks 公司负责 Calcite 项目，其工作经历对 Calcite 项目有很大的帮助。除了 Hortonworks，该项目的代码提交者还有 MapR、Salesforce 等公司，并且还在不断壮大。\nCalcite 的目标是one size fits all，希望能为不同计算平台和数据源提供统一的查询引擎，并以类似传统数据库的访问方式（SQL 和高级查询优化）来访问 Hadoop 上的数据。Calcite 的架构有三个特点：flexible, embeddable, and extensible，就是灵活性、组件可插拔、可扩展，它的 SQL Parser 层、Optimizer 层等都可以单独使用，这也是 Calcite 受总多开源框架欢迎的原因之一。\n2. 关系代数 # 关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。关系代数也是 Calcite 的核心，任何一个查询都可以表示成由关系运算符组成的树。在 Calcite 中，它会先将 SQL 转换成关系表达式（relational expression），然后通过规则匹配（rules match）进行相应的优化，优化会有一个成本（cost）模型为参考。\n名称 英文 符号 说明 选择 select σ 类似于 SQL 中的 where 投影 project Π 类似于 SQL 中的 select 并 union ∪ 类似于 SQL 中的 union 集合差 set-difference - SQL 中没有对应的操作符 笛卡儿积 Cartesian-product × 类似于 SQL 中不带 on 条件的 inner join 重命名 rename ρ 类似于 SQL 中的 as 集合交 intersection ∩ SQL 中没有对应的操作符 自然连接 natural join ⋈ 类似于 SQL 中的 inner join 赋值 assignment ← 3. 查询优化 # 查询优化主要是围绕着 等价交换 的原则做相应的转换，这部分可以参考数据库系统概念（中文第六版）第 13 章——查询优化。这里举个简单的例子说明下\n左图表示完整的一个原始的表达式树，如果将过滤条件 (SELECT) 下推，同时裁减相应的列 (PROJECT) 便可以的到右图所示的表达式树。在实际执行的时候，会极大的减少 IO 消耗以及中间结果产生的数据量。因此会有更高效的查询速度。\n4. Calcite 概念 # 类型 描述 特点 RelOptRule transforms an expression into another。对 expression 做等价转换 根据传递给它的 RelOptRuleOperand 来对目标 RelNode 树进行规则匹配，匹配成功后，会再次调用 matches() 方法（默认返回真）进行进一步检查。如果 mathes() 结果为真，则调用 onMatch() 进行转换。 ConverterRule Abstract base class for a rule which converts from one calling convention to another without changing semantics. 它是 RelOptRule 的子类，专门用来做数据源之间的转换（Calling convention），ConverterRule 一般会调用对应的 Converter 来完成工作，比如说：JdbcToSparkConverterRule 调用 JdbcToSparkConverter 来完成对 JDBC Table 到 Spark RDD 的转换。 RelNode relational expression，RelNode 会标识其 input RelNode 信息，这样就构成了一棵 RelNode 树 代表了对数据的一个处理操作，常见的操作有 Sort、Join、Project、Filter、Scan 等。它蕴含的是对整个 Relation 的操作，而不是对具体数据的处理逻辑。 Converter A relational expression implements the interface Converter to indicate that it converts a physical attribute, or RelTrait of a relational expression from one value to another. 用来把一种 RelTrait 转换为另一种 RelTrait 的 RelNode。如 JdbcToSparkConverter 可以把 JDBC 里的 table 转换为 Spark RDD。如果需要在一个 RelNode 中处理来源于异构系统的逻辑表，Calcite 要求先用 Converter 把异构系统的逻辑表转换为同一种 Convention。 RexNode Row-level expression 行表达式（标量表达式），蕴含的是对一行数据的处理逻辑。每个行表达式都有数据的类型。这是因为在 Valdiation 的过程中，编译器会推导出表达式的结果类型。常见的行表达式包括字面量 RexLiteral，变量 RexVariable，函数或操作符调用 RexCall 等。RexNode 通过 RexBuilder 进行构建。 RelTrait RelTrait represents the manifestation of a relational expression trait within a trait definition. 用来定义逻辑表的物理相关属性（physical property），三种主要的 trait 类型是：Convention、RelCollation、RelDistribution； Convention Calling convention used to repressent a single data source, inputs must be in the same convention 继承自 RelTrait，类型很少，代表一个单一的数据源，一个 relational expression 必须在同一个 convention 中； RelTraitDef 主要有三种：ConventionTraitDef：用来代表数据源 RelCollationTraitDef：用来定义参与排序的字段；RelDistributionTraitDef：用来定义数据在物理存储上的分布方式（比如：single、hash、range、random 等）； RelOptCluster An environment for related relational expressions during the optimization of a query. palnner 运行时的环境，保存上下文信息； RelOptPlanner A RelOptPlanner is a query optimizer: it transforms a relational expression into a semantically equivalent relational expression, according to a given set of rules and a cost model. 也就是优化器，Calcite 支持RBO（Rule-Based Optimizer） 和 CBO（Cost-Based Optimizer）。Calcite 的 RBO （HepPlanner）称为启发式优化器（heuristic implementation ），它简单地按 AST 树结构匹配所有已知规则，直到没有规则能够匹配为止；Calcite 的 CBO 称为火山式优化器（VolcanoPlanner）成本优化器也会匹配并应用规则，当整棵树的成本降低趋于稳定后，优化完成，成本优化器依赖于比较准确的成本估算。RelOptCost 和 Statistic 与成本估算相关； RelOptCost defines an interface for optimizer cost in terms of number of rows processed, CPU cost, and I/O cost. 优化器成本模型会依赖； 5. Calcite 架构 # Calcite 与传统数据库管理系统有一些相似之处，相比而言，它将数据存储、数据处理算法和元数据存储这些部分忽略掉了，这样设计带来的好处是：对于涉及多种数据源和多种计算引擎的应用而言，Calcite 因为可以兼容多种存储和计算引擎，使得 Calcite 可以提供统一查询服务，Calcite 将会是这些应用的最佳选择。\n在 Calcite 架构中，最核心地方就是 Optimizer，也就是优化器，一个 Optimization Engine 包含三个组成部分：\nrules：也就是匹配规则，Calcite 内置上百种 Rules 来优化 relational expression，当然也支持自定义 rules； metadata providers：主要是向优化器提供信息，这些信息会有助于指导优化器向着目标（减少整体 cost）进行优化，信息可以包括行数、table 哪一列是唯一列等，也包括计算 RelNode 树中执行 subexpression cost 的函数； planner engines：它的主要目标是进行触发 rules 来达到指定目标，比如 RBO，CBO 参考文献 # Apache Calcite：Hadoop 中新型大数据查询引擎 SQL 形式化语言——关系代数 calcite 概念和架构 "},{"id":33,"href":"/posts/apache-kylin/ch02/","title":"Ch02-Kylin 之 Cube","section":"Blog","content":"Kylin 引入了一个非常重要的概念 —— Cube 和 Cuboid，Cube 由若干 Cuboid 组成。\n1. 前置知识 # 名词 说明 维度 可以简单理解为观察数据的角度，一般是一组离散的值。 度量 被聚合的统计值，也是聚合运算的结果，一般指聚合函数 (如：sum、count、average 等)。 比如下述 SQL 语句中，GROUP BY d_year, p_brand 中的 d_year 和 p_brand 就是维度。而 SELECT SUM(lo_revenue) AS lo_revenue 中的 SUM(lo_revenue) 就是度量。\nSELECT SUM(lo_revenue) AS lo_revenue, d_year, p_brand FROM p_lineorder LEFT JOIN dates ON lo_orderdate = d_datekey LEFT JOIN part ON lo_partkey = p_partkey LEFT JOIN supplier ON lo_suppkey = s_suppkey WHERE p_category = \u0026#39;MFGR#12\u0026#39; AND s_region = \u0026#39;AMERICA\u0026#39; GROUP BY d_year, p_brand ORDER BY d_year, p_brand; 2. Cube # 而如果将表中的维度随意组合，那么每种组合我们称为 cuboid，而将这些所有的组合统称为 cube，因此其之间的关系如下图所示。\n每一种维度组合称之为 Cuboid（上图中的每个点），所有 Cuboid 的集合是 Cube（上图中的整张图）。其中由所有维度组成的 Cuboid 称为 Base Cuboid，图中（time，item，location，supplier）即为 Base Cuboid，所有的 Cuboid 都可以基于 Base Cuboid 计算出来。Cube 在 kylin 中用 json 表示，如附件所示。\nModel VS Cube\nModel 是 Cube 的基础，用于描述一个数据模型 有了数据模型，定义 Cube 可以直接从此模型定义的表和列中进行选择 基于一个数据模型可以创建多个 Cube\n3. 维度爆炸 # 其实很容易发现，维度如果设计的稍微不合理，就会导致出现若干 cuboid (最多 2^n 个)，而根据业务场景，可能相当多的 cuboid 根本用不到，因此需要一定的剪枝手段，来避免构建如此大量的 cuboid。\n因此 Kylin 在构建 Cube 的时候，用户可以选择 Mandatory Dimensions，Hierarchy Dimensions，Joint Dimensions 中的一种，进行剪枝，避免维度爆炸的问题。\nEntry 名词 说明 Mandatory Dimensions 必要维度 所有不含此维度的 cuboid 就可以被跳过计算 Hierarchy Dimensions 层级维度 例如“国家” -\u0026gt; “省” -\u0026gt; “市”是一个层级；不符合此层级关系的 cuboid 可以被跳过计算 Joint Dimensions 联合维度 有些维度往往一起出现，或者它们的基数非常接近（有 1:1 映射关系）。例如“user_id”和“email”。把多个维度定义为组合关系后，所有不符合此关系的 cuboids 会被跳过计算 4. 附件 # 4.1 model.json # { \u0026#34;uuid\u0026#34;: \u0026#34;c5e7d814-c2f7-e312-f470-0a609b517684\u0026#34;, \u0026#34;last_modified\u0026#34;: 1623556297179, \u0026#34;version\u0026#34;: \u0026#34;4.0.0.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;model_demo\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;ADMIN\u0026#34;, \u0026#34;is_draft\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fact_table\u0026#34;: \u0026#34;DEMO.T_STUDENT\u0026#34;, \u0026#34;lookups\u0026#34;: [ { \u0026#34;table\u0026#34;: \u0026#34;DEMO.T_PART\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;LOOKUP\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;T_PART\u0026#34;, \u0026#34;join\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;primary_key\u0026#34;: [ \u0026#34;T_PART.P_PARTKEY\u0026#34; ], \u0026#34;foreign_key\u0026#34;: [ \u0026#34;T_STUDENT.STU_PARTKEY\u0026#34; ] } }, { \u0026#34;table\u0026#34;: \u0026#34;DEMO.T_DATES\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;LOOKUP\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;T_DATES\u0026#34;, \u0026#34;join\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;primary_key\u0026#34;: [ \u0026#34;T_DATES.D_DATEKEY\u0026#34; ], \u0026#34;foreign_key\u0026#34;: [ \u0026#34;T_STUDENT.STU_DATEKEY\u0026#34; ] } } ], \u0026#34;dimensions\u0026#34;: [ { \u0026#34;table\u0026#34;: \u0026#34;T_STUDENT\u0026#34;, \u0026#34;columns\u0026#34;: [ \u0026#34;STU_NAME\u0026#34;, \u0026#34;STU_CLASS\u0026#34;, \u0026#34;STU_PARTKEY\u0026#34;, \u0026#34;STU_DATEKEY\u0026#34; ] }, { \u0026#34;table\u0026#34;: \u0026#34;T_PART\u0026#34;, \u0026#34;columns\u0026#34;: [ \u0026#34;P_CATEGORY\u0026#34;, \u0026#34;P_PARTKEY\u0026#34; ] }, { \u0026#34;table\u0026#34;: \u0026#34;T_DATES\u0026#34;, \u0026#34;columns\u0026#34;: [ \u0026#34;D_DATE\u0026#34;, \u0026#34;D_DATEKEY\u0026#34; ] } ], \u0026#34;metrics\u0026#34;: [ \u0026#34;T_STUDENT.STU_AGE\u0026#34;, \u0026#34;T_STUDENT.STU_SCORE\u0026#34; ], \u0026#34;filter_condition\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;partition_desc\u0026#34;: { \u0026#34;partition_date_column\u0026#34;: null, \u0026#34;partition_time_column\u0026#34;: null, \u0026#34;partition_date_start\u0026#34;: 0, \u0026#34;partition_date_format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34;, \u0026#34;partition_time_format\u0026#34;: \u0026#34;HH:mm:ss\u0026#34;, \u0026#34;partition_type\u0026#34;: \u0026#34;APPEND\u0026#34;, \u0026#34;partition_condition_builder\u0026#34;: \u0026#34;org.apache.kylin.metadata.model.PartitionDesc$DefaultPartitionConditionBuilder\u0026#34; }, \u0026#34;capacity\u0026#34;: \u0026#34;MEDIUM\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;project_demo\u0026#34; } 4.2 cube.json # { \u0026#34;uuid\u0026#34;: \u0026#34;9466fff1-072b-de1b-c80b-e3b2ce39868c\u0026#34;, \u0026#34;last_modified\u0026#34;: 1623556408303, \u0026#34;version\u0026#34;: \u0026#34;4.0.0.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cube_demo\u0026#34;, \u0026#34;is_draft\u0026#34;: false, \u0026#34;model_name\u0026#34;: \u0026#34;model_demo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;null_string\u0026#34;: null, \u0026#34;dimensions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;STU_NAME\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_STUDENT\u0026#34;, \u0026#34;column\u0026#34;: \u0026#34;STU_NAME\u0026#34;, \u0026#34;derived\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;STU_CLASS\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_STUDENT\u0026#34;, \u0026#34;column\u0026#34;: \u0026#34;STU_CLASS\u0026#34;, \u0026#34;derived\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;STU_DATEKEY\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_STUDENT\u0026#34;, \u0026#34;column\u0026#34;: \u0026#34;STU_DATEKEY\u0026#34;, \u0026#34;derived\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;STU_PARTKEY\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_STUDENT\u0026#34;, \u0026#34;column\u0026#34;: \u0026#34;STU_PARTKEY\u0026#34;, \u0026#34;derived\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;P_PARTKEY\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_PART\u0026#34;, \u0026#34;column\u0026#34;: null, \u0026#34;derived\u0026#34;: [ \u0026#34;P_PARTKEY\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;P_CATEGORY\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_PART\u0026#34;, \u0026#34;column\u0026#34;: null, \u0026#34;derived\u0026#34;: [ \u0026#34;P_CATEGORY\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;D_DATEKEY\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_DATES\u0026#34;, \u0026#34;column\u0026#34;: null, \u0026#34;derived\u0026#34;: [ \u0026#34;D_DATEKEY\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;D_DATE\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;T_DATES\u0026#34;, \u0026#34;column\u0026#34;: null, \u0026#34;derived\u0026#34;: [ \u0026#34;D_DATE\u0026#34; ] } ], \u0026#34;measures\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;_COUNT_\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;COUNT\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;constant\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;returntype\u0026#34;: \u0026#34;bigint\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;MIN_AGE\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;MIN\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;column\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;T_STUDENT.STU_AGE\u0026#34; }, \u0026#34;returntype\u0026#34;: \u0026#34;integer\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;SUM_SCORE\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;SUM\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;column\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;T_STUDENT.STU_SCORE\u0026#34; }, \u0026#34;returntype\u0026#34;: \u0026#34;bigint\u0026#34; } } ], \u0026#34;dictionaries\u0026#34;: [], \u0026#34;rowkey\u0026#34;: { \u0026#34;rowkey_columns\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;T_STUDENT.STU_NAME\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;dict\u0026#34;, \u0026#34;isShardBy\u0026#34;: false }, { \u0026#34;column\u0026#34;: \u0026#34;T_STUDENT.STU_CLASS\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;dict\u0026#34;, \u0026#34;isShardBy\u0026#34;: false }, { \u0026#34;column\u0026#34;: \u0026#34;T_STUDENT.STU_DATEKEY\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;dict\u0026#34;, \u0026#34;isShardBy\u0026#34;: false }, { \u0026#34;column\u0026#34;: \u0026#34;T_STUDENT.STU_PARTKEY\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;dict\u0026#34;, \u0026#34;isShardBy\u0026#34;: false } ] }, \u0026#34;hbase_mapping\u0026#34;: { \u0026#34;column_family\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;F1\u0026#34;, \u0026#34;columns\u0026#34;: [ { \u0026#34;qualifier\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;measure_refs\u0026#34;: [ \u0026#34;_COUNT_\u0026#34;, \u0026#34;MIN_AGE\u0026#34;, \u0026#34;SUM_SCORE\u0026#34; ] } ] } ] }, \u0026#34;aggregation_groups\u0026#34;: [ { \u0026#34;includes\u0026#34;: [ \u0026#34;T_STUDENT.STU_NAME\u0026#34;, \u0026#34;T_STUDENT.STU_CLASS\u0026#34;, \u0026#34;T_STUDENT.STU_DATEKEY\u0026#34;, \u0026#34;T_STUDENT.STU_PARTKEY\u0026#34; ], \u0026#34;select_rule\u0026#34;: { \u0026#34;hierarchy_dims\u0026#34;: [], \u0026#34;mandatory_dims\u0026#34;: [], \u0026#34;joint_dims\u0026#34;: [] } } ], \u0026#34;signature\u0026#34;: \u0026#34;6LXWNhNdvJC74BdBNyGRNw==\u0026#34;, \u0026#34;notify_list\u0026#34;: [], \u0026#34;status_need_notify\u0026#34;: [ \u0026#34;ERROR\u0026#34;, \u0026#34;DISCARDED\u0026#34;, \u0026#34;SUCCEED\u0026#34; ], \u0026#34;partition_date_start\u0026#34;: 0, \u0026#34;partition_date_end\u0026#34;: 3153600000000, \u0026#34;auto_merge_time_ranges\u0026#34;: [ 604800000, 2419200000 ], \u0026#34;volatile_range\u0026#34;: 0, \u0026#34;retention_range\u0026#34;: 0, \u0026#34;engine_type\u0026#34;: 6, \u0026#34;storage_type\u0026#34;: 4, \u0026#34;override_kylin_properties\u0026#34;: {}, \u0026#34;cuboid_black_list\u0026#34;: [], \u0026#34;parent_forward\u0026#34;: 3, \u0026#34;mandatory_dimension_set_list\u0026#34;: [], \u0026#34;snapshot_table_desc_list\u0026#34;: [] } "},{"id":34,"href":"/posts/apache-kylin/ch01/","title":"Ch01-Kylin 介绍","section":"Blog","content":"Apache Kylin™是一个开源的、分布式的分析型数据仓库，提供 Hadoop/Spark 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由 eBay 开发并贡献至开源社区。它能在亚秒内查询巨大的表。\n1. 基本信息 # 条目 说明 官网 https://kylin.apache.org/ 下载地址 https://kylin.apache.org/download/ 2. Apache Kylin 介绍 # Kylin 对于解决的问题有以下假设：\n大数据查询要的一般是统计结果，是多条记录经过聚合函数计算后的统计值 原始的记录不是必需的，或者访问频率和概率都极低 聚合是按维度进行的，有意义的维度聚合组合也是相对有限的，一般不会随着数据的膨胀而膨胀 基于以上两点，可以得到一个新的思路—预计算，应尽量多地预先计算聚合结果，在查询时应该尽量利用预计算的结果得出查询结果，从而避免直接扫描可能无限增大的原始记录。\n自 Kylin 4.0.0 开始，不再默认使用 HBase，而是使用 Parquet 直接存储到 HDFS 上面。同时计算引擎也不再默认为 MapReduce，而是 Spark 本系列 Blog 全部基于 HBase 4.0.0 讲解\n如上图所示，Kylin 在架构设计上，可大体分为四个部分：数据源，构建 Cube 的计算引擎，存储引擎，对外查询接口。其中数据源主要是 Hive、Kafka；计算框架默认为 Spark；结果存储在 HDFS 中；对外查询接口支持 REST API、JDBC、ODBC。\n组件 说明 REST Server REST Server 是一套面向应用程序开发的入口点，旨在实现针对 Kylin 平台的应用开发工作。此类应用程序可以提供查询、获取结果、触发 cube 构建任务、获取元数据以及获取用户权限等等。另外可以通过 Restful 接口实现 SQL 查询。 Query Engine 当 cube 准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其他组件进行交互，从而向用户返回对应的结果。 Routing 负责将解析的 SQL 生成的执行计划转换成 cube 缓存的查询，cube 是通过预计算缓存在 HDFS 中，这部分查询可以在秒级甚至毫秒级完成，还有一些操作使用过的原始数据 (存储在 Hadoop 的 hdfs 中通过 hive 查询)，这部分查询延迟较高。 Metadata Manager Kylin 是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在 Kylin 当中的所有元数据进行管理，其中包括最为重要的 cube 元数据，其它全部组建的正常运作都需以元数据管理工具为基础，包括 cube 的定义、星状模型的定义、job 的信息、job 的输出信息、维度的 directory 信息等等，Kylin 的元数据和 cube 都存储在 HDFS 中。 Cube Build Engine 这套引擎的设计目的在于处理所有离线任务，其中包括 shell 脚本、Java API 以及 MapReduce 任务等等。任务引擎对 Kylin 当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决期间出现的障碍。 Storage Engine 这套引擎负责管理底层存储，特别是 cuboid，其以键值对的形式进行保存。存储引擎使用的是 HDFS，这是目前 Hadoop 生态系统当中最理想的键值系统使用方案。Kylin 还能够通过扩展实现对其它键值系统的支持，例如 Redis。 Cube Build Engine https://kylin.apache.org/download/ 3. 模型 # 数据仓库理论中存在多种方式来建立数据模型，比如星型模型、雪花模型等。\n模型 说明 星型模型 有一张事实表、以及零个或多个维度表；事实表与维度表通过 主键/外键 相关联，维度表之间没有关联，就像很多星星围绕在一个恒星周围，顾命名为星型模型。 雪花模型 如果将星型模型中某些维度的表再做规范，抽取成更细的维度表，然后让维度表之间也进行关联，那么这种模型成为雪花模型（雪花模型可以通过一定的转换，变为星型模型）。 "},{"id":35,"href":"/posts/apache-hbase/ch16/","title":"Ch16-HBase 之 Scan","section":"Blog","content":"HBase Scan\n1. Scan 涉及到的模块说明 # 1.1 Scan 操作的 RPC 消息结构 # message Scan { repeated Column column = 1; repeated NameBytesPair attribute = 2; optional bytes start_row = 3; optional bytes stop_row = 4; optional Filter filter = 5; optional TimeRange time_range = 6; optional uint32 max_versions = 7 [default = 1]; optional bool cache_blocks = 8 [default = true]; optional uint32 batch_size = 9; optional uint64 max_result_size = 10; optional uint32 store_limit = 11; optional uint32 store_offset = 12; optional bool load_column_families_on_demand = 13; optional bool small = 14 [deprecated = true]; optional bool reversed = 15 [default = false]; optional Consistency consistency = 16 [default = STRONG]; optional uint32 caching = 17; optional bool allow_partial_results = 18; repeated ColumnFamilyTimeRange cf_time_range = 19; optional uint64 mvcc_read_point = 20 [default = 0]; optional bool include_start_row = 21 [default = true]; optional bool include_stop_row = 22 [default = false]; enum ReadType { DEFAULT = 0; STREAM = 1; PREAD = 2; } optional ReadType readType = 23 [default = DEFAULT]; optional bool need_cursor_result = 24 [default = false]; } 2. Scan 操作的流程介绍 # scan 过程总体上是分层处理的，与存储上的组织方式一致，脉络比较清晰； 具体来说，就是region -\u0026gt; store -\u0026gt; hfile/memstore，分别都有对应的 scanner 实现进行数据读取；\nscan 请求本身设置的条件，以及 server 和 table 层面的一些参数限制，会根据需要分布在不同层次的 scanner 中进行处理；\n2.1 HBase Client 的流程 # 客户端首先会根据配置文件中 zookeeper 地址连接 zookeeper，并读取/\u0026lt;hbase-rootdir\u0026gt;/meta-region-server节点信息，该节点信息存储 HBase 元数据（hbase:meta）表所在的 RegionServer 地址以及访问端口等信息。用户可以通过 zookeeper 命令 (get /\u0026lt;hbase-rootdir\u0026gt;/meta-region-server) 查看该节点信息。 根据 hbase:meta 所在 RegionServer 的访问信息，客户端会将该元数据表加载到本地并进行缓存。然后在表中确定待检索 rowkey 所在的 RegionServer 信息。 根据数据所在 RegionServer 的访问信息，客户端会向该 RegionServer 发送真正的数据读取请求。 2.2 HBase RegionServer 的流程 # 2.2.1 RegionScanner # 创建指定的各个列族对应的 storeScanner，如果未指定则是全部列族； storeScanner 创建过程中，会根据 startrow 参数，seek 到对应 cell； 将这些 storeScanner 放入一个 heap 中，heap 为优先级队列，比较器的 compare 方法中比较的是 KeyValueScanner 所 peek 到的 cell 大小； 全部放入到 heap（因为 heap 是小根堆，所以一定有序），poll 方法得到最小的 storeScanner，并将其赋值给 current 然后调用 current 的 next 方法，获取一行的全部 cell，获取完成后 seek 到下一行； 再将 current 放入到 heap 中，形成新的顺序； 再不断重复 poll，add 方法，直到新的行大于或等于 r3。 2.2.2 StoreScanner # StoreScanner 的数据驱动方式与 RegionScanner 类似，也是使用 heap 和 current 去进行控制； 除了数据获取之外，该类比较重要的 1 个部分是数据的检查，相关逻辑封装在 ScanQueryMatcher 中； ScanQueryMatcher 中主要包含 2 个组件：DeleteTracker 和 ColumnTracker，前者负责处理 delete 逻辑，后者负责检查当前 cell 的 column、version 及 value 等是否符合要求； 另外，在 getScanners 的过程中会根据 keyRange、timeRange、bloomBlock 等对 storeFile 进行过滤，以减少数据的读取； 2.2.3 StoreFileScanner # storeFileScanner 是真正涉及到 hfile 数据读取的地方，会根据 rowKey，基于内存中 indexBlock 的数据定位到具体的 dataBlock 位置，以 block 为单位进行读取； 读取后的 block 数据在内存中以 ByteBuffer 的形式存在，而 blockSeek 方法会将这个 ByteBuffer 的 position 推动到合适位置的过程； 接下来，会读取一个 cell 的数据作为返回，使上层的 storeScanner 能够据此对各个 storeFileScanner 进行排序； 值得一提的是，实际实现中，还存在 lazySeek 的优化，大致原理是根据 hfile 中存储的最小 time，返回一个假的 cell，如果该 cell 都不能排在前面，那就不需要关心真实的 cell 是什么了，等到该 cell 能够排在最前面的时候，再进行 realSeek，这个机制对于各个 hfile 按时间存在明显分界并且主要读取近期数据的场景，可以有效减少实际的数据读取量； 注意：\n很多博客说 StoreFileScanner 和 MemStoreScanner，实际上自 HBase2.0.0 开始HBASE-17655，MemStoreScanner 和 SnapshotScanner 均被移除了。目前实际意义上的 MemStoreScanner 是由 DefaultMemStore 和 CompositeImmutableSegment 共同实现的。\n3. 参考文献 # HBase 原理－数据读取流程解析 HBase 原理－迟到的‘数据读取流程’部分细节 HBase scan 过程简析 HBase 加载 Hfile 时的读取过程 【HBase】HBase Scan \u0026amp; Filter原理/流程详解（1） 【HBase】HBase Scan \u0026amp; Filter原理/流程详解（2） 【HBase】HBase Scan \u0026amp; Filter原理/流程详解（3） 【HBase】HBase Scan \u0026amp; Filter原理/流程详解（4） Read 全流程 "},{"id":36,"href":"/posts/apache-hbase/ch15/","title":"Ch15-HBase 之 Put","section":"Blog","content":"HBase Put\n1. Put 涉及到的模块说明 # HBase 采用 LSM 树架构，天生适用于写多读少的应用场景。在真实生产线环境中，也正是因为 HBase 集群出色的写入能力，才能支持当下很多数据激增的业务。需要说明的是，HBase 服务端并没有提供 update、delete 接口，HBase 中对数据的更新、删除操作在服务器端也认为是写入操作，不同的是，更新操作会写入一个最新版本数据，删除操作会写入一条标记为 deleted 的 KV 数据。所以 HBase 中更新、删除操作的流程与写入流程完全一致。当然，HBase 数据写入的整个流程随着版本的迭代在不断优化，但总体流程变化不大。\n写入流程的三个阶段\n从整体架构的视角来看，写入流程可以概括为三个阶段。\n客户端处理阶段：客户端将用户的写入请求进行预处理，并根据集群元数据定位写入数据所在的 RegionServer，将请求发送给对应的 RegionServer。 Region 写入阶段：RegionServer 接收到写入请求之后将数据解析出来，首先写入 WAL，再写入对应 Region 列簇的 MemStore。 MemStore Flush 阶段：当 Region 中 MemStore 容量超过一定阈值，系统会异步执行 flush 操作，将内存中的数据写入文件，形成 HFile。 用户写入请求在完成 Region MemStore 的写入之后就会返回成功。MemStoreFlush 是一个异步执行的过程。\n2 HBase Client 的流程 # 2.1 基本流程 # HBase 客户端处理写入请求的核心流程基本上可以概括为三步。\n用户提交 put 请求后，HBase 客户端会将写入的数据添加到本地缓冲区中，符合一定条件就会通过 AsyncProcess 异步批量提交。HBase 默认设置 autoflush=true，表示 put 请求直接会提交给服务器进行处理；用户可以设置 autoflush=false，这样，put 请求会首先放到本地缓冲区，等到本地缓冲区大小超过一定阈值（默认为 2M，可以通过配置文件配置）之后才会提交。很显然，后者使用批量提交请求，可以极大地提升写入吞吐量，但是因为没有保护机制，如果客户端崩溃，会导致部分已经提交的数据丢失。 在提交之前，HBase 会在元数据表 hbase:meta 中根据 rowkey 找到它们归属的 RegionServer，这个定位的过程是通过 HConnection 的 locateRegion 方法 (下图中的 getRegionLocation 方法最终调用到的方法) 完成的。如果是批量请求，还会把这些 rowkey 按照 HRegionLocation 分组，不同分组的请求意味着发送到不同的 RegionServer，因此每个分组对应一次 RPC 请求。 HBase 会为每个 HRegionLocation 构造一个远程 RPC 请求 MultiServerCallable，并通过 rpcCallerFactory.newCaller() 执行调用。将请求经过 Protobuf 序列化后发送给对应的 RegionServer。 2.2 备注 # 图中的 meta cache 基于 CopyOnWriteArrayMap 实现。\n客户端根据写入的表以及 rowkey 在元数据缓存中查找，如果能够查找出该 rowkey 所在的 RegionServer 以及 Region，就可以直接发送写入请求（携带 Region 信息）到目标 RegionServer。 如果客户端缓存中没有查到对应的 rowkey 信息，需要首先到 ZooKeeper 上 /hbase/meta-region-server 节点查找 HBase 元数据表所在的 RegionServer。向 hbase:meta 所在的 RegionServer 发送查询请求，在元数据表中查找 rowkey 所在的 RegionServer 以及 Region 信息。客户端接收到返回结果之后会将结果缓存到本地，以备下次使用。 客户端根据 rowkey 相关元数据信息将写入请求发送给目标 RegionServer，RegionServer 接收到请求之后会解析出具体的 Region 信息，查到对应的 Region 对象，并将数据写入目标 Region 的 MemStore 中。 3. HBase Master 的流程 # 4. HBase RegionServer 的流程 # 数据写入 Region 的流程可以抽象为两步：追加写入 HLog，随机写入 MemStore\n4.1 HLog 追加写入 # HLog 保证成功写入 MemStore 中的数据不会因为进程异常退出或者机器宕机而丢失，但实际上并不完全如此，HBase 定义了多个 HLog 持久化等级，使得用户在数据高可靠和写入性能之间进行权衡。\n4.1.1 HLog 持久化等级 # HBase 可以通过设置 HLog 的持久化等级决定是否开启 HLog 机制以及 HLog 的落盘方式。HLog 的持久化等级分为如下五个等级。\n等级 说明 SKIP_WAL 只写缓存，不写 HLog 日志。因为只写内存，因此这种方式可以极大地提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。 SYNC_WAL 同步将数据写入日志文件中，需要注意的是，数据只是被写入文件系统中，并没有真正落盘。HDFS Flush 策略详见 HADOOP-6313。 FSYNC_WAL 同步将数据写入日志文件并强制落盘。这是最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。 USER_DEFAULT 如果用户没有指定持久化等级，默认 HBase 使用 SYNC_WAL 等级持久化数据。 put.setDurability(Durability.SYNC_WAL); 4.1.2 HLog 写入模型 # 在 HBase 的演进过程中，HLog 的写入模型几经改进，写入吞吐量得到极大提升。之前的版本中，HLog 写入都需要经过三个阶段：首先将数据写入本地缓存，然后将本地缓存写入文件系统，最后执行 sync 操作同步到磁盘。\n很显然，三个阶段是可以流水线工作的，基于这样的设想，写入模型自然就想到“生产者 - 消费者”队列实现。然而之前版本中，生产者之间、消费者之间以及生产者与消费者之间的线程同步都是由 HBase 系统实现，使用了大量的锁，在写入并发量非常大的情况下会频繁出现恶性抢占锁的问题，写入性能较差。\n当前版本中，HBase 使用 LMAX Disruptor 框架实现了无锁有界队列操作。基于 Disruptor 的 HLog 写入模型如图所示。\n图中最左侧部分是 Region 处理 HLog 写入的两个前后操作：append 和 sync。当调用 append 后，WALEdit 和 HLogKey 会被封装成 FSWALEntry 类，进而再封装成 RingBufferTruck 类放入 Disruptor 无锁有界队列中。当调用 sync 后，会生成一个 SyncFuture，再封装成 RingBufferTruck 类放入同一个队列中，然后工作线程会被阻塞，等待 notify() 来唤醒。\n图最右侧部分是消费者线程，在 Disruptor 框架中有且仅有一个消费者线程工作。这个框架会从 Disruptor 队列中依次取出 RingBufferTruck 对象，然后根据如下选项来操作：\n如果 RingBufferTruck 对象中封装的是 FSWALEntry，就会执行文件 append 操作，将记录追加写入 HDFS 文件中。需要注意的是，此时数据有可能并没有实际落盘，而只是写入到文件缓存。 如果 RingBufferTruck 对象是 SyncFuture，会调用线程池的线程异步地批量刷盘，刷盘成功之后唤醒工作线程完成 HLog 的 sync 操作。 4.2 Region 写入阶段 # 服务器端 RegionServer 接收到客户端的写入请求后，首先会反序列化为 put 对象，然后执行各种检查操作，比如检查 Region 是否是只读、MemStore 大小是否超过 blockingMemstoreSize 等。检查完成之后，执行一系列核心操作\nAcquire locks：HBase 中使用行锁保证对同一行数据的更新都是互斥操作，用以保证更新的原子性，要么更新成功，要么更新失败。 Update LATEST_TIMESTAMP timestamps：更新所有待写入（更新）KeyValue 的时间戳为当前系统时间。 Build WAL edit：HBase 使用 WAL 机制保证数据可靠性，即首先写日志再写缓存，即使发生宕机，也可以通过恢复 HLog 还原出原始数据。该步骤就是在内存中构建 WALEdit 对象，为了保证 Region 级别事务的写入原子性，一次写入操作中所有 KeyValue 会构建成一条 WALEdit 记录。 Append WALEdit To WAL：将步骤 3 中构造在内存中的 WALEdit 记录顺序写入 HLog 中，此时不需要执行 sync 操作。当前版本的 HBase 使用了 disruptor 实现了高效的生产者消费者队列，来实现 WAL 的追加写入操作。 Write back to MemStore：写入 WAL 之后再将数据写入 MemStore。 Release row locks：释放行锁。 Sync wal：HLog 真正 sync 到 HDFS，在释放行锁之后执行 sync 操作是为了尽量减少持锁时间，提升写性能。如果 sync 失败，执行回滚操作将 MemStore 中已经写入的数据移除。 结束写事务：此时该线程的更新操作才会对其他读请求可见，更新才实际生效。 4.3 MemStore Flush 阶段 # 4.3.1 触发条件 # 见HBase Cache 介绍，这里不再重复说明。\n4.3.2 执行流程 # 为了减少 flush 过程对读写的影响，HBase 采用了类似于两阶段提交的方式，将整个 flush 过程分为三个阶段。\n阶段 说明 prepare 遍历当前 Region 中的所有 MemStore，将 MemStore 中当前数据集 CellSkipListSet（内部实现采用 ConcurrentSkipListMap）做一个快照 snapshot，然后再新建一个 CellSkipListSet 接收新的数据写入。prepare 阶段需要添加 updateLock 对写请求阻塞，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。 flush 遍历所有 MemStore，将 prepare 阶段生成的 snapshot 持久化为临时文件，临时文件会统一放到目录.tmp 下。这个过程因为涉及磁盘 IO 操作，因此相对比较耗时。 commit 遍历所有的 MemStore，将 flush 阶段生成的临时文件移到指定的 ColumnFamily 目录下，针对 HFile 生成对应的 storefile 和 Reader，把 storefile 添加到 Store 的 storef iles 列表中，最后再清空 prepare 阶段生成的 snapshot。 4.4 生成 HFile # HFile 的数据结构见HBase HFile 介绍，这里不再重复说明。\nMemStore 中 KV 在 flush 成 HFile 时首先构建 Scanned Block 部分，即 KV 写进来之后先构建 Data Block 并依次写入文件，在形成 Data Block 的过程中也会依次构建形成 Leaf index Block、Bloom Block 并依次写入文件。一旦 MemStore 中所有 KV 都写入完成，Scanned Block 部分就构建完成。 Non-scanned Block、Load-on-open 以及 Trailer 这三部分是在所有 KV 数据完成写入后再追加写入的。\n4.4.1 Scanned Block 构建 # MemStore 执行 flush，首先新建一个 Scanner，这个 Scanner 从存储 KV 数据的 CellSkipListSet 中依次从小到大读出每个 cell（KeyValue）。这里必须注意读取的顺序性，读取的顺序性保证了 HFile 文件中数据存储的顺序性，同时读取的顺序性是保证 HFile 索引构建以及布隆过滤器 Meta Block 构建的前提。 appendGeneralBloomFilter：在内存中使用布隆过滤器算法构建 BloomBlock，下文也称为 Bloom Chunk。 appendDeleteFamilyBloomFilter：针对标记为\u0026quot;DeleteFamily\u0026quot;或者\u0026quot;DeleteFamilyVersion\u0026quot;的 cell，在内存中使用布隆过滤器算法构建 BloomBlock，基本流程和 appendGeneralBloomFilter 相同。 (HFile.Writer)writer.append：将 cell 写入 Data Block 中，这是 HFile 文件构建的核心。 4.4.2 Bloom Block 构建 # 图为 Bloom Block 构建示意图，实际实现中使用 chunk 表示 Block 概念，两者等价。\n布隆过滤器内存中维护了多个称为 chunk 的数据结构，一个 chunk 主要由两个元素组成：\n一块连续的内存区域，主要存储一个特定长度的数组。默认数组中所有位都为 0，对于 row 类型的布隆过滤器，cell 进来之后会对其 rowkey 执行 hash 映射，将其映射到位数组的某一位，该位的值修改为 1。 firstkey，第一个写入该 chunk 的 cell 的 rowkey，用来构建 Bloom IndexBlock。 cell 写进来之后，首先判断当前 chunk 是否已经写满，写满的标准是这个 chunk 容纳的 cell 个数是否超过阈值。如果超过阈值，就会重新申请一个新的 chunk，并将当前 chunk 放入 ready chunks 集合中。如果没有写满，则根据布隆过滤器算法使用多个 hash 函数分别对 cell 的 rowkey 进行映射，并将相应的位数组位置为 1。\n4.4.3 Data Block 构建 # 一个 cell 在内存中生成对应的布隆过滤器信息之后就会写入 Data Block，写入过程分为两步。\nEncoding KeyValue：使用特定的编码对 cell 进行编码处理，HBase 中主要的编码器有 DiffKeyDeltaEncoder、FastDiffDeltaEncoder 以及 PrefixKeyDeltaEncoder 等。编码的基本思路是，根据上一个 KeyValue 和当前 KeyValue 比较之后取 delta，展开讲就是 rowkey、column family 以及 column 分别进行比较然后取 delta。假如前后两个 KeyValue 的 rowkey 相同，当前 rowkey 就可以使用特定的一个 f lag 标记，不需要再完整地存储整个 rowkey。这样，在某些场景下可以极大地减少存储空间。 将编码后的 KeyValue 写入 DataOutputStream。 随着 cell 的不断写入，当前 Data Block 会因为大小超过阈值（默认 64KB）而写满。写满后 Data Block 会将 DataOutputStream 的数据 f lush 到文件，该 Data Block 此时完成落盘。\n4.4.4 Leaf Index Block 构建 # Data Block 完成落盘之后会立刻在内存中构建一个 Leaf Index Entry 对象，并将该对象加入到当前 Leaf Index Block。Leaf Index Entry 对象有三个重要的字段。\nfirstKey：落盘 Data Block 的第一个 key。用来作为索引节点的实际内容，在索引树执行索引查找的时候使用。 blockOffset：落盘 Data Block 在 HFile 文件中的偏移量。用于索引目标确定后快速定位目标 Data Block。 blockDataSize：落盘 Data Block 的大小。用于定位到 Data Block 之后的数据加载。 Leaf Index Block 会随着 Leaf Index Entry 的不断写入慢慢变大，一旦大小超过阈值（默认 64KB），就需要 f lush 到文件执行落盘。需要注意的是，LeafIndex Block 落盘是追加写入文件的，所以就会形成 HFile 中 Data Block、LeafIndex Block 交叉出现的情况。\n和 Data Block 落盘流程一样，Leaf Index Block 落盘之后还需要再往上构建 RootIndex Entry 并写入 Root Index Block，形成索引树的根节点。但是根节点并没有追加写入\u0026quot;Scanned block\u0026quot;部分，而是在最后写入\u0026quot;Load-on-open\u0026quot;部分。\n可以看出，HFile 文件中索引树的构建是由低向上发展的，先生成 Data Block，再生成 Leaf Index Block，最后生成 Root Index Block。而检索 rowkey 时刚好相反，先在 Root Index Block 中查询定位到某个 Leaf Index Block，再在 Leaf IndexBlock 中二分查找定位到某个 Data Block，最后将 Data Block 加载到内存进行遍历查找。\n4.4.5 Bloom Block Index 构建 # 完成 Data Block 落盘还有一件非常重要的事情：检查是否有已经写满的 BloomBlock。如果有，将该 Bloom Block 追加写入文件，在内存中构建一个 BloomIndex Entry 并写入 Bloom Index Block。\n整个流程与 Data Block 落盘后构建 Leaf Index Entry 并写入 Leaf Index Block 的流程完全一样。在此不再赘述。\n4.5 流程总结 # flush 阶段生成 HFile 和 Compaction 阶段生成 HFile 的流程完全相同，不同的是，flush 读取的是 MemStore 中的 KeyValue 写成 HFile，而 Compaction 读取的是多个 HFile 中的 KeyValue 写成一个大的 HFile，KeyValue 来源不同。KeyValue 数据生成 HFile，首先会构建 Bloom Block 以及 Data Block，一旦写满一个 Data Block 就会将其落盘同时构造一个 Leaf Index Entry，写入 LeafIndex Block，直至 Leaf Index Block 写满落盘。实际上，每写入一个 KeyValue 就会动态地去构建\u0026quot;Scanned Block\u0026quot;部分，等所有的 KeyValue 都写入完成之后再静态地构建\u0026quot;Non-scanned Block\u0026quot;部分、\u0026ldquo;Load on open\u0026quot;部分以及\u0026quot;Trailer\u0026quot;部分。\n5. 参考文献 # miniHBase HBase 原理与实践 HBase－数据写入流程解析 高性能队列——Disruptor "},{"id":37,"href":"/posts/apache-hbase/ch14/","title":"Ch14-HBase 之 CreateTable","section":"Blog","content":"HBase CreateTable\n1. CreateTable 涉及到的模块说明 # 创建表的整个流程主要分为两大块，一大块是 Client 端，主要用于构造 RPC 请求，另一大块是 Master 端，主要负责实际的建表流程。\n整个流程借助上图还是比较容易理解的。因为 Procedure v2 框架的引入，客户端根据需要仅仅发送请求便可，然后实际的建表就交给 Procedure v2 进行实际的操作。\n1.1 创建表的 RPC 消息结构 # message TableSchema { optional TableName table_name = 1; repeated BytesBytesPair attributes = 2; repeated ColumnFamilySchema column_families = 3; repeated NameStringPair configuration = 4; } message CreateTableRequest { required TableSchema table_schema = 1; repeated bytes split_keys = 2; optional uint64 nonce_group = 3 [default = 0]; optional uint64 nonce = 4 [default = 0]; } message CreateTableResponse { optional uint64 proc_id = 1; } service MasterService { /** Creates a new table asynchronously */ rpc CreateTable(CreateTableRequest) returns(CreateTableResponse); /** Deletes a table */ rpc DeleteTable(DeleteTableRequest) returns(DeleteTableResponse); } 2. 创建表的流程介绍 # public boolean create(TableDescriptor table) { try (Admin admin = connection.getAdmin()) { admin.createTable(table); } catch (IOException e) { return false; } return true; } 整个建表的代码段大概如上所示，看起来比较简单，但是实际整个流程异常的繁琐。\n2.1 HBase Client 的流程 # 当调用 admin.createTable(table) 语句创建表时，会通过层层调用，最后由 MasterService 调用 BlockingInterface 发送 rpc 请求，请求的报文体则如前面 Protobuf 给出的结构。 HMaster 接收到该请求后会执行后续逻辑，同时会返回一个 proc Id，该 id 最终会被封装成 HBaseAdmin.CreateTableFuture 类，定时查询 proc Id 对应的 Procedure 执行结果，直到最后建表成功。\n简易版的如上图所示，完整版的如下图所示：\n2.2 HBase Master 的流程 # HBase Master 的流程较为繁琐，主要负责执行 Procedure。具体 Procedure 的工作原理，本节不再赘述，可以参考《HBase Procedure v2 原理说明》一节。这里直接开始讲述 Procedure 如何执行。\nProcedure 从 MasterProcedureScheduler 维护的 tableRunQueue 队列中取出 CreateTableProcedure 开始调用执行其中的 executeFromState() 方法。这部分代码即整个建表的核心逻辑。\n@Override protected Flow executeFromState(final MasterProcedureEnv env, final CreateTableState state) throws InterruptedException { LOG.info(\u0026#34;{} execute state={}\u0026#34;, this, state); try { switch (state) { case CREATE_TABLE_PRE_OPERATION: ... preCreate(env); setNextState(CreateTableState.CREATE_TABLE_WRITE_FS_LAYOUT); break; case CREATE_TABLE_WRITE_FS_LAYOUT: ... env.getMasterServices().getTableDescriptors().update(tableDescriptor, true); setNextState(CreateTableState.CREATE_TABLE_ADD_TO_META); break; case CREATE_TABLE_ADD_TO_META: newRegions = addTableToMeta(env, tableDescriptor, newRegions); setNextState(CreateTableState.CREATE_TABLE_ASSIGN_REGIONS); break; case CREATE_TABLE_ASSIGN_REGIONS: setEnablingState(env, getTableName()); addChildProcedure(env.getAssignmentManager().createRoundRobinAssignProcedures(newRegions)); setNextState(CreateTableState.CREATE_TABLE_UPDATE_DESC_CACHE); break; case CREATE_TABLE_UPDATE_DESC_CACHE: setEnabledState(env, getTableName()); setNextState(CreateTableState.CREATE_TABLE_POST_OPERATION); break; case CREATE_TABLE_POST_OPERATION: postCreate(env); return Flow.NO_MORE_STATE; default: throw new UnsupportedOperationException(\u0026#34;unhandled state=\u0026#34; + state); } } catch (IOException e) { if (isRollbackSupported(state)) { setFailure(\u0026#34;master-create-table\u0026#34;, e); } else { LOG.warn(\u0026#34;Retriable error trying to create table=\u0026#34; + getTableName() + \u0026#34; state=\u0026#34; + state, e); } } return Flow.HAS_MORE_STATE; } 上述代码转换成流程图如下所示，可以看到因为 Procedure v2 的存在，整个建表流程被拆的非常琐碎，如果途中出现了异常，那么会在 Failure 中将本次 Procedure 的状态设置为 FAILED，后续尝试回滚截至该步骤的所有操作所产生的行为。\n2.2 HBase RegionServer 的流程 # HBase Master 建表流程走到CREATE_TABLE_ASSIGN_REGIONS这一步的时候，会创建出一个 SubProcedure，该 SubProcedure 会调用CREATE_TABLE_ASSIGN_REGIONS将 Region 分配到各个 RegionServer 上面。\n3. 参考文献 # HBASE-12439\n"},{"id":38,"href":"/posts/apache-hbase/ch13/","title":"Ch13-HBase 之 RPC","section":"Blog","content":"HBase RPC\n1. 引入 RPC # 1.1 RPC 的基本概念 # RPC（Remote Procedure Call）远程过程调用，它是一种技术思想，而非规范或者协议。从效果上来看，A 节点应用可以像调用本地方法一样调用 B 节点服务的方法。为了达到这样的目的，需要解决通讯问题，序列化和反序列化问题，寻址问题等等。\n上图中的 Stub 类似于一个代理，一般的使用方法是response = stub.scan(controller, request)，比如在 A 节点的应用中使用stub.scan(...)实际上调用的就是 B 节点上指定服务的方法。\nRPC 的实现需要解决序列化和反序列化问题，在日常的开发中，这一块借由 protobuf 定义的 message 实现；寻址问题则借 protobuf 定义的 service 实现；通讯问题借助 Netty 实现。\n2. RPC 涉及到的模块说明 # HBase 整个 RPC 框架实现逻辑较为繁琐，不过依旧没有偏离上述的实现逻辑。\n2.1 RPC 操作的 RPC 消息结构 # 3. HBase RPC 操作的流程介绍 # 4. HBase Client 的流程 # 上图的流程被绿色和蓝色的线分割成了三部分，最左边的一部分完成了调度执行的功能，中间的部分完成了服务代理的功能，最右边的部分完成了通信模块的功能。\n4.2 调度执行 # 该模块主要提供接口转换、错误重试、服务分组等能力；\n名称 说明 接口转换 服务层定义的服务接口与用户层不同，比如 put/delete/increment/append 等操作底层都是调用的 mutate 接口，而 batch 相关的操作，无论是读还是写都调用 multi 接口；转换逻辑封装为一个 Callable 对象，交由 RpcRetryingCaller 处理 错误重试 RpcRetryingCaller 负责与服务代理模块直接交互，以及错误时的重试 服务分组 batch 相关的操作可能会涉及到多个 RegionServer，需要按照 RegionServer 进行分组，然后多线程并发请求，这些逻辑是在 AsyncProcess 中；对于非 batch 类请求则直接使用 RpcRetryingCaller 进行调用，AsyncProcess 的内部实际上也是依赖了该类来执行单个 RegionServer 请求 4.3 服务代理 # 服务代理实现了与服务端同样的接口； 对调度执行模块而言，调用 stub 的方法就相当于调用远程的服务，而不必关心实现细节；\n这部分依赖 protobuf 组件，通过在 proto 文件中定义 service 及 message 类型的参数，可直接生成接口和 stub 实现类；\n在 ConnectionImplementation 类中有一个 Map 类型的 stubs 变量，其 key 为 service name + regionserver，value 则是 stub 实例；\n4.4 通信模块 # 该模块主要进行序列化和 io 处理；\n目前 HBase 已采用 netty 作为底层的 io 框架，客户端的核心类为 NettyRpcClient；\n服务代理层的 stub 类中含有一个 BlockingRpcChannel 类型的变量，而 rpcClient 通过实现该接口并将实例注入来与之对接；\n序列化则是依赖 protobuf 组件，序列化与反序列化的逻辑都放在 NettyRpcDuplexHandler 中，该类注册在 netty 的 pipeline，会基于不同的事件自动调用；\n5. HBase RegionServer 的流程 # BlockingQueue\u0026lt;CallRunner\u0026gt; q = new LinkedBlockingQueue\u0026lt;\u0026gt;()\n上图的流程被绿色和蓝色的线分割成了三部分，最左边的一部分完成了通信模块的功能，中间的部分完成了调度执行的功能，最右边的部分完成了无复实现的功能。\n5.1 通信模块 # 该模块主要负责数据的读取、反序列化并封装为 call 对象；\n核心实现类为 NettyRpcServer，通过在 pipeline 中注册的一些 handler 来完成上述处理；\n5.2 调度执行 # 通信模块得到的 call 对象会交由 rpcScheduler 进行调度，目前默认实现为 SimpleRpcScheduler；\nrpcScheduler 的主要作用是根据请求类型把请求分配给不同的 rpcExecutor 实例，请求类型有 3 种：普通请求、高优先级请求和 replication 请求，而 rpcExecutor 的实现目前主要由 RWQueueRpcExecutor 和 FastPathBalancedQueueRpcExecutor 两种，不同的类型使用了不同实现，关系如下：\nRWQueueRpcExecutor 的特点是内部可以对读写隔离，以及对 get 和 scan 隔离，所谓隔离的意思是，call 对象会放入独立的 callQueue，并使用独立的 handler 进行处理；\n5.3 服务实现 # 服务端实现类需要实现一些接口，例如 AdminService.BlockingInterface、ClientService.BlockingInterface、MasterService.BlockingInterface 等；\nHMaster 的服务实现类主要是 MasterRpcServices，HRegionServer 的服务实现类主要是 RSRpcServices；\nservice 相关的类会在启动阶段进行初始化，然后在请求处理时根据 connection 的 serviceName 获取到对应的 service 实例，再根据 call 对象的 method 和 param 进行方法的调用；\n6. HBase Master 的流程 # Master 做为一种特殊的 RegionServer，整个流程与 RegionServer 类似，唯独不同的是服务实现类不同，Master 的实现类是 MasterRpcServices。\n7. 参考文献 # Remote procedure call HBase rpc 框架介绍 8. 附件 # 8.1 RWQueueRpcExecutor # RWQueueRpcExecutor 的特点是内部可以对读写隔离，以及对 get 和 scan 隔离，所谓隔离的意思是，call 对象会放入独立的 callQueue，并使用独立的 handler 进行处理；\n8.2 FastPathBalancedQueueRpcExecutor # FastPathBalancedQueueRpcExecutor 不支持隔离，其特点是对于空闲的 handler，让其自旋而不是阻塞，以减少线程上下文切换的消耗；\n"},{"id":39,"href":"/posts/apache-hbase/ch12/","title":"Ch12-HBase 之 Region Split","section":"Blog","content":"HBase Region Split 整个过程可以分为如下几步，触发 Region Split，寻找 SplitPoint，拆分 Region，等待 major compaction 删除旧的 Region\n1. 触发 Region Split # 触发 Region Split 有好几种策略，比如除了 3 种默认过的策略，还有 DelimitedKeyPrefixRegionSplitPolicy、KeyPrefixRegionSplitPolicy、DisableSplitPolicy 等策略，这里只介绍 3 种默认的策略。分别是 ConstantSizeRegionSplitPolicy 策略、IncreasingToUpperBoundRegionSplitPolicy 策略和 SteppingSplitPolicy 策略。\n1.1 ConstantSizeRegionSplitPolicy # ConstantSizeRegionSplitPolicy 策略是 0.94 版本之前的默认拆分策略，这个策略的拆分规则是：当 region 大小达到 hbase.hregion.max.filesize（默认 10G）后拆分。 这种拆分策略对于小表不太友好，按照默认的设置，如果 1 个表的 Hfile 小于 10G 就一直不会拆分。注意 10G 是压缩后的大小，如果使用了压缩的话。\n如果 1 个表一直不拆分，访问量小也不会有问题，但是如果这个表访问量比较大的话，就比较容易出现性能问题。这个时候只能手工进行拆分。还是很不方便。\n1.2 IncreasingToUpperBoundRegionSplitPolicy # 这种切分策略微微有些复杂，总体来看和 ConstantSizeRegionSplitPolicy 思路相同，一个 region 中最大 store 大小大于设置阈值就会触发切分。但是这个阈值并不像 ConstantSizeRegionSplitPolicy 是一个固定的值，而是会在一定条件下不断调整，调整规则和 region 所属表在当前 regionserver 上的 region 个数有关系：(#regions) * (#regions) * (#regions) * flush size * 2，当然阈值并不会无限增大，最大值为用户设置的 MaxRegionFileSize。\n从这个算是我们可以得出 flushsize 为 128M、maxFileSize 为 10G 的情况下，可以计算出 Region 的分裂情况如下：\n第一次拆分大小为：min(10G，11128M)=128M 第二次拆分大小为：min(10G，33128M)=1152M 第三次拆分大小为：min(10G，55128M)=3200M 第四次拆分大小为：min(10G，77128M)=6272M 第五次拆分大小为：min(10G，99128M)=10G 第五次拆分大小为：min(10G，1111128M)=10G 1.3 SteppingSplitPolicy # SteppingSplitPolicy 是在 Hbase 2.0 版本后的默认策略，，拆分规则为：If region=1 then: flush size * 2 else: MaxRegionFileSize。\n还是以 flushsize 为 128M、maxFileSize 为 10 场景为列，计算出 Region 的分裂情况如下：\n第一次拆分大小为：2*128M=256M 第二次拆分大小为：10G 从上面的计算我们可以看出，这种策略兼顾了 ConstantSizeRegionSplitPolicy 策略和 IncreasingToUpperBoundRegionSplitPolicy 策略，对于小表也肯呢个比较好的适配。\n从上面的计算我们可以看到这种策略能够自适应大表和小表，但是这种策略会导致小表产生比较多的小 region，对于小表还是不是很完美。\n2. 寻找 SplitPoint # SplitPoint 规定为整个 region 中最大 store 中的最大文件中最中心的一个 block 的首个 rowkey。如果定位到的 rowkey 是整个文件的首个 rowkey 或者最后一个 rowkey 的话，就认为没有切分点。\n3. 拆分 Region # 从上图我们可以看出 Region 切分的详细流程如下：\n会 ZK 的 /hbase/region-in-transition/region-name 下创建一个 znode，并设置状态为 SPLITTING master 通过 watch 节点检测到 Region 状态的变化，并修改内存中 Region 状态的变化 RegionServer 在父 Region 的目录下创建一个名称为 .splits 的子目录 RegionServer 关闭父 Region，强制将数据刷新到磁盘，并这个 Region 标记为 offline 的状态。此时，落到这个 Region 的请求都会返回 NotServingRegionException 这个错误 RegionServer 在 .splits 创建 daughterA 和 daughterB，并在文件夹中创建对应的 reference 文件，指向父 Region 的 Region 文件 RegionServer 在 HDFS 中创建 daughterA 和 daughterB 的 Region 目录，并将 reference 文件移动到对应的 Region 目录中 在 hbase:meta 表中设置父 Region 为 offline 状态，不再提供服务，并将父 Region 的 daughterA 和 daughterB 的 Region 添加到 hbase:meta 表中，已表名父 Region 被拆分成了 daughterA 和 daughterB 两个 Region RegionServer 并行开启两个子 Region，并正式提供对外写服务 RegionSever 将 daughterA 和 daughterB 添加到 hbase:meta 表中，这样就可以从 hbase:meta 找到子 Region，并可以对子 Region 进行访问了 RegionServr 修改 /hbase/region-in-transition/region-name 的 znode 的状态为 SPLIT 备注：\n为了减少对业务的影响，Region 的拆分并不涉及到数据迁移的操作，而只是创建了对父 Region 的指向。只有在做大合并的时候，才会将数据进行迁移。\n4. 其他问题 # 4.1 reference 文件查找流程 # 根据文件名来判断是否是 reference 文件 由于 reference 文件的命名规则为前半部分为父 Region 对应的 File 的文件名，后半部分是父 Region 的名称，因此读取的时候也根据前半部分和后半部分来识别 根据 reference 文件的内容来确定扫描的范围，reference 的内容包含两部分，一部分是切分点 splitkey，另一部分是 boolean 类型的变量（true 或者 false）。如果为 true 则扫描文件的上半部分，false 则扫描文件的下半部分 接下来确定了扫描的文件，以及文件的扫描范围，那就按照正常的文件检索了 4.2 父 region 的数据什么时候会迁移到子 region 目录？ # 子 region 发生 major_compaction 时。我们知道 compaction 的执行实际上是将 store 中所有小文件一个 KV 一个 KV 从小到大读出来之后再顺序写入一个大文件，完成之后再将小文件删掉，因此 compaction 本身就需要读取并写入大量数据。子 region 执行 major_compaction 后会将父目录中属于该子 region 的所有数据读出来并写入子 region 目录数据文件中。可见将数据迁移放到 compaction 这个阶段来做，是一件顺便的事。\n4.3 父 region 什么时候会被删除？ # 实际上 HMaster 会启动一个线程定期遍历检查所有处于 splitting 状态的父 region，确定检查父 region 是否可以被清理。检测线程首先会在 meta 表中揪出所有 split 列为 true 的 region，并加载出其分裂后生成的两个子 region（meta 表中 splitA 列和 splitB 列），只需要检查此两个子 region 是否还存在引用文件，如果都不存在引用文件就可以认为该父 region 对应的文件可以被删除。\n4.4 Region 切分事务性如何保证？ # HBase 2.0 之前基于状态机保证（未完全保证，仍可能会存在大量 RIT），HBase 2.0 之后基于内部分布式事务框架 Procedure V2(HBASE-12439) 保证。\n5. 参考文献 # HBase 原理 – 所有 Region 切分的细节都在这里了 "},{"id":40,"href":"/posts/apache-hbase/ch11/","title":"Ch11-HBase 之 Region Compaction","section":"Blog","content":"HBase Region Compaction\n1. Compaction 涉及到的模块说明 # 1.1 Compaction 种类 # Compaction 会从一个 region 的一个 store 中选择一些 hfile 文件进行合并。合并说来原理很简单，先从这些待合并的数据文件中读出 KeyValues，再按照由小到大排列后写入一个新的文件中。之后，这个新生成的文件就会取代之前待合并的所有文件对外提供服务。HBase 根据合并规模将 Compaction 分为了两类：MinorCompaction 和 MajorCompaction。\n1.2 Minor Compaction # 选取一些小的、相邻的 StoreFile 将他们合并成一个更大的 StoreFile，在这个过程中不会处理已经 Deleted 或 Expired 的 Cell。一次 Minor Compaction 的结果是更少并且更大的 StoreFile。\n1.3 Major Compaction # 将所有的 StoreFile 合并成一个 StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL 过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction 时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发 Major Compaction 功能，改为手动在业务低峰期触发。\n2. Compaction 的流程介绍 # Compaction 的整个流程中涉及到了非常重要的几个组件，StoreEngine，CompactionContext，CompactionPolicy，Compactor。\nCompactionContext 决定了 StoreFile 的选取策略 (CompactionPolicy)，也决定了 StoreFile 的合并策略 (Compactor)。而 CompactionContext 最终由 StoreEngine 初始化（读取配置，通过反射的方式创建对应的类）。\nhbase.hstore.defaultengine.storeflusher.class=DefaultStoreFlusher.class hbase.hstore.engine.class=DefaultStoreEngine.class hbase.hstore.defaultengine.compactor.class=DefaultCompactor.class hbase.hstore.defaultengine.compactionpolicy.class=ExploringCompactionPolicy.class CompactionContext 是个抽象类，有三个非常重要的方法preSelect，select，compact。\npreSelect 方法在 coprocessor 的 preCompactSelection 方法之间调用，主要是为 coprocessor 过滤一些 storefile（比如排除掉那些明显本次就不会被 compac 的文件）。 select 方法由 compaction 选择文件时调用，一般会依赖具体的算法实现，最后与 compact 方法配合使用。\n2.1 Compaction 触发时机 # HBase 中可以触发 compaction 的因素有很多，最常见的因素有这么三种：MemStore Flush、后台线程周期性检查、手动触发。\n2.2 MemStore Flush # 应该说 compaction 操作的源头就来自 flush 操作，memstore flush 会产生 HFile 文件，文件越来越多就需要 compact。因此在每次执行完 Flush 操作之后，都会对当前 Store 中的文件数进行判断，一旦文件数大于等于 hbase.hstore.compactionThreshold，就会触发 compaction。需要说明的是，compaction 都是以 Store 为单位进行的，而在 Flush 触发条件下，整个 Region 的所有 Store 都会执行 compact，所以会在短时间内执行多次 compaction。\n2.3 后台线程周期性检查 # 后台线程 CompactionChecker 定期触发检查是否需要执行 compaction，检查周期为：hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multiplier。和 flush 不同的是，该线程优先检查文件数是否大于 hbase.hstore.compactionThreshold，一旦大于就会触发 compaction。如果不满足，它会接着检查是否满足 major compaction 条件，简单来说，如果当前 store 中 hfile 的最早更新时间早于某个值 mcTime，就会触发 major compaction，HBase 预想通过这种机制定期删除过期数据。\n2.4 手动触发 # 一般来讲，手动触发 compaction 通常是为了执行 major compaction，原因有三\n其一是因为很多业务担心自动 major compaction 影响读写性能，因此会选择低峰期手动触发； 其二也有可能是用户在执行完 alter 操作之后希望立刻生效，执行手动触发 major compaction； 其三是 HBase 管理员发现硬盘容量不够的情况下手动触发 major compaction 删除大量过期数据； 无论哪种触发动机，一旦手动触发，HBase 会不做很多自动化检查，直接执行合并。\n3. 选择合适 HFile 合并 # 3.1 RatioCompactionPolicy # 从老到新逐一扫描所有候选文件，满足其中条件之一便停止扫描：\n当前文件大小 \u0026lt; 比它更新的所有文件大小总和 * ratio，其中 ratio 是一个可变的比例，在高峰期时 ratio 为 1.2，非高峰期为 5，也就是非高峰期允许 compact 更大的文件。那什么时候是高峰期，什么时候是非高峰期呢？用户可以配置参数 hbase.offpeak.start.hour 和 hbase.offpeak.end.hour 来设置高峰期 当前所剩候选文件数 \u0026lt;= hbase.store.compaction.min（默认为3），停止扫描后，待合并文件就选择出来了，即为当前扫描文件 + 比它更新的所有文件 3.2 ExploringCompactionPolicy # 该策略思路基本和 RatioBasedCompactionPolicy 相同，不同的是，Ratio 策略在找到一个合适的文件集合之后就停止扫描了，而 Exploring 策略会记录下所有合适的文件集合，并在这些文件集合中寻找最优解。最优解可以理解为：待合并文件数最多或者待合并文件数相同的情况下文件大小较小，这样有利于减少 compaction 带来的 IO 消耗。\n3.3 StripCompactionPolicy # 3.4 用户自定义 Policy # 4. 挑选合适的执行线程池 # HBase 实现中有一个专门的线程 CompactSplitThead 负责接收 compact 请求以及 split 请求，而且为了能够独立处理这些请求，这个线程内部构造了多个线程池：largeCompactions、smallCompactions以及 splits 等，其中 splits 线程池负责处理所有的 split 请求，largeCompactions 和 smallCompaction 负责处理所有的 compaction 请求，其中前者用来处理大规模 compaction，后者处理小规模 compaction。\n注意\n待 compact 的文件总大小如果大于值 throttlePoint（可以通过参数 hbase.regionserver.thread.compaction.throttle 配置，默认为 2.5G），分配给 largeCompactions 处理，否则分配给 smallCompactions 处理。所以并不是 largeCompactions 就处理 Major Compaction，而 smallCompactions 就去处理 Minor Compaction。 largeCompactions 线程池和 smallCompactions 线程池默认都只有一个线程，用户可以通过参数 hbase.regionserver.thread.compaction.large 和 hbase.regionserver.thread.compaction.small 进行配置 5. HFile 文件合并 # 上文一方面选出了待合并的 HFile 集合，一方面也选出来了合适的处理线程，万事俱备，只欠最后真正的合并。合并流程说起来也简单HBASE-2231，主要分为如下几步：\n分别读出待合并 hfile 文件的 KV，并顺序写到位于 regionName/tmp 目录下的临时文件中 将临时文件移动到对应 region 的数据目录 将 compaction 的输入文件路径和输出文件路径封装为 KV 写入 WAL 日志，并打上 compaction 标记，最后强制执行 sync 将对应 region 数据目录下的 compaction 输入文件全部删除 注意\n上述四个步骤看起来简单，但实际是很严谨的，具有很强的容错性和完美的幂等性：\n如果 RS 在步骤 2 之前发生异常，本次 compaction 会被认为失败，如果继续进行同样的 compaction，上次异常对接下来的 compaction 不会有任何影响，也不会对读写有任何影响。唯一的影响就是多了一份多余的数据。 如果 RS 在步骤 2 之后、步骤 3 之前发生异常，同样的，仅仅会多一份冗余数据。 如果在步骤 3 之后、步骤 4 之前发生异常，RS 在重新打开 region 之后首先会从 WAL 中看到标有 compaction 的日志，因为此时输入文件和输出文件已经持久化到 HDFS，因此只需要根据 WAL 移除掉 compaction 输入文件即可 6. 参考文献 # HBase Compaction 的前生今世－身世之旅 HBase Compaction 策略 Flush 与 Compaction "},{"id":41,"href":"/posts/apache-hbase/ch10/","title":"Ch10-HBase 之 Procedure v2","section":"Blog","content":"HBase Procedure V2\n1. HBase Procedure v2 之 Procedure # 1.2 Procedure 类型 # 名称 说明 Meta Procedure execute(), rollback() Server Procedure 唯一的一种类型为 ServerCrashProcedure，用来负责 RegionServer 进程故障后的处理 Peer Procedure 与 Replication 相关，如 AddPeerProcedure, RemovePeerProcedure 等等 Table Procedure 类型最为丰富，如 CreateTableProcedure, DisableTableProcedure, EnableTableProcedure, AssignProcedure, SplitTableRegionProcedure, \u0026hellip;涵盖表级别、Region 级别的各类操作。 在 ProcedureScheduler 中，需要同时调度这几种类型的 Procedure，调度的优先级顺序 (由高到低) 为：\nMeta -\u0026gt; Server -\u0026gt; Peer -\u0026gt; Table。\n1.3 Procedure 状态 # 名称 说明 INITIALIZING Procedure in construction, not yet added to the executor RUNNABLE Procedure added to the executor, and ready to be executed WAITING The procedure is waiting on children to be completed WAITING_TIMEOUT The procedure is waiting a timout or an external event ROLLEDBACK The procedure failed and was rolledback SUCCESS The procedure execution is completed successfully. FAILED The procedure execution is failed, may need to rollback 根据 Procedure 的生命周期，整个状态切换大概如下图所示。\n1.4 Procedure 的实现 # 1.4.1 Procedure 的实现体系 # Procedure 的实现非常丰富，这里仅仅举出几个常见的 Procedure 的继承关系。\n其中最常见的就是 StateMachineProcedure，对于所有执行存在先后顺序，且存在状态切换的 Procedure 实现都会继承此类，同时这些 Procedure 也会定义一些自己的状态，比如 MasterProcedure.proto 中定义了 CreateTableState，CreateTableState 又进一步定义了该 Procedure 在调度中会存在下述定义的状态。\nenum CreateTableState { CREATE_TABLE_PRE_OPERATION = 1; CREATE_TABLE_WRITE_FS_LAYOUT = 2; CREATE_TABLE_ADD_TO_META = 3; CREATE_TABLE_ASSIGN_REGIONS = 4; CREATE_TABLE_UPDATE_DESC_CACHE = 5; CREATE_TABLE_POST_OPERATION = 6; } StateMachineProcedure 使用 int[] states 数组来存储本次Procedure中所涉及到的 Procedure 状态的编号（即上面的1,2\u0026hellip;等数字），stateCount 用来存储 states 的下标。这里有个细节提下，即便是多个 Procedure 也一定是从前往后执行，所以 states 中的最后一个表示的一定是当前 Procedure 的状态。\n1.4.2 Procedure 的主要实现方法 # 最简单的 Procedure 可以采用如下的方式（取自 TestProcedureToString.java）实现。其中 execute() 和 rollback() 分别定义如何执行和回滚该 Procedure 操作。而 serializeStateData() 和 descrializeStateData() 则定义了如何序列化和反序列化该 Procedure。\n/** * A do-nothing basic procedure just for testing toString. */ static class BasicProcedure extends Procedure\u0026lt;BasicProcedureEnv\u0026gt; { @Override protected Procedure\u0026lt;BasicProcedureEnv\u0026gt;[] execute(BasicProcedureEnv env) throws ProcedureYieldException, InterruptedException { return new Procedure [] {this}; } @Override protected void rollback(BasicProcedureEnv env) throws IOException, InterruptedException { } @Override protected boolean abort(BasicProcedureEnv env) { return false; } @Override protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException { } @Override protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException { } } 2. HBase Procedure v2 之 ProcedureExecutor # 2.1 ProcedureExecutor 的实现 # ProcedureExecutor 功能是比较单一的，实现上面也仅仅只有一个 ProcedureExecutor 类而已。ProcedureExecutor 最核心的方法大概有 init()，submitProcedure()，bypassProcedure()。init() 方法主要负责完成 ProcedureExecutor 的初始化，并且接着将 ProcedureExecutor 启动起来；submitProcedure 完成的功能顾名思义是将 Procedure 提交到 ProcedureExecutor，交由其执行。bypassProcedure() 并不是一个非常常规的方法，会绕过 Procedure 的 execute() 和 rollback()，直接持久化 Procedure。\n3. HBase Procedure v2 之 ProcedureStore # 3.1 ProcedureStore 的实现 # 3.1.1 ProcedureStore 的实现体系 # ProcedureStore 的实现就只有如下两种，一种是存储在内存的 NoopProcedureStore，另一种是存储到 HDFS 的 RegionProcedureStore。\n注意： NoopProcedureStore 说存储在内存表述也不恰当，因为它没有使用任何数据结构去承载 Procedure。 RegionProcedureStore 则是借用 Master 的自有 Flush 逻辑，将 Procedure 持久化到 HDFS 的 MasterData 目录，有点类似于 Master 特有的 table。\n3.1.2 ProcedureStore 的主要实现方法 # 最简单的 Procedure 可以采用如下的方式（取自 NoopProcedureStore.java）实现。其中 start() 和 stop() 定义了如何打开 ProcedureStore，insert(), delete(), update(), load() 则定义了 ProcedureStore 的增删改查。\npublic class NoopProcedureStore extends ProcedureStoreBase { private int numThreads; @Override public void start(int numThreads) throws IOException { if (!setRunning(true)) { return; } this.numThreads = numThreads; } @Override public void stop(boolean abort) { setRunning(false); } @Override public void recoverLease() throws IOException { } @Override public int getNumThreads() { return numThreads; } @Override public int setRunningProcedureCount(final int count) { return count; } @Override public void load(final ProcedureLoader loader) throws IOException { loader.setMaxProcId(0); } @Override public void insert(Procedure\u0026lt;?\u0026gt; proc, Procedure\u0026lt;?\u0026gt;[] subprocs) { } @Override public void insert(Procedure\u0026lt;?\u0026gt;[] proc) { } @Override public void update(Procedure\u0026lt;?\u0026gt; proc) { } @Override public void delete(long procId) { } @Override public void delete(Procedure\u0026lt;?\u0026gt; proc, long[] subprocs) { } @Override public void delete(long[] procIds, int offset, int count) { } } 4. HBase Procedure v2 之 Scheduler # 4.1 Scheduler 的实现体系 # Scheduler 的实现总共两种，而 SimpleProcedureScheduler 几乎处于废弃的状态。MasterProcedureScheduler 与其说调度，倒不如说是维护四种与 Procedure 类型对应的的队列入队与出队。 注意，这个队列本质上是个完全平衡二叉树，而不是个普通的链表。而且这些队列自身并没有任何并发访问控制，在入队和出队的时候，需要先获取 schedulerLock（使用 ReentrantLock 实现），然后才能进行操作。\n每种 Procedure 被提交时，最终都会存储到对应的 Procedure 的队列中。\nprivate final FairQueue\u0026lt;ServerName\u0026gt; serverRunQueue = new FairQueue\u0026lt;\u0026gt;(); private final FairQueue\u0026lt;TableName\u0026gt; tableRunQueue = new FairQueue\u0026lt;\u0026gt;(); private final FairQueue\u0026lt;String\u0026gt; peerRunQueue = new FairQueue\u0026lt;\u0026gt;(); private final FairQueue\u0026lt;TableName\u0026gt; metaRunQueue = new FairQueue\u0026lt;\u0026gt;(); 5. HBase Procedure 的执行流程 # Procedure 的执行流程还是比较清晰的，以建表为例，当在客户端输入 create 't', 'f1'的时候，接着该请求会被发送到 HMaster，HMaster 接收到请求后，通过该请求参数构造成对应的 Procedure(CreateTableProcedure)，然后借助 MasterProcedureUtil 工具将其提交到 ProcedureExecutor，ProcedureExecutor 接收到该 Procedure 之后，将其提交到 MasterProcedureScheduler，接着由 MasterProcedureScheduler 将其插入 CreateTableProcedure 对应的队列 (tableRunQueue) 中。 因为 ProcedureExecutor 维护了一个 WorkerThread 线程池，该线程池在 HMaster 启动的时候，完成初始化并将其启动，当向由 MasterProcedureScheduler 中的队列插入 Procedure 后，该线程池中的 WorkerThread 发现（while 循环不断尝试发现）队列中插入了 Procedure，会将其取出来（因为获取队列 Procedure 之前，会先尝试获取 chedulerLock，所以完全不用担心多个线程同时获取到同一个 Procedure），根据 Procedure 中定义的 execute() 方法执行具体步骤。\n6. 参考文献 # HBASE-12439\n"},{"id":42,"href":"/posts/apache-hbase/ch09/","title":"Ch09-HBase 之 Procedure v2","section":"Blog","content":"HBase Procedure V2 其主要目标是提供多步执行的事务能力，跨多节点的通知机制，长时间 Procedure 运行的协同机制。\n1. 基本介绍 # HBase Procedure v2 提供了一种对内的事务能力，注意这里是对内的事务。简单来说，这里不是 HBase 数据层面的事务，而是操作步骤层面的事务。在 Procedure v2 出来之前，HBase 的操作过程处于一种伪事务的状态，同步操作的流程也处于一种伪同步的流程。在分布式的环境下，如果一个操作环节出现了问题，那么处理起来也非常棘手。\n比如在 Master 中的 CreateHandler() 任务交给了线程池，并没有实现真正意义上的同步。而且如果 CreateHander() 的创建过程如果非常长，或者出现了失败，也没有足够的回滚机制来将 Master 中存储的信息会滚到之前的状态。这些在 HBase 的稳定性上面都存在极大的隐患，基于此，HBase 社区与 2.0.0 引入 Procedure V2 来解决这一系列问题。 比如上图的问题，采用 Procedure V2 就可以实现为如下，将这个“伪异步”的线程作为一个 procedure，当执行后，返回一个 procId，然后客户端就可以根据这个 procId 确认该步骤是否完成。\n2. 基本架构 # 整个 Procedure v2 模块分为图中所示的三大块，每块各司其主共同完成事务操作。 ProcedureExecutor 首先会将 Procedure 本身提交给 ProcedureStore 完成持久化，接着由 ProcedureExecutor 将该 Procedure Load 出来，push 到 ProcedureScheduler。ProcedureExecutor 然后从 ProcedureScheduler 的队列中 poll 出 Procedure 交由 WorkThread 执行，如果中间产生了若干状态或者中间结果，也会将其插入到 ProcedureStore 中暂存下来。\n3. 核心组件 # Procedure 相关的类均被定义在 hbase-procedure 包里面，目前也仅仅只有 HMaster 会用到。Procedure 的几个组成模块中，每个模块定义了几个非常重要的方法。\n名称 方法 说明 Procedure execute(), rollback() 定义了具体的执行步骤，有点类似与执行模板/蓝图之类的概念 ProcedureExecutor sumitProcedure(Procedure), isFinished(procId), getResult(procId) 负责提交、执行 Procedure。Procedure 的执行操作主要由内部的多个 WorkerThread 来完成 ProcedureStore load(Proc), insert(Proc), update(Proc), delete(Proc) 用来持久化新提交的 Procedure 以及后续的每一次状态更新值，默认实现类为 WALProcedureStore ProcedureScheduler enqueue(), dequeue() 负责调度一个集群内的各种类型的 Procedure 请求，支持按优先级调度，相同优先级的 Procedure 则支持公平调度。 注意：\nProcedure##execute() 方法和 Procedure##rollback() 方法，实现要求必须具有幂等性，即无论执行多少次，其结果应该是一样的。也正是因为这两个方法的存在，使得 Procedure 可以保证事务性。 ProcedureStore 默认实现类为 WALProcedureStore。当文件超过一定大小后或者超过一定的时间周期后，会 roll 一个新的 WAL 文件出来，避免一个 WAL 文件过大；如果 WAL 日志无用的时候，会进行清理。。 ProcedureScheduler 共有两种，一种是 MasterProcedureScheduler，另一种是 SimpleProcedureScheduler(几乎弃用)。 4. 应用场景 # 名称 说明 Acls 访问控制 WAL Splitting 分配 WAL 文件，让 RegionServer 在启动的时候进行重放 AssignmentManager V2 将 Region 分配到不同的 RegionServer 上 5. 常见执行模型 # 条目 说明 StateMachineProcedure 主要基于状态机确定每个 Procedure 的执行状态是否正确，实际的应用比如 CreateTableProcedure RemoteProcedureDispatcher 通过 RPC 确定每个 Procedure 的执行状态是否正确，实际的应用比如 ServerCrashProcedure 6. 参考文献 # HBASE-12439 HBase2.0 procedureV2 原理简析 HBase Procedure V2 介绍 hbasecon-asia-2019 "},{"id":43,"href":"/posts/apache-flink/ch14/","title":"Ch14-Flink 之 Exactly-Once 语义","section":"Blog","content":"Flink 需要搭配特定 Source 和 Sink 才能实现精确一次处理语义。\n在 Flink 中需要端到端精准一次处理的位置有三个\nSource 端：通过保存消费数据的偏移量来保证一致性，所以 source 必须具有可以重置读位置的能力才行。 Flink 内部端：利用 Checkpoint 机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性。 Sink 端在 Flink 1.4 版本引入了两阶段提交 Sink 来解决精确一次处理语义，sink 必须具有幂等性写入和事务性写入的特性。 参考文献 # 硬核！八张图搞懂 Flink 端到端精准一次处理语义 Exactly-once（深入原理，建议收藏） "},{"id":44,"href":"/posts/apache-flink/ch13/","title":"Ch13-Flink 之 细粒度资源管理","section":"Blog","content":"Flink 之 细粒度资源管理\n1. Flink 资源管理 # 2. Flink 资源切割 # 3. 参考文献 # 细粒度资源管理及调度 finegrained_resource 深入解读 Flink 资源管理机制 Flink FOrward Asia 2021 "},{"id":45,"href":"/posts/apache-flink/ch12/","title":"Ch12-Flink 之 Window","section":"Blog","content":"Flink 之 Window\n1. Time Window # 1.1 Tumbling Time Window # 1.1 Sliding Time Window # 2. Count Window # 2.1 Tumbling Count Window # 2.2 Sliding Count Window # 3. Sesstion Window # Reference # Flink 原理与实现：Window 机制\n"},{"id":46,"href":"/posts/apache-flink/ch11/","title":"Ch11-Flink 之 Watermark","section":"Blog","content":"Flink 之 watermark\n1. Flink 中的时间 # 在做实时计算的时候，首先就需要搞清楚一个问题，这个实时到底是怎么样的一个时间概念。在 Flink 中，总共有 3 种时间概念，分别是事件时间(Event time)，处理时间(Processing time)，接入时间(Ingestion time).\n事件时间 (Event time) 就是真实的用户发生操作的时候所产生的时间，对应到 flink 中，需要用户显式的告诉 flink 到底每个输入中的哪一个字段代表这个事件时间。 接入时间 (Ingestion time) 和处理时间 (Processing time) 是不需要用户去指定的，flink 自己会去处理这个时间。接入时间的代表的是一个事件通过 source Operator 的时间，相比于 event time, ingestion time 不能处理乱序事件，因此也就不用生成对应的 watermark. 处理时间是指事件在操作算子计算过程中获取到的所在主机的时间。processing time 适合用于时间计算精度要求不是特别高的计算场景，例如统计某些延时非常高的日志数据。 2. watermark 分类 # 2.1 单分区 watermark # watermark 这个概念在 flink 中是与 event time 这个时间概念相互依存的，其目的是为了解决数据乱序到达和系统延迟的问题。flink 会把读取进系统的最新事件时间减去固定的时间间隔作为 watermark。还是用一张图来解释 watermark 的作用。\n当事件进入 flink 中的时候，根据提取的 event time 产生 watermark 时间戳，记为 X, 进入 flink 中的 event time 记为 Y. 当窗口的 end time \u0026lt; X 的时候，则触发窗口计算结果并输出。只要 X \u0026lt; end time, 那么 事件就可以 一直进入到当前窗口中，这样的话即便发生乱序，也可以在窗口中调整。调整的方法就是按照 Y.\n用伪代码描述大概如下\nif (x \u0026gt; windowEndtime) { compute(); } else { pushToWindow(); } 2.2 多分区 watermark # 具有两个或多个输入流（如 Union 或 CoFlatMap）的算子任务也会以所有分区 watermark 的最小值作为事件时间时钟。它们并不区分不同输入流的分区 watermark，所以两个输入流的数据都是基于相同的事件时间时钟进行处理的。\n3. 使用 watermark # 3.1 在 Source Function 中 直接指定 Timestamps 和 Watermark # DataStream\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; stream = env.addSource( new SourceFunction\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt;() { @Override public void run(SourceContext\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; ctx) throws Exception { for (Tuple3\u0026lt;Long, String, Integer\u0026gt; item : list) { long timestamp = item.f0; ctx.collectWithTimestamp(item, timestamp); ctx.emitWatermark(new Watermark(timestamp - 3)); } } @Override public void cancel() { } } ); 3.2 通过 Flink 自带的 Timestamp Assigner 指定 Timestamp 和 生成 watermark # 3.2.1 BoundedOutOfOrdernessTimestampExtractor # DataStream\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; stream = env.fromCollection(list) .assignTimestampsAndWatermarks( new BoundedOutOfOrdernessTimestampExtractor\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt;(Time.seconds(3)) { @Override public long extractTimestamp(Tuple3\u0026lt;Long, String, Integer\u0026gt; element) { return element.f0; } } ); 3.2.2 AscendingTimestampExtractor # DataStream\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; stream = env.fromCollection(list) .assignTimestampsAndWatermarks( new AscendingTimestampExtractor\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt;() { @Override public long extractAscendingTimestamp(Tuple3\u0026lt;Long, String, Integer\u0026gt; element) { return element.f0; } } ); 3.2.3 自定义 Timestamp Assigner 和 Watermark Generator # 注意： 下述两种 watermark 在 Flink 1.11 中已经废弃\nclass BoundedOutOfOrdernessGenerator implements AssignerWithPeriodicWatermarks\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; { private long maxOutOfOrderness; private long currentMaxTimestamp; BoundedOutOfOrdernessGenerator(long maxOutOfOrderness) { this.maxOutOfOrderness = maxOutOfOrderness; } @Override public long extractTimestamp(Tuple3\u0026lt;Long, String, Integer\u0026gt; element, long previousElementTimestamp) { long timestamp = element.f0; currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp); return timestamp; } @Override public Watermark getCurrentWatermark() { // return the watermark as current highest timestamp minus the out-of-orderness bound return new Watermark(currentMaxTimestamp - maxOutOfOrderness); } } class PunctuatedAssigner implements AssignerWithPunctuatedWatermarks\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; { @Nullable @Override public Watermark checkAndGetNextWatermark(Tuple3\u0026lt;Long, String, Integer\u0026gt; lastElement, long extractedTimestamp) { return new Watermark(extractedTimestamp); } @Override public long extractTimestamp(Tuple3\u0026lt;Long, String, Integer\u0026gt; element, long previousElementTimestamp) { return element.f0; } } 4. 参考文献 # Generating Timestamps / Watermarks watermark 的传递和事件时间 "},{"id":47,"href":"/posts/apache-flink/ch10/","title":"Ch10-Flink 之 BackPressure","section":"Blog","content":"Flink 1.5 之前使用的是 TCP-Based 反压机制，自 Flink 1.5(包含) 使用的是 Credit-Based 反压机制。\n1. 参考文献 # Flink 原理与实现：如何处理反压问题 Flink 反压机制 "},{"id":48,"href":"/posts/apache-flink/ch09/","title":"Ch09-Flink 之 Savepoint","section":"Blog","content":"Savepoint 跟 Checkpoint 的差别在于 Checkpoint 是 Flink 对于一个有状态应用在运行中利用分布式快照持续周期性的产生 Checkpoint，而 Savepoint 则是手动产生的 Checkpoint，Savepoint 记录着流式应用中所有运算元的状态。\n当手动产生一个 Checkpoint 的时候，就叫做一个 Savepoint。\n如图，Savepoint A 和 Savepoint B，无论是变更底层代码逻辑、修 bug 或是升级 Flink 版本，重新定义应用、计算的平行化程度等，最先需要做的事情就是产生 Savepoint。\nSavepoint 产生的原理是在 Checkpoint barrier 流动到所有的 Pipeline 中手动插入从而产生分布式快照，这些分布式快照点即 Savepoint。Savepoint 可以放在任何位置保存，当完成变更时，可以直接从 Savepoint 恢复、执行。\n从 Savepoint 的恢复执行需要注意，在变更应用的过程中时间在持续，如 Kafka 在持续收集资料，当从 Savepoint 恢复时，Savepoint 保存着 Checkpoint 产生的时间以及 Kafka 的相应位置，因此它需要恢复到最新的数据。无论是任何运算，Event – Time 都可以确保产生的结果完全一致。\n假设恢复后的重新运算用 Process Event – Time，将 Windows 窗口设为 1 小时，重新运算能够在 10 分钟内将所有的运算结果都包含到单一的 Windows 中。而如果使用 Event – Time，则类似于做 Bucketing。在 Bucketing 的状况下，无论重新运算的数量多大，最终重新运算的时间以及 Windows 产生的结果都一定能保证完全一致。\n参考文献 # Apache Flink 零基础入门（一\u0026amp;二）：基础概念解析\n"},{"id":49,"href":"/posts/apache-flink/ch08/","title":"Ch08-Flink 之 Checkpoint","section":"Blog","content":"Checkpoint 属于一种机制，State + ABS(Asynchronouse Barrier Snapshot)。\nFlink 的失败恢复依赖于 checkpoint 机制 + 可部分重发的数据源。\n检查点机制机制：checkpoint 定期触发，产生快照（快照记录了检查点开始时数据源中消息的 offset 和所有有状态的 operator 的状态信息） 可部分重发的数据源：比如在 Apache Kafka 中，消费者可以从指定偏移量开始重新消费 1. Chandy-Lamport 算法 # Chandy-Lamport 算法将分布式系统抽象成 DAG（暂时不考虑有闭环的图），节点表示进程，边表示两个进程间通信的管道。分布式快照的目的是记录下整个系统的状态，即可以分为节点的状态（进程的状态）和边的状态（信道的状态，即传输中的数据）。因为系统状态是由输入的消息序列驱动变化的，我们可以将输入的消息序列分为多个较短的子序列，图的每个节点或边先后处理完某个子序列后，都会进入同一个稳定的全局统状态。利用这个特性，系统的进程和信道在子序列的边界点分别进行本地快照，即使各部分的快照时间点不同，最终也可以组合成一个有意义的全局快照。\n2. Flink Checkpoint # 从实现上看，Flink 通过在 DAG 数据源定时向数据流注入名为 Barrier 的特殊元素，将连续的数据流切分为多个有限序列，对应多个 Checkpoint 周期。每当接收到 Barrier，算子进行本地的 Checkpoint 快照，并在完成后异步上传本地快照，同时将 Barrier 以广播方式发送至下游。当某个 Checkpoint 的所有 Barrier 到达 DAG 末端且所有算子完成快照，则标志着全局快照的成功。下图中 Barrier N 代表着所有在这个范围里面的数据都是 Checkpoint barrier N。\n在有多个输入 Channel 的情况下，为了数据准确性，算子会等待所有流的 Barrier 都到达之后才会开始本地的快照，这种机制被称为 Barrier 对齐。在对齐的过程中，算子只会继续处理的来自未出现 Barrier Channel 的数据，而其余 Channel 的数据会被写入输入队列，直至在队列满后被阻塞。当所有 Barrier 到达后，算子进行本地快照，输出 Barrier 到下游并恢复正常处理。\n比起其他分布式快照，该算法的优势在于辅以 Copy-On-Write 技术的情况下不需要“Stop The World”影响应用吞吐量，同时基本不用持久化处理中的数据，只用保存进程的状态信息，大大减小了快照的大小。\n3. 举例说明 # 假设现在需要产生 Checkpoint barrier N，但实际上在 Flink 中是由 Job manager 触发 Checkpoint，Checkpoint 被触发后开始从数据源产生 Checkpoint barrier。当 Job 开始做 Checkpoint barrier N 的时候，可以理解为 Checkpoint barrier N 需要逐步填充左下角的表格。\n如图，当部分事件标为红色，Checkpoint barrier N 也是红色时，代表着这些数据或事件都由 Checkpoint barrier N 负责。Checkpoint barrier N 后面白色部分的数据或事件则不属于 Checkpoint barrier N。\n在以上的基础上，当数据源收到 Checkpoint barrier N 之后会先将自己的状态保存，以读取 Kafka 资料为例，数据源的状态就是目前它在 Kafka 分区的位置，这个状态也会写入到上面提到的表格中。下游的 Operator 1 会开始运算属于 Checkpoint barrier N 的数据，当 Checkpoint barrier N 跟着这些数据流动到 Operator 1 之后，Operator 1 也将属于 Checkpoint barrier N 的所有数据都反映在状态中，当收到 Checkpoint barrier N 时也会直接对 Checkpoint 去做快照。\n当快照完成后继续往下游走，Operator 2 也会接收到所有数据，然后搜索 Checkpoint barrier N 的数据并直接反映到状态，当状态收到 Checkpoint barrier N 之后也会直接写入到 Checkpoint N 中。以上过程到此可以看到 Checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做 Distributed Snapshots，即分布式快照。分布式快照可以用来做状态容错，任何一个节点挂掉的时候可以在之前的 Checkpoint 中将其恢复。继续以上 Process，当多个 Checkpoint 同时进行，Checkpoint barrier N 已经流到 Job manager 2，Flink job manager 可以触发其他的 Checkpoint，比如 Checkpoint N + 1，Checkpoint N + 2 等等也同步进行，利用这种机制，可以在不阻挡运算的状况下持续地产生 Checkpoint。\n4. Unaligned Checkpoint # Flink 1.11 的 Unaligned Checkpoint 主要解决在高反压情况下作业难以完成 Checkpoint 的问题，同时它以磁盘资源为代价，避免了 Checkpoint 可能带来的阻塞，有利于提升 Flink 的资源利用率。随着流计算的普及，未来的 Flink 应用大概会越来越复杂，在未来经过实战打磨完善后 Unaligned Checkpoint 很有可能会取代 Aligned Checkpoint 成为 Flink 的默认 Checkpoint 策略。\nFlink 1.11 Unaligned Checkpoint 解析\n5. 参考文献 # Apache Flink 零基础入门（一\u0026amp;二）：基础概念解析 Unaligned Checkpoint 解析 "},{"id":50,"href":"/posts/apache-flink/ch07/","title":"Ch07-Flink 之 State","section":"Blog","content":"State 指一个具体的 task/operator 的状态，State Backends 指具体状态的存储介质\n1. State # state 本质上有点像分布式缓存，用来存储 task/operator 的状态。\n1.1 Managed State VS Raw State # Managed State 是 Flink 自动管理的 State，而 Raw State 是原生态 State。\n区分角度 Managed State Raw State 从状态管理方式来说 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构 从状态数据结构来说 支持已知的数据结构，如 Value、List、Map 等 只支持字节数组，所有状态都要转换为二进制字节数组才可以 从推荐使用场景来说 大多数情况下均可使用 比如需要自定义 Operator 时，推荐使用 Raw State。 1.2 Keyed State VS Operator State # Managed State 分为两种，一种是 Keyed State；另外一种是 Operator State(Non-Key State)。\n在 Flink Stream 模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream。在 Flink Stream 模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream。每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。\n区分角度 Keyed State Operator State 从使用方式来看 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source 从访问角度来看 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口 从数据结构来看 支持较多的数据结构 支持较少的数据结构 注意： Keyed State 跟 KeyedStream 绑定（ValueState, MapState, , ListState, ReducingState, AggregatingState） Operator State 由 Operator 自行管理 (ListState, UnionListState, BroadcastState)\n2. State Backends # 目前支持的 State Backends 有\nStateBackend in-flight checkpoint 吞吐 推荐使用场景 HashMapStateBackend TaskManager Memory JM Memory 高 调试、无状态或对数据丢失或重复无要求 RocksDBStateBackend RocksDB on TaskManager FS/HDFS 低 超大状态、超长窗口、大型 KV 结构 "},{"id":51,"href":"/posts/apache-flink/ch06/","title":"Ch06-Flink 之 容错机制","section":"Blog","content":"Flink 容错机制\n1. 作业容错 # 1.1 Task Failover 策略 # RestartAll RestartIndividualStrategy RestartPipelinedRegionStrategy 1.2 Job Restart 策略 # FixedDelayRestartStrategy FailureRateRestartStrategy NoRestartStrategy 2. 守护进程容错 # 2.1 JobManager 容错 # 2.1.1 ResourceManager 容错 # 2.1.2 JobMaster 容错 # 2.2 TaskManager 容错 # 2.3 并发故障 # 2.4 参考文献 # 深入理解 Flink 容错机制 "},{"id":52,"href":"/posts/apache-flink/ch05/","title":"Ch05-Flink 之 内存管理","section":"Blog","content":"Flink 内存管理\n1. JobManager # 2. TaskManager # 2.1 Total Memory # 3. 参考文献 # Flink 原理与实现：内存管理 Flink 内存管理机制 "},{"id":53,"href":"/posts/apache-flink/ch04/","title":"Ch04-Flink 之 图","section":"Blog","content":"Flink 图的转换概述\n1. 图 # 2. 参考文献 # Flink 原理与实现：架构和拓扑概览 Flink 原理与实现：如何生成 StreamGraph Flink 原理与实现：如何生成 JobGraph "},{"id":54,"href":"/posts/apache-hbase/ch08/","title":"Ch08-HBase 之 事务","section":"Blog","content":"HBase 事务\n1. 原子性 (Atomicity) # HBase 数据会首先写入 WAL，再写入 Memstore。写入 Memstore 异常很容易可以回滚，因此保证写入/更新原子性只需要保证写入 WAL 的原子性即可。而 WAL 的格式如下。\n\u0026lt;logseq#-for-entire-txn\u0026gt;:\u0026lt;WALEdit-for-entire-txn\u0026gt; \u0026lt;logseq#-for-entire-txn\u0026gt;:\u0026lt;-1, 3, \u0026lt;Keyvalue-for-edit-c1\u0026gt;, \u0026lt;KeyValue-for-edit-c2\u0026gt;, \u0026lt;KeyValue-for-edit-c3\u0026gt;\u0026gt; 每个事务操作只会产生一个 WAL 条目，这样就保证了 WAL 的原子性。\n2. 一致性 (Consistency) # 由原子性，隔离性，持久性共同保证。\n3. 隔离性 (Isolation) # 隔离主要体现在多个操作相互影响导致结果不对，比如两个 update 线程同时修改一行数据，最后修改出来的数据结果完全错误；比如一个 write 线程写数据，还没有写完的时候，read 线程就来读取数据，结果读到的数据完全错误。\n3.1 写写并发控制 # 在写入（或更新）之前先获取行锁，如果获取不到，说明已经有其他线程拿了该锁，就需要不断重试等待或者自旋等待，直至其他线程释放该锁。拿到锁之后开始写入数据，写入完成之后释放行锁即可。\n3.2 读写并发控制 # 借助 MVCC(Mutil Version Concurrent Control) 机制。每个写（更新）事务分配一个 Region 级别自增的序列号，并且为每一个读请求分配一个已完成的最大写事务序列号。这样就保证了读操作读到的数据一定是写（更新）事务完成后的数据。\n4. 持久性 (Durability) # HBase 事务持久化可以理解为 WAL 持久化，目前实现了多种持久化策略：SKIP_WAL，ASYNC_WAL，SYNC_WAL，FSYNC_WAL。\n5. 参考文献 # HBASE-12751 数据库事务系列－HBase 行级事务模型 "},{"id":55,"href":"/posts/apache-hbase/ch07/","title":"Ch07-HBase 之 Cache","section":"Blog","content":"HBase 相关的 Cache\n1. HBase Cache 介绍 # HBase 在实现中提供了两种缓存结构：MemStore和BlockCache。\nMemStore 称为写缓存，HBase 执行写操作首先会将数据写入 MemStore，并顺序写入 HLog，等满足一定条件后统一将 MemStore 中数据刷新到磁盘，这种设计可以极大地提升 HBase 的写性能。不仅如此，MemStore 对于读性能也至关重要，假如没有 MemStore，读取刚写入的数据就需要从文件中通过 IO 查找，这种代价显然是昂贵的。\nBlockCache 称为读缓存，HBase 会将一次文件查找的 Block 块缓存到 Cache 中，以便后续同一请求或者邻近数据查找请求，可以直接从内存中获取，避免昂贵的 IO 操作。\n2. MemStore # HBase 是基于 LSM-Tree 模型的，所有的数据更新插入操作都首先写入 MemStore 中（同时会顺序写到日志 HLog 中），达到指定大小之后再将这些修改操作批量写入磁盘，生成一个新的 HFile 文件，这种设计可以极大地提升 HBase 的写入性能；另外，HBase 为了方便按照 RowKey 进行检索，要求 HFile 中数据都按照 RowKey 进行排序，MemStore 数据在 flush 成为 HFile 之前会进行一次排序，将数据有序化；还有，根据局部性原理，新写入的数据会更大概率被读取，因此 HBase 在读取数据的时候首先检查请求的数据是否在 MemStore，写缓存未命中的话再到读缓存中查找，读缓存还未命中才会到 HFile 文件中查找，最终返回 merged 的一个结果给用户。可见，MemStore 无论是对 HBase 的写入性能还是读取性能都至关重要。在 MemStore 相关的数据操作中，flush 操作是 MemStore 最核心的操作。\n2.1 MemStore Flush 触发条件 # HBase 会在如下几种情况下触发 flush 操作，这里需要注意的是 MemStore 的最小 flush 单元是 HRegion 而不是单个 MemStore。因此如果一个 HRegion 中 MemStore 过多，每次 flush 的开销必然会很大，故建议进行表设计的时候尽可能的减少 ColumnFamily 的个数。\n2.1.1 MemStore 级别限制 # 当 Region 中任意一个 MemStore 的大小达到了上限（hbase.hregion.memstore.flush.size，默认 128MB），会触发 MemStore 刷新。\n2.1.2 Region 级别限制 # 当 Region 中所有 MemStore 的大小总和达到了上限（hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，默认 2* 128M = 256M），会触发 memstore 刷新。\n2.1.3 Region Server 级别限制 # 当一个 Region Server 中所有 MemStore 的大小总和达到了上限（hbase.regionserver.global.memstore.upperLimit * hbase_heapsize，默认 40% 的 JVM 内存使用量），会触发部分 MemStore 刷新。\nFlush 顺序是按照 MemStore 由大到小执行，先 Flush MemStore 最大的 Region，再执行次大的，直至总体 MemStore 内存使用量低于阈值（hbase.regionserver.global.memstore.lowerLimit * hbase_heapsize，默认 38% 的 JVM 内存使用量）。\n2.1.4 HLog 数量达到上限 # 当一个 Region Server 中 HLog 数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog 对应的一个或多个 Region 进行 flush。\n2.1.5 HBase 定期刷新 MemStore # 默认周期为 1 小时，确保 MemStore 不会长时间没有持久化。为避免所有的 MemStore 在同一时间都进行 flush 导致的问题，定期的 flush 操作有 20000 左右的随机延时。\n2.1.6 手动执行 flush # 用户可以通过命令 flush 'tablename' 或者flush 'region name'分别对一张表或者一个 Region 进行 flush 操作。\n3.1 MemStore Flush 流程 # 为了减少 flush 过程对读写的影响，HBase 采用了类似于两阶段提交的方式，将整个 flush 过程分为三个阶段：\n3.1.1 prepare 阶段 # 遍历当前 Region 中的所有 MemStore，将 MemStore 中当前数据集 kvset 做一个快照 snapshot，然后再新建一个新的 kvset。后期的所有写入操作都会写入新的 kvset 中，而整个 flush 阶段读操作会首先分别遍历 kvset 和 snapshot，如果查找不到再会到 HFile 中查找。prepare 阶段需要加一把 updateLock 对写请求阻塞，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。\n3.1.2 flush 阶段 # 遍历所有 MemStore，将 prepare 阶段生成的 snapshot 持久化为临时文件，临时文件会统一放到目录.tmp 下。这个过程因为涉及到磁盘 IO 操作，因此相对比较耗时。\n3.1.3 commit 阶段 # 遍历所有的 MemStore，将 flush 阶段生成的临时文件移到指定的 ColumnFamily 目录下，针对 HFile 生成对应的 StoreFile 和 Reader，把 StoreFile 添加到 HStore 的 StoreFiles 列表中，最后再清空 prepare 阶段生成的 snapshot。\n4. BlockCache # BlockCache 是 Region Server 级别的，一个 Region Server 只有一个 Block Cache，在 Region Server 启动的时候完成 Block Cache 的初始化工作。到目前为止，HBase 先后实现了 3 种 Block Cache 方案，LRUBlockCache 是最初的实现方案，也是默认的实现方案；HBase 0.92 版本实现了第二种方案 SlabCache，见 HBASE-4027；HBase 0.96 之后官方提供了另一种可选方案 BucketCache，见 HBASE-7404。 这三种方案的不同之处在于对内存的管理模式，其中 LRUBlockCache 是将所有数据都放入 JVM Heap 中，交给 JVM 进行管理。而后两者采用了不同机制将部分数据存储在堆外，交给 HBase 自己管理。这种演变过程是因为 LRUBlockCache 方案中 JVM 垃圾回收机制经常会导致程序长时间暂停，而采用堆外内存对数据进行管理可以有效避免这种情况发生。\n4.1 LRUBlockCache # LRUBlockCache 是 HBase 目前默认的 BlockCache 机制，实现机制比较简单。它使用一个 ConcurrentHashMap 管理 BlockKey 到 Block 的映射关系，缓存 Block 只需要将 BlockKey 和对应的 Block 放入该 HashMap 中，查询缓存就根据 BlockKey 从 HashMap 中获取即可。同时该方案采用严格的 LRU 淘汰算法，当 Block Cache 总量达到一定阈值之后就会启动淘汰机制，最近最少使用的 Block 会被置换出来。在 HBase 中，计算可以用来做 cache 的容量可以通过计算 number of region servers * heap size * hfile.block.cache.size * 0.99 得到。具体的实现细节方面，需要关注三点：\n4.1.1 缓存分层策略 # HBase 在 LRU 缓存基础上，采用了缓存分层设计，将整个 BlockCache 分为三个部分：single-access、mutil-access 和 inMemory，其中每部分分别占到整个 BlockCache 大小的 25%、50%、25%。 一次随机读中，一个 Block 块从 HDFS 中加载出来之后首先放入 single 区，后续如果有多次请求访问到这块数据的话，就会将这块数据移到 mutil-access 区。而 in-memory 区表示数据可以常驻内存，一般用来存放访问频繁、数据量小的数据，比如元数据，用户也可以在建表的时候通过设置列族属性 IN-MEMORY= true 将此列族放入 in-memory 区（设置的时候应特别注意，确保此列族数据量很小且访问频繁，否则有可能会将 hbase:meta 元数据挤出内存，严重影响所有业务性能）。 无论哪个区，系统都会采用严格的 Least-Recently-Used 算法，当 BlockCache 总量达到一定阈值之后就会启动淘汰机制，最少使用的 Block 会被置换出来，为新加载的 Block 预留空间。\n4.1.2 LRU 淘汰算法实现 # 系统在每次 cache block 时将 BlockKey 和 Block 放入 HashMap 后都会检查 BlockCache 总量是否达到阈值，如果达到阈值，就会唤醒淘汰线程对 Map 中的 Block 进行淘汰。系统设置三个 MinMaxPriorityQueue 队列，分别对应上述三个分层，每个队列中的元素按照最近最少被使用排列，系统会优先 poll 出最近最少使用的元素，将其对应的内存释放。可见，三个分层中的 Block 会分别执行 LRU 淘汰算法进行淘汰。\n4.1.3 LRU 方案优缺点 # LRU 方案使用 JVM 提供的 HashMap 管理缓存，简单有效。但随着数据从 single-access 区晋升到 mutil-access 区，基本就伴随着对应的内存对象从 young 区到 old 区，晋升到 old 区的 Block 被淘汰后会变为内存垃圾，最终由 CMS 回收掉（Conccurent Mark Sweep，一种标记清除算法），然而这种算法会带来大量的内存碎片，碎片空间一直累计就会产生臭名昭著的 Full GC。尤其在大内存条件下，一次 Full GC 很可能会持续较长时间，甚至达到分钟级别。大家知道 Full GC 是会将整个进程暂停的（称为 stop-the-wold 暂停），因此长时间 Full GC 必然会极大影响业务的正常读写请求。也正因为这样的弊端，SlabCache 方案和 BucketCache 方案才会横空出世。\n4.2 BucketCache # 相比 LRUBlockCache 而言，BucketCache 实现相对比较复杂。它没有使用 JVM 内存管理算法来管理缓存，而是自己对内存进行管理，因此不会因为出现大量碎片导致 Full GC 的情况发生。本小节主要介绍 BucketCache 的具体实现方式（包括 BucketCache 的内存组织形式、缓存写入读取流程等）以及如何配置使用 BucketCache。\n4.2.1 内存组织形式 # 下图是 BucketCache 的内存组织形式图，其中上面部分是逻辑组织结构，下面部分是对应的物理组织结构。\nHBase 启动之后会在内存中申请大量的 bucket，如上图中黄色矩形所示，每个 bucket 的大小默认都为 2MB。每个 bucket 会有一个 baseoffset 变量和一个 size 标签，其中 baseoffset 变量表示这个 bucket 在实际物理空间中的起始地址，因此 block 的物理地址就可以通过 baseoffset 和该 block 在 bucket 的偏移量唯一确定；而 size 标签表示这个 bucket 可以存放的 block 块的大小，比如图中左侧 bucket 的 size 标签为 65KB，表示可以存放 64KB 的 block，右侧 bucket 的 size 标签为 129KB，表示可以存放 128KB 的 block。\nHBase 中使用 BucketAllocator 类实现对 Bucket 的组织管理：\nHBase 会根据每个 bucket 的 size 标签对 bucket 进行分类，相同 size 标签的 bucket 由同一个 BucketSizeInfo 管理，如上图，左侧存放 64KB block 的 bucket 由 65KB BucketSizeInfo 管理，右侧存放 128KB block 的 bucket 由 129KB BucketSizeInfo 管理。 HBase 在启动的时候就决定了 size 标签的分类，默认标签有 (4+1)K、(8+1)K、(16+1)K … (48+1)K、(56+1)K、(64+1)K、(96+1)K … (512+1)K。而且系统会首先从小到大遍历一次所有 size 标签，为每种 size 标签分配一个 bucket，最后所有剩余的 bucket 都分配最大的 size 标签，默认分配 (512+1)K，如上图所示。 Bucket 的 size 标签可以动态调整，比如 64K 的 block 数目比较多，65K 的 bucket 被用完了以后，其他 size 标签的完全空闲的 bucket 可以转换成为 65K 的 bucket，但是至少保留一个该 size 的 bucket。 4.2.2 Block 缓存写入、读取流程 # 下图是 block 写入缓存以及从缓存中读取 block 的流程示意图，图中主要包括 5 个模块，其中\nRAMCache 是一个存储 blockkey 和 block 对应关系的 HashMap； WriteThead 是整个 block 写入的中心枢纽，主要负责异步的写入 block 到内存空间； BucketAllocator 在上一节详细介绍过，主要实现对 bucket 的组织管理，为 block 分配内存空间； IOEngine 是具体的内存管理模块，主要实现将 block 数据写入对应地址的内存空间； BackingMap 也是一个 HashMap，用来存储 blockKey 与对应物理内存偏移量的映射关系，用来根据 blockkey 定位具体的 block；其中紫线表示 cache block 流程，绿线表示 get block 流程。 4.2.2.1 Block 缓存写入流程 # 将 block 写入 RAMCache。实际实现中，HBase 设置了多个 RAMCache，系统首先会根据 blockkey 进行 hash，根据 hash 结果将 block 分配到对应的 RAMCache 中； WriteThead 从 RAMCache 中取出所有的 block。和 RAMCache 相同，HBase 会同时启动多个 WriteThead 并发的执行异步写入，每个 WriteThead 对应一个 RAMCache； 每个 WriteThead 会将遍历 RAMCache 中所有 block 数据，分别调用 bucketAllocator 为这些 block 分配内存空间； BucketAllocator 会选择与 block 大小对应的 bucket 进行存放（具体细节可以参考上节‘内存组织形式’所述），并且返回对应的物理地址偏移量 offset； WriteThead 将 block 以及分配好的物理地址偏移量传给 IOEngine 模块，执行具体的内存写入操作； 写入成功后，将类似\u0026lt;blockkey,offset\u0026gt;这样的映射关系写入 BackingMap 中，方便后续查找时根据 blockkey 可以直接定位； 4.2.2.2 Block 缓存读取流程 # 首先从 RAMCache 中查找。对于还没有来得及写入到 bucket 的缓存 block，一定存储在 RAMCache 中； 如果在 RAMCache 中没有找到，再在 BackingMap 中根据 blockKey 找到对应物理偏移地址 offset； 根据物理偏移地址 offset 可以直接从内存中查找对应的 block 数据； 4.2.3 BucketCache 工作模式 # BucketCache 默认有三种工作模式：heap、offheap和file；这三种工作模式在内存逻辑组织形式以及缓存流程上都是相同的，不同的是三者对应的最终存储介质有所不同，即上述所讲的 IOEngine 有所不同。\nheap 模式和 offheap 模式都使用内存作为最终存储介质，内存分配查询也都使用 Java NIO ByteBuffer 技术，不同的是，heap 模式分配内存会调用 byteBuffer.allocate 方法，从 JVM 提供的 heap 区分配，而后者会调用 byteBuffer.allocateDirect 方法，直接从操作系统分配。这两种内存分配模式会对 HBase 实际工作性能产生一定的影响。影响最大的无疑是 GC，相比 heap 模式，offheap 模式因为内存属于操作系统，所以基本不会产生 CMS GC，也就在任何情况下都不会因为内存碎片导致触发 Full GC。除此之外，在内存分配以及读取方面，两者性能也有不同，比如，内存分配时 heap 模式需要首先从操作系统分配内存再拷贝到 JVM heap，相比 offheap 直接从操作系统分配内存更耗时；但是反过来，读取缓存时 heap 模式可以从 JVM heap 中直接读取，而 offheap 模式则需要首先从操作系统拷贝到 JVM heap 再读取，显得后者更费时。\nfile 模式和前面两者不同，它使用 Fussion-IO 或者 SSD 等作为存储介质，相比昂贵的内存，这样可以提供更大的存储容量，因此可以极大地提升缓存命中率。\n4.3 SlabCache # 为了解决 LRUBlockCache 方案中因为 JVM 垃圾回收导致的服务中断，SlabCache 方案使用 Java NIO DirectByteBuffer 技术实现了堆外内存存储，不再由 JVM 管理数据内存。默认情况下，系统在初始化的时候会分配两个缓存区，分别占整个 BlockCache 大小的 80% 和 20%，每个缓存区分别存储固定大小的 Block 块，其中前者主要存储小于等于 64K 大小的 Block，后者存储小于等于 128K Block，如果一个 Block 太大就会导致两个区都无法缓存。和 LRUBlockCache 相同，SlabCache 也使用 Least-Recently-Used 算法对过期 Block 进行淘汰。和 LRUBlockCache 不同的是，SlabCache 淘汰 Block 的时候只需要将对应的 bufferbyte 标记为空闲，后续 cache 对其上的内存直接进行覆盖即可。\n线上集群环境中，不同表不同列族设置的 BlockSize 都可能不同，很显然，默认只能存储两种固定大小 Block 的 SlabCache 方案不能满足部分用户场景，比如用户设置 BlockSize = 256K，简单使用 SlabCache 方案就不能达到这部分 Block 缓存的目的。因此 HBase 实际实现中将 SlabCache 和 LRUBlockCache 搭配使用，称为 DoubleBlockCache。一次随机读中，一个 Block 块从 HDFS 中加载出来之后会在两个 Cache 中分别存储一份；缓存读时首先在 LRUBlockCache 中查找，如果 Cache Miss 再在 SlabCache 中查找，此时如果命中再将该 Block 放入 LRUBlockCache 中。\n经过实际测试，DoubleBlockCache 方案有很多弊端。比如 SlabCache 设计中固定大小内存设置会导致实际内存使用率比较低，而且使用 LRUBlockCache 缓存 Block 依然会因为 JVM GC 产生大量内存碎片。因此在 HBase 0.98 版本之后，该方案已经被不建议使用。\n5. 压缩 BlockCache # HBASE-11331 引入了延迟解压缩 BlockCache 的功能，即压缩 BlockCache，当该功能启用的时候，data block 和 encoded data block 会被以存储在磁盘的文件格式存储在 BlockCache 中，简单来说，就是此时的 BlockCache 中的 block 会被压缩。\n这么做的好处是，RegionServer 可以将更多的数据存储在 cache 中，经过测试得出，如果使用 SNAPPY 压缩，其吞吐量会增加 50%，平均延迟性能提升 30%，于此同时，GC 增加 80%，CPU 负载增加 2%。因此是否启用该功能还是要根据应用取舍，如果应用是资源敏感性应用，那么便不是非常适合启用该功能。\n启用方式通过在 hbase-site.xml 中设置 hbase.block.data.cachecompressed=true 便可。\n6. 其他注意事项 # 在实际 BlockCache 部署中会采用混合部署方式，这种方式中 LRUBlockCache 和 BucketCache 一起部署，采用分级策略。简单来说 INDEX 和 BLOOM 是存在 LRUBlockCache 这一层面的 (L1)，DATA blocks 会被保存在 BucketCache 这一次层 (L2)。在 HBase2.0.0 之前可以通过设置 hbase.bucketcache.combinedcache.enabled=false 关闭这种模式，即从 LRUBlockCache 淘汰出来的 block 会存储到 BucketCache 中，但现在这种方式已经不可用了。\n7. 参考文献 # HBase Book HBase – Memstore Flush 深度解析 HBase BlockCache 系列 – 走进 BlockCache HBase BlockCache 系列 － 探求 BlockCache 实现机制 HBase BlockCache 系列 － 性能对比测试报告 一条数据的 HBase 之旅，简明 HBase 入门教程 11：In-Memory Flush "},{"id":56,"href":"/posts/apache-hbase/ch06/","title":"Ch06-HBase 之 StoreFile","section":"Blog","content":"HFile 是 HBase 存储数据的文件组织形式，参考 BigTable 的 SSTable 和 Hadoop 的 TFile 实现。从 HBase 开始到现在，HFile 经历了三个版本，其中 V2 在 0.92 引入，V3 在 0.98 引入。HFileV1 版本的在实际使用过程中发现它占用内存多，HFile V2 版本针对此进行了优化，HFile V3 版本基本和 V2 版本相同，只是在 cell 层面添加了 Tag 数组的支持。\n1. HFile V2 逻辑结构 # 文件主要分为四个部分：Scanned block section，Non-scanned block section，Opening-time data section和Trailer。如下图所示：\n其中每个部分的作用如下\n名称 说明 Scanned block section 表示顺序扫描 HFile 时所有的数据块将会被读取，包括 DataBlock，Leaf Index Block，Bloom Block Non-scanned block section 表示在 HFile 顺序扫描的时候数据不会被读取，主要包括 Meta Block(用来兼容 v1，v2 后就不再存储数据了) 和 Intermediate Level Data Index Blocks 两部分 Load-on-open-section 该部分数据在 HBase 的 region server 启动时，会被加载到内存中。其中包括 FileInfo、Bloom filter block、Data block index 和 Meta block index。 Trailer 记录了 HFile 的基本信息、各个部分的偏移值和寻址信息。 2. HFile V2 物理结构 # 如上图所示，HFile 会被切分为多个大小相等的 block 块，每个 block 的大小可以在创建表列簇的时候通过参数blocksize ＝\u0026gt; '65535'进行指定，默认为 64k，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询，因而需要权衡。\n可以使用下述命令查看实际的一个 HFile，具体内容可以参考附件。\nhbase org.apache.hadoop.hbase.io.hfile.HFile -v -p -m -f /hbase/data/default/tmp/bf9e4d84d5b5c58123d2436c0745ac02/cf/d6da722c69a74d75852f87f6c009dacd 3. HFileBlock 介绍 # HFileBlock 主要分为 3 部分：Block Header，Block Data 和 Block Tail。其中，Block Header 用来存储元数据，包括block 类型，压缩 block 大小，上一个 block 的偏移量等；而 Block Data 主要存储具体的数据；Block Tail 则主要用来存储校验码。\nHFile 中的 BlockType 总共有如下几种，后面会对几种比较重要的 Block 详细说明。\n(图中 META 和 METABLKc 标反了)\n3.1 DATA # DataBlock 是 HBase 中数据存储的最小单元。DataBlock 中主要存储用户的 KeyValue 数据（KeyValue 后面一般会跟一个 timestamp，图中未标出），而 KeyValue 结构是 HBase 存储的核心，每个数据都是以 KeyValue 结构在 HBase 中进行存储。KeyValue 结构在内存和磁盘中可以表示为下图\n每个 KeyValue 都由 4 个部分构成，分别为 key length，value length，key 和 value。其中 key value 和 value length 是两个固定长度的数值，而 key 是一个复杂的结构，首先是 rowkey 的长度，接着是 rowkey，然后是 ColumnFamily 的长度，再是 ColumnFamily，之后是 ColumnQualifier，最后是时间戳和 KeyType（keytype 有四种类型，分别是 Put、Delete、DeleteColumn 和 DeleteFamily），value 就没有那么复杂，就是一串纯粹的二进制数据。\n3.2 ROOT_INDEX | LEAF_INDEX | INTERMEDIATE_INDEX # HFile 中的index level是不固定的，根据不同的数据类型和数据大小有不同的选择，主要有两类，一类是 single-level（单级索引），另一类是 multi-level（多级索引，索引 block 无法在内存中存放，所以采用多级索引）。\nHFile 中的index chunk有两大类，分别是 root index chunk、nonRoot index chunk。而 nonRoot index chunk 又分为 interMetadiate index chunk 和 leaf index chunk，但 intermetadiate index chunk 和 leaf index chunk 在内存中的分布是一样的。\n对于 meta block 和 bloom block 采用的索引是 single-level 形式，只用 root index chunk 来保存指向 block 的索引信息（root_index -\u0026gt; xxx_block） 对于 data 当 data block 数量较少时，采用的是 single level，一般情况下是一级索引 (root_index -\u0026gt; data_block)； 当 data block 数量较多时，采用的是 multi-level，一般情况下是两级索引，使用 root index chunk 和 leaf index chunk 来保存索引信息 (root_index -\u0026gt; leaf_index -\u0026gt; data_block)； 当 data block 数量很多时，采用的是 multi-level，一般情况下是三级索引，使用 root index chunk、intermetadiate index chunk 和 leaf index chunk 来保存指向数据的索引（root_index -\u0026gt; intermediate_index -\u0026gt; leaf_index-\u0026gt;data_block）。 3.2.1 ROOT_INDEX # Root Index Block 表示索引树根节点索引块，可以作为 bloom 的直接索引，也可以作为 data 索引的根索引。\n其中 Index Entry 表示具体的索引对象，每个索引对象由 3 个字段组成，Block Offset 表示索引指向数据块的偏移量，BlockDataSize 表示索引指向数据块在磁盘上的大小，BlockKey 表示索引指向数据块中的第一个 key。除此之外，还有另外 3 个字段用来记录 MidKey 的相关信息，MidKey 表示 HFile 所有 Data Block 中中间的一个 Data Block，用于在对 HFile 进行 split 操作时，快速定位 HFile 的中间位置。需要注意的是 single-level 索引结构和 mutil-level 结构相比，就只缺少 MidKey 这三个字段。\nRoot Index Block 会在 HFile 解析的时候直接加载到内存中，此处需要注意在 Trailer Block 中有一个字段为 dataIndexCount，就表示此处 Index Entry 的个数。因为 Index Entry 并不定长，只有知道 Entry 的个数才能正确的将所有 Index Entry 加载到内存。\n3.2.2 LEAF_INDEX | INTERMEDIATE_INDEX # 当 HFile 中 Data Block 越来越多，single-level 结构的索引已经不足以支撑所有数据都加载到内存，需要分化为 mutil-level 结构。mutil-level 结构中 NonRoot Index Block 作为中间层节点或者叶子节点存在，无论是中间节点还是叶子节点，其都拥有相同的结构，如下图所示\n和 Root Index Block 相同，NonRoot Index Block 中最核心的字段也是 Index Entry，用于指向叶子节点块或者数据块。不同的是，NonRoot Index Block 结构中增加了 block 块的内部索引 entry Offset 字段，entry Offset 表示 index Entry 在该 block 中的相对偏移量（相对于第一个 index Entry)，用于实现 block 内的二分查找。所有非根节点索引块，包括 Intermediate index block 和 leaf index block，在其内部定位一个 key 的具体索引并不是通过遍历实现，而是使用二分查找算法，这样可以更加高效快速地定位到待查找 key。\n3.3 BLOOM_CHUNK # Bloom Index Block 结构中 totalByteSize 表示位数组的 bit 数，numChunks 表示 Bloom Block 的个数，hashCount 表示 hash 函数的个数，hashType 表示 hash 函数的类型，totalKeyCount 表示 bloom filter 当前已经包含的 key 的数目，totalMaxKeys 表示 bloom filter 当前最多包含的 key 的数目，Bloom Index Entry 对应每一个 bloom filter block 的索引条目，作为索引分别指向 scanned block section 部分的 Bloom Block，Bloom Block 中就存储了对应的位数组。\nBloom Index Entry 的结构见上图左边所示，BlockOffset 表示对应 Bloom Block 在 HFile 中的偏移量，FirstKey 表示对应 BloomBlock 的第一个 Key。根据上文所说，一次 get 请求进来，首先会根据 key 在所有的索引条目中进行二分查找，查找到对应的 Bloom Index Entry，就可以定位到该 key 对应的位数组，加载到内存进行过滤判断。\n3.4 TRAILER # 主要记录了 HFile 的基本信息、各个部分的偏移值和寻址信息，下图为 Trailer 内存和磁盘中的数据结构，其中只显示了部分核心字段：\nHFile 在读取的时候首先会解析 Trailer Block 并加载到内存，然后再进一步加载 LoadOnOpen 区的数据。\n4. Reference # Document HFile v3 HBase Book HBase – 存储文件 HFile 结构解析 HBase – 探索 HFile 索引机制 HBase 加载 Hfile 时的读取过程 HBase 高性能随机查询之道 – HFile 原理解析 HADOOP-3315 5. 附件 # li@li-pc: hbase org.apache.hadoop.hbase.io.hfile.HFile -v -p -m -f /hbase/data/default/tmp/bf9e4d84d5b5c58123d2436c0745ac02/cf/d6da722c69a74d75852f87f6c009dacd Scanning -\u0026gt; /hbase/data/default/tmp/bf9e4d84d5b5c58123d2436c0745ac02/cf/d6da722c69a74d75852f87f6c009dacd K: row01/cf:f1/1616850516066/Put/vlen=5/seqid=4 V: value K: row02/cf:f1/1616855810928/Put/vlen=7/seqid=8 V: value02 K: row03/cf:f1/1616855815901/Put/vlen=7/seqid=9 V: value03 K: row04/cf:f1/1617116267526/Put/vlen=7/seqid=13 V: value04 K: row05/cf:f1/1617206213608/Put/vlen=7/seqid=18 V: value04 K: row06/cf:f1/1617206237852/Put/vlen=7/seqid=19 V: value04 K: row07/cf:f1/1617206337001/Put/vlen=7/seqid=20 V: value04 K: row08/cf:f1/1617439598102/Put/vlen=7/seqid=24 V: value04 K: row09/cf:f1/1617440409630/Put/vlen=7/seqid=25 V: value04 Block index size as per heapsize: 320 reader=/hbase/data/default/tmp/bf9e4d84d5b5c58123d2436c0745ac02/cf/d6da722c69a74d75852f87f6c009dacd, compression=none, cacheConf=cacheDataOnRead=false, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheDataCompressed=false, prefetchOnOpen=false, firstKey=Optional[row01/cf:f1/1616850516066/Put/seqid=0], lastKey=Optional[row09/cf:f1/1617440409630/Put/seqid=0], avgKeyLen=21, avgValueLen=6, entries=9, length=5286 Trailer: fileinfoOffset=529, loadOnOpenDataOffset=421, dataIndexCount=1, metaIndexCount=0, totalUncomressedBytes=5195, entryCount=9, compressionCodec=NONE, uncompressedDataIndexSize=34, numDataIndexLevels=1, firstDataBlockOffset=0, lastDataBlockOffset=0, comparatorClassName=org.apache.hadoop.hbase.CellComparatorImpl, encryptionKey=NONE, majorVersion=3, minorVersion=3 Fileinfo: BLOOM_FILTER_TYPE = ROW COMPACTION_EVENT_KEY = PBUF\\x0A 574e0cd2f81942f6b4d6bcca5a1de7bd\\x0A 9e99e162bcb7463dafe0e2f2b1a7938c\\x0A a945bf413e214c1785706d1615c59eb0 DELETE_FAMILY_COUNT = 0 EARLIEST_PUT_TS = 1616850516066 KEY_VALUE_VERSION = 1 LAST_BLOOM_KEY = row09 MAJOR_COMPACTION_KEY = true MAX_MEMSTORE_TS_KEY = 25 MAX_SEQ_ID_KEY = 25 TIMERANGE = 1616850516066....1617440409630 hfile.AVG_KEY_LEN = 21 hfile.AVG_VALUE_LEN = 6 hfile.CREATE_TIME_TS = 1617447255148 hfile.LASTKEY = row09/cf:f1/1617440409630/Put/vlen=0/mvcc=0 Mid-key: Optional[row01/cf:f1/1616850516066/Put/seqid=0] Bloom filter: BloomSize: 16 No of Keys in bloom: 9 Max Keys for bloom: 13 Percentage filled: 69% Number of chunks: 1 Comparator: ByteArrayComparator Delete Family Bloom filter: Not present Scanned kv count -\u0026gt; 9 "},{"id":57,"href":"/posts/apache-flink/ch03/","title":"Ch03-Flink 之 Job 执行流程","section":"Blog","content":"Flink Job 执行流程\n1. Application Mode # bin/flink run-application -t yarn-application ./examples/streaming/TopSpeedWindowing.jar 2. Per-Job Mode # bin/flink run -t yarn-per-job --detached ./examples/streaming/TopSpeedWindowing.jar 3. Session Mode # bin/flink run -t yarn-session -Dyarn.application.id=application_XXXX_YY ./examples/streaming/TopSpeedWindowing.jar 4. 参考文献 # 详解 Flink 作业提交流程 "},{"id":58,"href":"/posts/apache-flink/ch02/","title":"Ch02-Flink 之 应用执行模式","section":"Blog","content":"Flink 应用程序 是从其 main() 方法产生的一个或多个 Flink 作业的任何用户程序。这些作业的执行可以在本地 JVM（LocalEnvironment）中进行，或具有多台机器的集群的远程设置（RemoteEnvironment）中进行。\nFlink 应用程序的作业可以被提交到长期运行的 Flink Session 集群、专用的 Flink Job 集群 或 Flink Application 集群。这些选项之间的差异主要与集群的生命周期和资源隔离保证有关。\n1. Flink Session 集群 # 名称 说明 集群生命周期 在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群，该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。因此，Flink Session 集群的寿命不受任何 Flink 作业寿命的约束。 资源隔离 TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争 — 例如提交工作阶段的网络带宽。此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；类似的，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。 其他注意事项 拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响 — 就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。 注意：以前，Flink Session 集群也被称为 session 模式下的 Flink 集群。\n2. Flink Job 集群 # 名称 说明 集群生命周期 在 Flink Job 集群中，可用的集群管理器（例如 YARN 或 Kubernetes）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。在这里，客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。一旦作业完成，Flink Job 集群将被拆除。 资源隔离 JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。 其他注意事项 由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业。 注意：以前，Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。\n3. Flink Application 集群 # 名称 说明 集群生命周期 Flink Application 集群是专用的 Flink 集群，仅从 Flink 应用程序执行作业，并且 main() 方法在集群上而不是客户端上运行。提交作业是一个单步骤过程：无需先启动 Flink 集群，然后将作业提交到现有的 session 集群；相反，将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，并且集群入口（ApplicationClusterEntryPoint）负责调用 main() 方法来提取 JobGraph。例如，这允许你像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。 资源隔离 在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。 注意：Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。\n4. 参考文献 # flink deployment "},{"id":59,"href":"/posts/apache-flink/ch01/","title":"Ch01-Flink 之 介绍","section":"Blog","content":"Apache Flink 是 Apache 基金会旗下的一个开源大数据处理框架。应用于分布式、高性能、高可用的数据流应用程序。可以处理有限数据流和无限数据流，即能够处理有边界和无边界的数据流。\n1. 基本信息 # 条目 说明 官网 https://flink.apache.org/ 下载地址 https://flink.apache.org/downloads.html 2. 架构介绍 # 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。\n2.1 Client # Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程 ./bin/flink run 中运行。\n2.2 JobManager # JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：\n2.2.1 ResourceManager # ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位（请参考 TaskManagers）。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。\n2.2.2 Dispatcher # Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。\n2.2.3 JobMaster # JobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。\n始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby（请参考 高可用（HA））。\n2.3 TaskManagers # TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。\n必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子。\n3. Tasks # 3.1 Tasks and Operator Chains # 对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。Operator Chains 行为是可以配置的。\n上图中样例数据流用 5 个 subtask 执行，因此有 5 个并行线程。\n3.2 Task Slots and Resources # 每个 worker（TaskManager）都是一个 JVM 进程，可以在单独的线程中执行一个或多个 subtask。为了控制一个 TaskManager 中接受多少个 task，就有了所谓的 task slots（至少一个）。\n每个 task slot 代表 TaskManager 中资源的固定子集。例如，具有 3 个 slot 的 TaskManager，会将其托管内存 1/3 用于每个 slot。分配资源意味着 subtask 不会与其他作业的 subtask 竞争托管内存，而是具有一定数量的保留托管内存。注意此处没有 CPU 隔离；当前 slot 仅分离 task 的托管内存。\n通过调整 task slot 的数量，用户可以定义 subtask 如何互相隔离。每个 TaskManager 有一个 slot，这意味着每个 task 组都在单独的 JVM 中运行（例如，可以在单独的容器中启动）。具有多个 slot 意味着更多 subtask 共享同一 JVM。同一 JVM 中的 task 共享 TCP 连接（通过多路复用）和心跳信息。它们还可以共享数据集和数据结构，从而减少了每个 task 的开销。\n默认情况下，Flink 允许 subtask 共享 slot，即便它们是不同的 task 的 subtask，只要是来自于同一作业即可。结果就是一个 slot 可以持有整个作业管道。\n允许 slot 共享有两个主要优点：\nFlink 集群所需的 task slot 和作业中使用的最大并行度恰好一样。无需计算程序总共包含多少个 task（具有不同并行度）。 容易获得更好的资源利用。如果没有 slot 共享，非密集 subtask（source/map()）将阻塞和密集型 subtask（window）一样多的资源。通过 slot 共享，我们示例中的基本并行度从 2 增加到 6，可以充分利用分配的资源，同时确保繁重的 subtask 在 TaskManager 之间公平分配。 4. 参考文献 # flink-architecture "},{"id":60,"href":"/posts/apache-hbase/ch05/","title":"Ch05-HBase 之 MemStore","section":"Blog","content":"HBase memstore 也被称为写缓存，优化大概可以分为三个阶段，其中每一种都是在上一次的基础上进行不断优化得到。memstore 最底层的数据结构都是 skiplist。\n1. 分类 # 1.1 DefaultMemstore # 1.1.1 引入 MemStoreLAB # 对于 HBase 这样基于 LSM 实现的 MemStore 来说，上述实现方案每写入一个 KeyValue，在没有写入 ConcurrentSkipList 之前就需要申请一个内存对象，这些对象在 flush 之前会一直存在，随着 GC 这些内存对象可能会晋升到老生代，尤其执行完 Major GC 后会存在大量的非常小的内存碎片，这些内存碎片会引起频繁的 Full GC，而且每次 Full GC 的时间会异常的长。针对上面的问题，MemStore 借鉴 TLAB（Thread Local Allocation Buffer）机制，实现了 MemStoreLAB，简称 MSLAB。\n1.1.1.1 原理 # 一个 KeyValue 写入之后不再单独为 KeyValue 申请内存，而是提前申请好一个 2M 大小的内存区域（Chunk）。 将写入的 KeyValue 顺序复制到申请的 Chunk 中，一旦 Chunk 写满，再申请下一个 Chunk。 将 KeyValue 复制到 Chunk 中后，生成一个 Cell 对象（这个 Cell 对象在源码中为 ByteBufferChunkKeyValue），这个 Cell 对象指向 Chunk 中的 KeyValue 内存区域。 将这个 Cell 对象作为 Key 和 Value 写入 ConcurrentSkipListMap 中。 原生的 KeyValue 对象写入到 Chunk 之后就没有再被引用，所以很快就会被 Young GC 回收掉。 1.1.1.2 配置 # hbase.hregion.memstore.mslab.enabled=true 1.1.2 引入 ChunkPool # MSLAB 机制中 KeyValue 写入 Chunk，如果 Chunk 写满了会在 JVM 堆内存申请一个新的 Chunk。引入 ChunkPool 后，申请 Chunk 都从 ChunkPool 中申请，如果 ChunkPool 中没有可用的空闲 Chunk，才会从 JVM 堆内存中申请新 Chunk。如果一个 MemStore 执行 flush 操作后，这个 MemStore 对应的所有 Chunk 都可以被回收，回收后重新进入池子中，以备下次使用。\n1.1.2.1 原理 # 每个 RegionServer 会有一个全局的 Chunk 管理器，负责 Chunk 的生成、回收等。MemStore 申请 Chunk 对象会发送请求让 Chunk 管理器创建新 Chunk，Chunk 管理器会检查当前是否有空闲 Chunk，如果有空闲 Chunk，就会将这个 Chunk 对象分配给 MemStore，否则从 JVM 堆上重新申请。每个 MemStore 仅持有 Chunk 内存区域的引用，如图中 MemStoreLAB 的小格子。\n1.1.2.2 配置 # hbase.hregion.memstore.chunkpool.maxsize=2097152 1.1.3 引入 Offheap # 除过 ChunkPool 之外，HBase 2.x 版本针对 Chunk 对象优化的另一个思路是将 Chunk 使用的这部分内存放到的堆外。\n1.1.3.1 原理 # Chunk 堆外化实现比较简单，在创建新 Chunk 时根据用户配置选择是否使用堆外内存，如果使用堆外内存，就使用 JDK 提供的 ByteBuffer.allocateDirect 方法在堆外申请特定大小的内存区域，否则使用 ByteBuffer.allocate 方法在堆内申请。如果不做配置，默认使用堆内内存，用户可以设置 hbase.regionserver.offheap.global.memstore.size 这个值为大于 0 的值开启堆外，表示 RegionServer 中所有 MemStore 可以使用的堆外内存总大小。\n1.1.3.2 配置 # hbase.regionserver.offheap.global.memstore.size=0 1.2 CompactingMemStore # 1.2.1 原理\n將 Chunk 做一次封裝形成 Segment，能夠支持写入操作的叫做 MutableSegment，不能支持的叫做 ImmutableSegment。当 MutableSegment 的大小超过 2M，就会执行 In-memory Flush 操作，将 MutableSegment 变为 ImmutableSegment，并重新生成一个新的 MutableSegment 接收写入。每次执行完 In-memory Flush 之后，RegionServer 都会启动一个异步线程执行 In-memory Compaction，将 Chunk 复制到（这里没有深拷贝）数组里面。\n如果参与 Compaction 的 Segment 个数超过 1 个，会有两种 Compaction 的形式：Merge 和 Compact。\nMerge：顺序遍历 Segment，取出对应的 Cell，然后顺序写入 CellChunkImmutableSegment 中 Compact：顺序遍历 Segment，读取对应 Cell 之后会检测是否有多个版本的 Cell，如果存在超过设置版本数的 Cell，就将老版本的 Cell 删掉。因为存在原始 KV 的变更，所以新生成的 Chunk 会进行重建，将其顺序写入到 CellChunkImmutableSegment 中 1.2.2 配置\nhbase.hregion.compacting.memstore.type=BASIC hbase.hregion.compacting.pipeline.segments.limit=2 hbase.memstore.inmemoryflush.threshold.factor=0.014 1.3 CCSMapMemStore # 阿里巴巴提出，尚未合入 HBase。CCSMapMemStore 是将整个跳表实现替换掉，无论是 active 还是 snapshot，从最大程度上减少了对象的创建，也避免了 CompactingMemstore 中 compact 带来的开销。\n2. 参考文献 # HBase 内存管理之 MemStore 进化论 从数据结构比较 HBase 的 3 种 memstore 实现方案 "},{"id":61,"href":"/posts/antlr4/ch01/","title":"Ch01-Antlr V4 介绍","section":"Blog","content":"ANTLR（ANother Tool for Language Recognition）是一个强大的解析器生成器，用于读取、处理、执行或翻译结构化文本或二进制文件。它被广泛用于构建语言、工具和框架。ANTLR 根据语法定义生成解析器，解析器可以构建和遍历解析树。\n1. 基本信息 # 条目 说明 官网 http://antlr.org/ 下载地址 https://github.com/antlr/antlr4 2. 基本概念 # 词法分析器 (Lexer) 词法分析是指在计算机科学中，将字符序列转换为单词 (Token) 的过程。词法分析器 (Lexer) 一般是用来供语法解析器 (Parser) 调用的。 语法解析器 (Parser) 语法解析器通常作为编译器或解释器出现。它的作用是进行语法检查，并构建由输入单词 (Token) 组成的数据结构 (即抽象语法树)。语法解析器通常使用词法分析器 (Lexer) 从输入字符流中分离出一个个的单词 (Token)，并将单词 (Token) 流作为其输入。实际开发中，语法解析器可以手工编写，也可以使用工具自动生成。 抽象语法树 (Abstract Syntax Tree，AST) 抽象语法树是源代码结构的一种抽象表示，它以树的形状表示语言的语法结构。抽象语法树一般可以用来进行代码语法的检查，代码风格的检查，代码的格式化，代码的高亮，代码的错误提示以及代码的自动补全等等。 3. 访问模式 # ANTLR 提供了两种方法来访问 ParseTree，一种是通过 Parse-Tree Listener 的方法，另一种是通过 Parse-Tree Visitor 的方法。\n条目 说明 Listener 访问 AST 的所有节点；重写（Override）进入时（enterXXX 方法）和退出时（exitXXX 方法）要执行的方法；要重写的方法没有返回值，因此需要在属性中保留所需的值 Visitor 并非所有 AST 节点都被访问；根据目的重写进入节点时要执行的过程（visitXXX 方法）；重写方法有一个返回值，因此您不必在属性中保存所需的值 "},{"id":62,"href":"/posts/apache-spark/ch10/","title":"Ch10-Spark 之内存管理","section":"Blog","content":"Spark 作为一个以擅长内存计算为优势的计算引擎，内存管理方案是其非常重要的模块；Spark 的内存可以大体归为两类：execution（运行内存）和 storage（存储内存），前者包括 shuffles、joins、sorts 和 aggregations 所需内存，后者包括 cache 和节点间数据传输所需内存；\n在 Spark 1.5 和之前版本里（Static Memory Manager），运行内存和存储内存是静态配置的，不支持借用；Spark 1.6 之后（Unified Memory Manager）引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，提供更好的性能。它们可以通过参数 spark.memory.useLegacyMode=true/false 配置。\n1. Static Memory Manager # 1.1 基本介绍 # Spark 最原始的内存管理模式，默认通过系统固定的内存配置参数，分配相应的 Storage、Execution 等内存空间，支持用户自定义修改配置。\n堆内内存空间整体被分为 Storage(存储内存)、Execution(执行内存)、Other(其他内存) 三部分，默认按照 6:2:2 的比率划分。其中 Storage 内存区域参数：spark.storage.memoryFraction(默认为 0.6)，Execution 内存区域参数：spark.shuffle.memoryFraction(默认为 0.2)。Other 内存区域主要用来存储用户定义的数据结构、Spark 内部元数据，占系统内存的 20%。\nStorage 内存区域中，10% 的大小被用作 Reserved 预留空间，防止内存溢出情况，由参数：spark.shuffle.safetyFraction(默认 0.1) 控制。90% 的空间当作可用的 Storage 内存，这里是 Executor 进行 RDD 数据缓存和 broadcast 数据的内存区域，参数和 Reserved 一致。还有一部分 Unroll 区域，这一块主要存储 Unroll 过程的数据，占用 20% 的可用 Storage 空间。 Execution 内存区域中，20% 的大小被用作 Reserved 预留空间，防止 OOM 和其他内存不够的情况，由参数：spark.shuffle.safetyFraction(默认 0.2) 控制。80% 的空间当作可用的 Execution 内存，缓存 shuffle 过程的中间数据，参数：spark.shuffle.safetyFraction(默认 0.8)。 堆外内存默认为 384M，由系统参数 spark.yarn.executor.memoryOverhead 设定。整体内存分为 Storage 和 Execution 两部分，此部分分配和堆内内存一致，由参数：spark.memory.storageFaction 决定。堆外内存一般存储序列化后的二进制数据 (字节流)，在存储空间中是一段连续的内存区域，其大小可精确计算，故此时无需设置预留空间。\n1.2 计算公式 # 可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction 可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 2. Unified Memory Manager # 2.1 基本介绍 # 为了解决 (Static Memory Manager) 静态内存管理的内存失衡等问题，Spark 在 1.6 之后使用了一种新的内存管理模式—Unified Memory Manager(统一内存管理)。在新模式下，移除了旧模式下的 Executor 内存静态占比分配，启用了内存动态占比机制，并将 Storage 和 Execution 划分为统一共享内存区域。\n堆内内存整体划分为 Usable Memory(可用内存) 和 Reversed Memory(预留内存) 两大部分。其中预留内存作为 OOM 等异常情况的内存使用区域，默认被分配 300M 的空间。可用内存可进一步分为 (Unified Memory) 统一内存和 Other 内存其他两部分，默认占比为 6:4。\n统一内存中的 Storage(存储内存) 和 Execution(执行内存) 以及 Other 内存，其参数及使用范围均与静态内存模式一致，不再重复赘述。只是此时的 Storage、Execution 之间启用了动态内存占用机制。\n2.1 动态内存占用机制 # 设置内存的初始值，即 Execution 和 Storage 均需设定各自的内存区域范围 (默认参数 0.5) 若存在一方内存不足，另一方内存空余时，可占用对方内存空间 双方内存均不足时，需落盘处理 Execution 内存被占用时，Storage 需将此部分转存硬盘并归还空间 Storage 内存被占用时，Execution 无需归还 堆外内存默认值为 384M，这个和静态管理模式一致。整体分为 Storage 和 Execution 两部分，且启用动态内存占用机制，其中默认的初始化占比值均为 0.5。\n2.3 计算公式 # 可用的存储\u0026amp;执行内存 = (systemMaxMemory -ReservedMemory) * spark.memoryFraction * spark.storage.storageFraction (启用内存动态分配机制，己方内存不足时可占用对方) 3. 参考文献 # Spark 内存管理 万字最全 Spark 内存管理详解 "},{"id":63,"href":"/posts/apache-spark/ch09/","title":"Ch09-Spark 之 BlockMananger","section":"Blog","content":"Spark 的一个重要特性是能够把计算结果数据保存到内存或磁盘中，供后面的操作读取，这就是 RDD 的缓存，这个过程也可称为 persist 或 caching（Spark 提供了 persist() 和 cache() 函数来缓存 RDD）。\n1. 原理介绍 # 1.1 基本架构 # BlockManager，运行在每个节点（driver 和 executors）上，提供接口用于读写本地和远程各种存储设备 (内存、磁盘和 off-heap)，是 Spark 存储体系中的核心组件。 Driver 端的 BlockManagerMaster 和 Executor 端的 BlockManager 组成了 Master-Slave 架构，其中 BlockManagerMaster 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 BlockManager 负责将 Block 的更新等状态上报到 BlockManagerMaster 以及 接收 BlockManagerMaster 的命令。\n组件 说明 MemoryStore 负责对内存中的数据进行操作 DiskStore 负责对磁盘上的数据进行处理 BlockTransferService 负责对远程节点上的数据进行读写操作。当执行 ShuffleRead 时，如数据在远程节点，则会通过该服务拉取所需数据。 ConnectionManager 创建当前节点 BlockManager 到远程其他节点的网络连接 1.2 工作流程 # 当 BlockManager 创建之后，首先向 Driver 所在的 BlockManagerMaster 注册，此时 BlockManagerMaster 会为该 BlockManager 创建对应的 BlockManagerInfo。BlockManagerInfo 管理集群中每个 Executor 中的 BlockManager 元数据信息，并存储到 BlockStatus 对象中。 当使用 BlockManager 执行写操作时，例如持久化 RDD 计算过程中的中间数据，此时会优先将数据写入内存中，如果内存不足且支持磁盘存储则会将数据写入磁盘。如果指定 Replica 主备模式，则会将数据通过 BlockTransferService 同步到其他节点。 当使用 BlockManager 执行了数据变更操作，则需要将 BlockStatus 信息同步到 BlockManagerMaster 节点上，并在对应的 BlockManagerInfo 更新 BlockStatus 对象，实现全局元数据的维护。 1.3 空间申请策略 # 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间。\nUnroll 概念\nRDD 在缓存到内存之前，partition 中 record 对象实例在堆内 other 内存区域中的不连续空间中存储。RDD 的缓存过程中，不连续存储空间内的 partition 被转换为连续存储空间的 Block 对象，并在 Storage 内存区域存储，此过程被称作为 Unroll(展开)。\n对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。 对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。 Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。\n非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用 ArrayList 进行管理； 序列化的 Block 则以 SerializedMemoryEntry 的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据，这些数据使用 LinkedHashMap 进行管理。 1.4 空间淘汰策略 # 由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。其淘汰规则为：\n被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存； 新旧 Block 不能属于同一个 RDD，避免循环淘汰； 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题； 遍历 LinkedHashMap 中的 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。 2. BlockManager 与 RDD 之间的关系 # RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID）。\n"},{"id":64,"href":"/posts/apache-spark/ch08/","title":"Ch08-Spark 之 Checkpoint","section":"Blog","content":"checkpoint 的机制保证了需要访问重复数据的应用 Spark 的 DAG 执行行图可能很庞大，task 中计算链可能会很长，这时如果 task 中途运行出错，那么 task 的整个需要重算非常耗时，因此，有必要将计算代价较大的 RDD checkpoint 一下，当下游 RDD 计算出错时，可以直接从 checkpoint 过的 RDD 那里读取数据继续算。\n1. Checkpoint 介绍 # checkpoint 在 spark 中主要有两处应用：\n在 spark core 中对 RDD 做 checkpoint，可以切断做 checkpoint RDD 的依赖关系，将 RDD 数据保存到可靠存储（如 HDFS）以便数据恢复； spark streaming 中使用 checkpoint 用来保存 DStreamGraph 以及相关配置信息，以便在 Driver 崩溃重启的时候能够接着之前进度继续进行处理（如之前 waiting batch 的 job 会在重启后继续处理）。 2. Spark Core Checkpoint # 2.1 使用方法 # sc.setCheckpointDir(checkpointDir.toString) val rdd = sc.makeRDD(1 to 20, numSlices = 1) rdd.checkpoint() 2.2 写入流程 # 对 RDD 调用了 checkpoint() 方法之后，它就接受 RDDCheckpointData 对象的管理。 RDDCheckpointData 对象会负责将调用了 checkpoint() 方法的 RDD 的状态，设置为 MarkedForCheckpoint。 在 RDD 所在的那个 job 运行结束之后，会调用 job 中，最后一个 RDD 的 doCheckPoint() 方法，该方法沿着 finalRDD 的 lineage 向上查找，将标记为 MarkedForCheckpoint 的 RDD 的标记改为 CheckpointingInProgress。 然后启动一个单独的 job，来将 lineage 中，标记为 CheckpointingInProgress 的 RDD，进行 checkpoint 的操作，也就是将这个 RDD 中每个 parition 的数据依次写入到 checkpoint 目录（writePartitionToCheckpointFile），此外如果该 RDD 中的 partitioner 如果不为空，则也会将该对象序列化后存储到 Sparkcontext.setCheckpointDir() 方法设置的文件系统中。 2.3 读取流程 # 在做完 checkpoint 后，获取原来 RDD 的依赖以及 partitions 数据都将从 CheckpointRDD 中获取。也就是说获取原来 rdd 中每个 partition 数据以及 partitioner 等对象，都将转移到 CheckPointRDD 中。\n在 CheckPointRDD 的一个具体实现 ReliableRDDCheckpintRDD 中的 compute 方法中可以看到，将会从 hdfs 的 checkpoint 目录中恢复之前写入的 partition 数据。而 partitioner 对象（如果有）也会从之前写入 hdfs 的 paritioner 对象恢复。\n3. Spark Streaming Checkpoint # 3.1 使用方法 # // Function to create and setup a new StreamingContext def functionToCreateContext(): StreamingContext = { val ssc = new StreamingContext(...) // new context val lines = ssc.socketTextStream(...) // create DStreams ... ssc.checkpoint(checkpointDirectory) // set checkpoint directory ssc } // Get StreamingContext from checkpoint data or create a new one val context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _) 3.2 写入流程 # 在 spark streaming 中，jobGenerator 会定期生成任务（jobGenerator#generateJobs)。在任务生成后将会调用 doCheckpoint 方法对系统做 checkpoint。此外，在当前批次任务结束，清理 metadata 时，也会调用 doCheckpoint 方法。\ncheckpoint 的主要逻辑基本都在 JobGenerator#doCheckpoint 中，首先更新当前时间段需要做 checkpoint RDD 的相关信息（如 DirectKafkaInputDStream，将已经生成的 RDD 信息的时间，topic，partition，offset 等相关信息进行更新。）接着会通过 checkpointWriter 将 Checkpoint 对象写入到 checkpoint 目录中（CheckPoint#write -\u0026gt; CheckpointWriteHandle），这样就完成了 checkpoint 的写入。\n3.3 读取流程 # 在 spark streaming 任务从 checkpoint 恢复 streamingContext 时，将会触发对之前保存的 checkpoint 对象的读取动作。在 StreamingContext 的 getOrCreate 方法中，通过 checkpoint#read 方法从 checkpoint 目录中恢复之前保存的 Checkpoint 对象。一旦该对象存在，将使用 Checkpoint 创建 streamingContext。于此同时，在 StreamingContext 中 DStreamGraph 的恢复借助之前保存的对象，并且调用 restoreCheckpointData 恢复之前生成而未计算的 RDD，从而接着之前的进度进行数据处理。\n4. Checkpoint VS Persist # persist 只是将数据保存在 BlockManager 中，但是 RDD 的 lineage(血缘关系，依赖关系) 是不变的。checkpoint 执行完之后，rdd 已经没有之前所谓的依赖 rdd 了，而只有一个强行为其设置的 checkpointRDD，checkpoint 之后 rdd 的 lineage 就改变了。\npersist 的数据丢失的可能性更大，因为节点的故障会导致磁盘、内存的数据丢失。但是 checkpoint 的数据通常是保存在高可用的文件系统中，比如 HDFS 中，所以数据丢失可能性比较低。\n5. 参考文献 # spark checkpoint 详解 "},{"id":65,"href":"/posts/apache-spark/ch07/","title":"Ch07-Spark 之缓存","section":"Blog","content":"Spark 的一个重要特性是能够把计算结果数据保存到内存或磁盘中，供后面的操作读取，这就是 RDD 的缓存，这个过程也可称为 persist 或 caching（Spark 提供了 persist() 和 cache() 函数来缓存 RDD）。\n1. persist # persist() 函数只是对 RDD 的 storageLevel（存储级别）进行了设置，函数内部并没有执行任何数据缓存的动作。\n在通过 RDD 的 iterator 读取 RDD 分区数据时，判断 RDD 的 storageLevel 变量，若该变量的值不是默认值 StorageLevel.NONE，则说明 storageLevel 的值已经被 persist 函数配置过。进入第 2 步。 storageLevel 不为 StorageLevel.NONE，则首先从内存或磁盘中获取数据，此时会调用 BlockManager#get 函数根据 storageLevel 从内存或磁盘中获取数据块。若获取到了数据块则直接返回。没有从内存或磁盘获取到数据块，则需要计算数据块，进入第 3 步。 若从内存或磁盘中没有获取到数据，则需要计算该数据。计算完成后，会根据存储级别（storageLevel 的值）把计算出来的分区数据保存到内存或磁盘中，供下一次读取。 1.1 StorageLevel # 持久化级别 说明 MEMORY_ONLY 将 RDD 存储为 JVM 中的反序列化 Java 对象。如果 RDD 不存在内存时，则某些分区将不会被缓存，并且每次需要时都会重新计算。 MEMORY_AND_DISK 将 RDD 存储为 JVM 中的反序列化 Java 对象。如果 RDD 不适合保存在内存中，则可以保存在磁盘中，需要时可以从磁盘读取。 MEMORY_ONLY_SER 将 RDD 存储为 JVM 中的序列化 Java 对象。通常比反序列化对象更节省空间，特别是在使用快速序列化器时，但是读取 CPU 密集程度更高。 MEMORY_AND_DISK_SER 与 MEMORY_ONLY_SER 类似，但会将不适合内存的分区溢出到磁盘。而不是每次需要时在实时计算它们。 DISK_ONLY 仅将 RDD 分区存储在磁盘上。 MEMORY_ONLY_2MEMORY_AND_DISK_2 将 RDD 在其他节点上再保存一份，在数据丢失时，不需要再次计算。 OFF_HEAP RDD 存储在对外内存中。 2. cache # def cache(): this.type = persist() "},{"id":66,"href":"/posts/apache-spark/ch06/","title":"Ch06-Spark 之容错机制","section":"Blog","content":"Spark 并不直接对数据进行处理，而是将数据抽象成了分布式数据集这种数据结构。目前该数据结构主要经历了三代变迁。\n1. 作业容错 # Spark RDD 实现基于 Lineage 的容错机制，基于 RDD 的各项 transformation 构成了 compute chain，在部分计算结果丢失的时候可以根据 Lineage 重新恢复计算。\n在窄依赖中，在子 RDD 的分区丢失，要重算父 RDD 分区时，父 RDD 相应分区的所有数据都是子 RDD 分区的数据，并不存在冗余计算。 在宽依赖情况下，丢失一个子 RDD 分区，重算的每个父 RDD 的每个分区的所有数据并不是都给丢失的子 RDD 分区用的，会有一部分数据相当于对应的是未丢失的子 RDD 分区中需要的数据，整个 RDD 都要重新计算，这样就会产生冗余计算开销和巨大的性能浪费。所以如果调用链路比较长的话，宽依赖最好做一次 Checkpoint。 Lineage 过长会造成容错成本过高，这时在中间阶段做 Checkpoint 容错，如果之后有节点出现问题而丢失分区，从 Checkpoint 的 RDD 开始重做 Lineage，就会减少开销。\n2. 守护进程容错 # Master 和 Worker 运行在 Yarn 上面后，借助 Yarn 的容错（重启）机制，来实现守护作业容错。\n3. 总结 # Worker 或者 Executor 异常退出没关系，Spark stage 会重新执行调度。如果 Spark job 链路过长的话，建议在宽依赖那里执行 CheckPoint，加快 spark job 的恢复\n4. 参考文献 # Spark 容错机制 Spark 的容错机制 "},{"id":67,"href":"/posts/apache-spark/ch05/","title":"Ch05-Spark 之数据抽象","section":"Blog","content":"Spark 并不直接对数据进行处理，而是将数据抽象成了分布式数据集这种数据结构。目前该数据结构主要经历了三代变迁。\n1. RDD # RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。 RDD 算子按照返回是否是 RDD 可以分为 Transformation 算子（返回 RDD）和 Action 算子（不返回 RDD）。因此 RDD 与 RDD 之间的依赖只会存在于 Transformation 算子中。\n分类 说明 窄依赖 每个父 RDD 的一个 Partition 最多被子 RDD 的一个 Partition 所使用，例如 map、filter、union 等操作都会产生窄依赖（多对一）。 宽依赖 一个父 RDD 的 Partition 会被多个子 RDD 的 Partition 所使用，例如 groupByKey、reduceByKey、sortByKey 等操作都 2. DataFrame # DataFrame 是 DataSet 的一个特例 type DataFrame = DataSet[Row]，相比较 RDD 而言额外提供了 Schema 等信息。DataFrame 每一行的类型固定为 Row, 每一列的值没法直接访问，只有通过解析才能获取各个字段的值。\n3. DataSet # Dataset 是 DataFrame API 的扩展，它提供了类型安全 (type-safe)，面向对象 (object-oriented) 的编程接口。Dataset 利用 Catalyst optimizer 可以让用户通过类似于 sql 的表达式对数据进行查询。\n4. 数据抽象的区别 # "},{"id":68,"href":"/posts/apache-spark/ch04/","title":"Ch04-Spark 之 Shuffle","section":"Blog","content":"Spark Shuffle 经过了若干次优化。\n1. Shuffle 分类 # 1.1 Hash Shuffle # 1.2 Sort Shuffle # 2. 参考文献 # Spark 的两种核心 Shuffle 详解（建议收藏） Spark-Shuffle 梳理 "},{"id":69,"href":"/posts/apache-spark/ch03/","title":"Ch03-Spark 之 Job 执行流程","section":"Blog","content":"Spark Job 执行流程。\n1. 执行流程 # 构建 Spark Application 的运行环境，启动 SparkContext SparkContext 向资源管理器（可以是 Standalone，Mesos，Yarn）申请运行 Executor 资源 SparkContext 将应用程序分发给 Executor SparkContext 构建成 DAG 图，将 DAG 图分解成 Stage、将 Taskset 发送给 Task Scheduler，最后由 Task Scheduler 将 Task 发送给 Executor 运行 Task 在 Executor 上运行，运行完释放所有资源 这张图不是非常准确\nDAGScheduler 输入 DAG，输出 Stage TaskScheduler 输入 Stage，输出 TaskSet TaskSetManager 输入 TaskSet，输出若干 Task 2. 其他细节 # 2.1 划分依据 # Job 划分依据是 Action 算子 Stage 划分依据是 RDD 宽窄依赖 Task 划分是依据 RDD 的分区数（并行度） 2.2 划分算法 # 核心算法是回溯算法，从后往前回溯/反向解析，遇到窄依赖加入本 Stage，遇见宽依赖进行 Stage 切分。\nSpark 内核会从触发 Action 操作的那个 RDD 开始从后往前推，首先会为最后一个 RDD 创建一个 Stage，然后继续倒推，如果发现对某个 RDD 是宽依赖，那么就会将宽依赖的那个 RDD 创建一个新的 Stage，那个 RDD 就是新的 Stage 的最后一个 RDD。然后依次类推，继续倒推，根据窄依赖或者宽依赖进行 Stage 的划分，直到所有的 RDD 全部遍历完成为止。\n2.3 监控 # DAGScheduler 监控 Job，Task 的运行状态以及 Executor 的生命状态。\n2.4 结果返回 # 一个具体的任务在 Executor 中执行完毕后，其结果需要以某种形式返回给 DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。\n对于 FinalStage 所对应的任务，返回给 DAGScheduler 的是运算结果本身； 对于 中间调度阶段对应的任务 ShuffleMapTask，返回给 DAGScheduler 的是一个 MapStatus 里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。 "},{"id":70,"href":"/posts/apache-kafka/ch12/","title":"Ch12-Kafka 为什么这么快","section":"Blog","content":"Apache Kafka 以牺牲延迟和抖动为代价优化了吞吐量，但并没有牺牲，比如持久性、严格的记录有序性和至少一次的分发语义。当有人说“Kafka 速度很快”，并假设他们至少有一定的能力时，你可以认为他们指的是 Kafka 在短时间内分发大量记录的能力。\n1. Broker 优化 # 1.1 顺序写磁盘 # Kafka 利用分段、追加日志的方式，在很大程度上将读写限制为顺序 I/O（sequential I/O）。 在一块普通的 7200 RPM SATA 硬盘上，随机 I/O（random I/O）与顺序I/O相比，随机I/O的性能要比顺序I/O慢3到4个数量级。\n1.2 充分利用 Page Cache # Broker 收到数据后，写磁盘时只是将数据写入 Page Cache，并不保证数据一定完全写入磁盘。如果数据消费速度与生产速度相当，甚至不需要通过物理磁盘交换数据，而是直接通过 Page Cache 交换数据。同时，Follower 从 Leader Fetch 数据时，也可通过 Page Cache 完成。 I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能，还会将一些写操作重新按顺序排好，从而减少磁盘头的移动时间。\n1.3 支持多 Disk Drive # Broker 的 log.dirs 配置项，允许配置多个文件夹。如果机器上有多个 Disk Drive，可将不同的 Disk 挂载到不同的目录，然后将这些目录都配置到 log.dirs 里。Kafka 会尽可能将不同的 Partition 分配到不同的目录，也即不同的 Disk 上，从而充分利用了多 Disk 的优势。\n1.4 零拷贝 # Kafka 使用了 Zero Copy 技术提升了消费的效率。前面所说的 Kafka 将消息先写入页缓存，如果消费者在读取消息的时候如果在页缓存中可以命中，那么可以直接从页缓存中读取，这样又节省了一次从磁盘到页缓存的 copy 开销。文件大小从几 MB 到 1GB 的范围内，传统拷贝和零拷贝相比，结果显示零拷贝的性能提高了两到三倍。\n2. Producer 优化 # 2.1 批量发送 # Producer 支持一次性发送多条消息。由于每次网络传输，除了传输消息本身以外，还要传输非常多的网络协议本身的一些内容（称为 Overhead），所以将多条消息合并到一起传输，可有效减少网络传输的 Overhead，进而提高了传输效率。\n2.2 高效的序列化方式 # Kafka 消息的 Key 和 Payload（或者说 Value）的类型可自定义，只需同时提供相应的序列化器和反序列化器即可。因此用户可以通过使用快速且紧凑的序列化 - 反序列化方式（如 Avro，Protocal Buffer）来减少实际网络传输和磁盘存储的数据规模，从而提高吞吐率。\n3. Consumer 优化 # 3.1 数据压缩 # Broker 接收消息后，并不直接解压缩，而是直接将消息以压缩后的形式持久化到磁盘。Consumer Fetch 到数据后再解压缩。因此 Kafka 的压缩不仅减少了 Producer 到 Broker 的网络传输负载，同时也降低了 Broker 磁盘操作的负载，也降低了 Consumer 与 Broker 间的网络传输量，从而极大得提高了传输效率，提高了吞吐量。\n4. 参考文献 # kafka——高效读写数据 "},{"id":71,"href":"/posts/apache-hbase/ch04/","title":"Ch04-HBase 之 HLog","section":"Blog","content":"Write-ahead logs (WALs)，HBase 2.0 之前，WAL 接口的实现是 HLog，所以先前大家也会将 WAL 称为 HLog。\n在 HBase 2.0 之前，WAL 切分用的是 Split WAL based on ZK Coordination(常见的策略有 LogSpliting，Distributed Log Spliting，Distributed Log Replay，但是现在这些策略已经全部废除)。目前使用的是 WAL Splitting Based on Procedure V2。\n1. 切分策略 # 1.1 Split WAL based on ZK Coordination # LogSpliting 这种算法是 HMaser 读取 HLog 文件，然后将其拆分到不同的 Region 目录下面，当 RegionServer 启动的时候，回放这些 HLog 文件，这样就完成了 HLog 的数据恢复工作，缺点也很明显严重依赖 HMaster，效率比较低。 Distributed Log Spliting 这种算法则是 HMaster 将 HLog 包装成任务发布到 Zookeeper 的 /hbase/splitWAL 节点上，RegionServer 启动的时候通过竞争的方式去获取任务，然后将 HLog 复制到不同的 Region 目录下面进行回放。 Distributed Log Replay 这种算法与 Distributed Log Spliting 类似，只是少了将 HLog 复制到 Region 目录的过程，RegionServer 启动的时候直接回放。 1.2 Split WAL based on Procedure v2 # 2. 文件结构 # WAL 的实现类为 HLog，每个 Region Server 拥有一个 HLog 日志，所有 region 的写入都是写到同一个 HLog。下图表示同一个 Region Server 中的 3 个 region 共享一个 HLog。当数据写入时，是将数据对\u0026lt;HLogKey,WALEdit\u0026gt;按照顺序追加到 HLog 中，以获取最好的写入性能。\nHLogKey 主要存储了 log sequence number，更新时间 write time，region name，table name 以及 cluster ids。其中 log sequncece number 作为 HFile 中一个重要的元数据，和 HLog 的生命周期息息相关；region name 和 table name 分别表征该段日志属于哪个 region 以及哪张表；cluster ids 用于将日志复制到集群中其他机器上。\nWALEdit 用来表示一个事务中的更新集合，在之前的版本，如果一个事务中对一行 row R 中三列 c1，c2，c3 分别做了修改，那么 hlog 中会有 3 个对应的日志片段如下所示：\n\u0026lt;logseq1-for-edit1\u0026gt;:\u0026lt;keyvalue-for-edit-c1\u0026gt; \u0026lt;logseq2-for-edit2\u0026gt;:\u0026lt;keyvalue-for-edit-c2\u0026gt; \u0026lt;logseq3-for-edit3\u0026gt;:\u0026lt;keyvalue-for-edit-c3\u0026gt; 然而，这种日志结构无法保证行级事务的原子性，假如刚好更新到 c2 之后发生宕机，那么就会产生只有部分日志写入成功的现象。为此，hbase 将所有对同一行的更新操作都表示为一个记录，如下：\n\u0026lt;logseq#-for-entire-txn\u0026gt;:\u0026lt;WALEdit-for-entire-txn\u0026gt; 其中 WALEdit 会被序列化为格式\u0026lt;-1, # of edits, \u0026lt;KeyValue\u0026gt;, \u0026lt;KeyValue\u0026gt;, \u0026lt;KeyValue\u0026gt;\u0026gt;，比如\u0026lt;-1, 3, \u0026lt;keyvalue-for-edit-c1\u0026gt;, \u0026lt;keyvalue-for-edit-c2\u0026gt;, \u0026lt;keyvalue-for-edit-c3\u0026gt;\u0026gt;，其中 -1 作为标示符表征这种新的日志结构。\n3. 写入流程 # 下图主要描述了 HRegion 中调用 append 和 sync 后，hbase 的 wal 线程流转模型。最左边是有多个 client 提交到 HRegion 的 append 和 sync 操作。\n当调用 append 后 WALEdit 和 WALKey 会被封装成 FSWALEntry 类进而再封装成 RinbBufferTruck 类放入一个线程安全的 Buffer(LMAX Disruptor RingBuffer) 中。 当调用 sync 后会生成一个 SyncFuture 进而封装成 RinbBufferTruck 类同样放入这个 Buffer 中，然后工作线程此时会被阻塞等待被 notify() 唤醒。\n在每个 Buffer 的最右边会有一个且只有一个线程专门去处理这些 RinbBufferTruck，如果是 FSWALEntry 则写入 HLog 文件。因为文件缓存的存在，这时候很可能 client 数据并没有落盘。所以进一步如果是 SyncFuture 会被批量的放到一个线程池中，异步的批量去刷盘，刷盘成功后唤醒工作线程完成 wal。\n4. Hlog 的生命周期 # Hlog 从产生到最后删除需要经历如下几个过程：\n产生 所有涉及到数据的变更都会先写 Hlog，除非是你关闭了 Hlog。\n滚动 Hlog 的大小通过参数 hbase.regionserver.logroll.period 控制，默认是 1 个小时，时间达到 hbase.regionserver.logroll.period 设置的时间，Hbase 会创建一个新的 Hlog 文件。这就实现了 Hlog 滚动的目的。Hbase 通过 hbase.regionserver.maxlogs 参数控制 Hlog 的个数。滚动的目的，为了控制单个 Hlog 文件过大的情况，方便后续的过期和删除。\n过期 前面我们有讲到 sequenceid 这个东东，Hlog 的过期依赖于对 sequenceid 的判断。Hbase 会将 Hlog 的 sequenceid 和 Hfile 最大的 sequenceid（刷新到的最新位置）进行比较，如果该 Hlog 文件中的 sequenceid 比刷新的最新位置的 sequenceid 都要小，那么这个 Hlog 就过期了，过期了以后，对应 Hlog 会被移动到.oldlogs 目录。\n这里有个问题，为什么要将过期的 Hlog 移动到.oldlogs 目录，而不是直接删除呢？ 答案是因为 Hbase 还有一个主从同步的功能，这个依赖 Hlog 来同步 Hbase 的变更，有一种情况不能删除 Hlog，那就是 Hlog 虽然过期，但是对应的 Hlog 并没有同步完成，因此比较好的做好是移动到别的目录。再增加对应的检查和保留时间。\n删除 如果 Hbase 开启了 replication，当 replication 执行完一个 Hlog 的时候，会删除 Zoopkeeper 上的对应 Hlog 节点。在 Hlog 被移动到.oldlogs 目录后，Hbase 每隔 hbase.master.cleaner.interval（默认 60 秒）时间会去检查.oldlogs 目录下的所有 Hlog，确认对应的 Zookeeper 的 Hlog 节点是否被删除，如果 Zookeeper 上不存在对应的 Hlog 节点，那么就直接删除对应的 Hlog。hbase.master.logcleaner.ttl（默认 10 分钟）这个参数设置 Hlog 在.oldlogs 目录保留的最长时间。\n5. 持久化等级 # 持久化等级 说明 SKIP_WAL 只写缓存，不写 HLog 日志。可以极大的提升写入性能，但是数据有丢失的风险。 ASYNC_WAL 异步将数据写入 HLog 日志中。 SYNC_WAL 同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘。 FSYNC_WAL 同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。 USER_DEFAULT 默认如果用户没有指定持久化等级，HBase 使用 SYNC_WAL 等级持久化数据。 6. 参考文献 # WAL Splitting Based on Procedure V2 HBase － 数据写入流程解析 Hbase 技术细节笔记（下） "},{"id":72,"href":"/posts/apache-kafka/ch11/","title":"Ch11-Kafka 之 Quota","section":"Blog","content":"kafka Quota\n1. 实现原理 # Kafka 集群可以对客户端请求进行 quota，控制集群资源的使用。Kafka broker 可以对客户端做两种类型资源的配额限制，同一个 group 的 client 共享 quota。\n定义字节率的阈值来限定网络带宽的 quota。 (从 0.9 版本开始) request 请求率的 quota，网络和 I/O线程 cpu 利用率的百分比。 (从 0.11 版本开始) 1.1 Network Bandwidth Quotas # Network Bandwidth Quotas 使用字节速率来定义每个 group 的客户端的共享 Quotas，默认情况下，每个 client 都会收到由集群配置的固定 quota(X bytes/sec)。这个quota是在每个broker基础上定义的。每个client在抵达这个quota之前，每秒可以向单个broker 发布/拉取任意bytes(小于给定的quota)的数据。\n1.2 Request Rate Quotas # Request Rate Quotas 定义了一个客户端可以使用 request handler I/O threads 和 network threads 在一个 quota 窗口内百分比。n% 的 quota 代表一个线程的 n%的使用率，所以这种 quota 的最大可配置参数是 ((num.io.threads + num.network.threads) * 100)%之上的。由于分配给 request handler I/O threads 和 network threads 的数量是基于 broker 的核数，所以 request rate quotas 表示的是共享 quota 的每组 client 可以使用的 CPU 百分比\n2. 注意事项 # 默认情况下，每个 client group 都可以接收到由 cluster 配置的 quota。This quota is defined on a per-broker basis。每个 client 在达到这个配置的阈值之前，可以随意使用。之所以在每个 broker 层面定义 quota，而不是在 cluster 层面定义 quota，主要是因为在 cluster 层面定义 quota 需要一种机制来同步每个 broker 的 quota 的用量，这个实现起来比较复杂。\n当检测到某个 request 超过了 quota，broker 首先会去计算这个请求的需要的延迟时间，然后根据这个延迟时间延迟返回 response。对于 fetch request，response 将不会返回数据，接着该 broker 将不会响应来自该 client 的请求，直到这个延迟时间耗尽。对于 kafka client，如果发现了这种带延迟的 response，在延迟期间，将不再向 broker 发送请求。\n对于一些老版本的 client，不会理会这种带延迟的 response，会依旧向 broker 发送请求，broker 通过经过 sockets channel 实现的背压仍然可以实现对这种 client 的限制。那些向限制的 channel 发送 request 的 client 在延迟结束之后，才会收到 response。\nByte-rate 和 thread utilization 使用多个小窗口来衡量（例如 1 秒 30 个窗口），以便快速的检测和修正违反 quota 的 request。一般情况下，如果使用较大的测量窗口（例如 30 秒 10 个窗口）可能会导致大量的突发流量，接着出现长时间的延迟，最后严重影响客户体验。\n如果一个集群有 x 个 broker，一个 topic 有多个分区，这里假定是 y 个分区 (x \u0026gt; y)。对于一个 client 限制 quota 为 n bytes/s如果一个client对应一个topic，那么站在producer的角度而言，那么被限速大小应该是 y * n bytes/s，如果有m个producer，那么理想情况下，每个producer的被限速大小应该是 y * n / m bytes/s。 如果一个集群有 x 个 broker，一个 topic 有多个分区，这里假定是 y 个分区 (x \u0026lt; y)。对于一个 client 限制 quota 为 n bytes/s如果一个client对应一个topic，那么站在producer的角度而言，那么被限速大小应该是x * n bytes/s。如果有m个producer，那么理想情况下，每个producer的被限速大小应该是 x * n / m bytes/s。 如果有多个 producer，并且它们使用同一个 client。也就是说，假定最终限额为 1MB/s，如果 producerA 使用了256KB/s，那么 producerB 就只能使用 768KB/s了。 最混乱的是有两个 topic，且每个 topic 的分区数（小于 broker 数量）不一样，然后两个 producer 使用同一个 client（配置为 1bytes/s），那么站在每个 producer 的角度来看，其理论被限速大小应该是 x * 1 bytes/s和 y * 1 bytes/s，经过测试，发现两个producer的被限速大小约为 (x + y)/2 bytes/s。 "},{"id":73,"href":"/posts/apache-hbase/ch03/","title":"Ch03-HBase 之 CatalogTables","section":"Blog","content":"HBase Catalog Table\nHBase Catalog Tables 介绍 # HBase Catalog Tables 即 hbase:meta 表，之前的版本中也称为.META.，它存储了在系统中所有的 region 信息，而该表的位置信息被保存在 zookeeper 的/hbase-unsecure/meta-region-server中。\nhbase:meta 表结构 # 名称 说明 **** Key [table],[region start key],[region id] Values info:regioninfo Region Info，和.regioninfo 内容相同 info:server Region Server 地址和端口，如 slave1:16020 info:serverstartcode Region Server 启动 Code，实质上就是 Region Server 启动的时间戳 info:sn Region Server Node，由 server和serverstartcode组成，如slave1,16020,1557998852385 info:state Region 状态 info:seqnumDuringOpen 表示 Region 在线时长的一个二进制串 "},{"id":74,"href":"/posts/apache-kafka/ch10/","title":"Ch10-Kafka 之事务","section":"Blog","content":"kafka Transaction.\n1. 幂等性发送 # 为了实现 Producer 的幂等语义，Kafka 入了 Producer ID（即 PID）和 Sequence Number。每个新的 Producer 在初始化的时候会被分配一个唯一的 PID，该 PID 对用户完全透明而不会暴露给用户。\n对于每个 PID，该 Producer 发送数据的每个\u0026lt;Topic, Partition\u0026gt;都对应一个从 0 开始单调递增的 Sequence Number。类似地，Broker 端也会为每个\u0026lt;PID, Topic, Partition\u0026gt;维护一个序号，并且每次 Commit 一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比 Broker 维护的序号（即最后一次 Commit 的消息的序号）大一，则 Broker 会接受它，否则将其丢弃：\n如果消息序号比 Broker 维护的序号大 1 以上，说明中间有数据尚未写入，也即乱序，此时 Broker 拒绝该消息，Producer 抛出 InvalidSequenceNumber 如果消息序号小于等于 Broker 维护的序号，说明该消息已被保存，即为重复消息，Broker 直接丢弃该消息，Producer 抛出 DuplicateSequenceNumber 该幂等设计只能保证单个 Producer 对于同一个\u0026lt;Topic, Partition\u0026gt;的 Exactly Once 语义，它并不能保证写操作的原子性——即多个写操作，要么全部被 Commit 要么全部不被 Commit。\n2. 事务支持 # 为了支持事务，Kafka 引入了两个新的组件：Transaction Coordinator 和 Transaction Log。Transaction Coordinator 并不是一个单独的进程，而是 Broker 内部的一个线程，Transaction Log 则以 __transaction_state 这么个内部 Topic 形式存在。Transaction Log 每个分区都有一个 leader，该 leade 对应哪个 kafka broker，哪个 broker 上的 transaction coordinator 就负责对这些分区的写操作，其可靠性主要有 Kafka 自身的可靠性机制保证。\n整个事务的执行过程如下所示。\nKAFKA 生产者通过 initTransactions API 将自定义的 transactional.id 注册到 transactional coordinator。此时 coordinator 会关闭所有有相同 transactional.id 且处于 pending 状态的事务，同时也会递增 epoch 来屏蔽僵尸生产者（zombie producers）。该操作对每个 producer session 只执行一次 producer.initTransaction()。 KAFKA 生产者通过 beginTransaction API 开启事务，并通过 send API 发送消息到目标 topic。此时消息对应的 partition 会首先被注册到 transactional coordinator，然后 producer 按照正常流程发送消息到目标 topic，且在发送消息时内部会通过校验屏蔽掉僵尸生产者 producer.beginTransaction();producer.send()*N;。 KAFKA 生产者通过 commitTransaction API 提交事务或通过 abortTransaction API 回滚事务。此时会向 transactional coordinator 提交请求，开始两阶段提交协议 producer.commitTransaction();producer.abortTransaction(); 第一阶段，transactional coordinator 更新内存中的事务状态为“prepare_commit”，并将该状态持久化到 transaction log 中 第二阶段，coordinator 首先写 transaction marker 标记到目标 topic 的目标 partition，在向目标 topic 的目标 partition 写完控制消息后，会更新事务状态为“commited”或“abort”，并将该状态持久化到 transaction log 中。 3. 事务对性能的影响 # 3.1 对 Producer 的影响 # 开启事务后，producer 有些许写放大，这主要涉及到：\n事务开始前，producer 需要将生产的消息的 partition 注册到 transaction coordinator，这涉及到 rpc 调用； 事务结束时，transaction coordinator 需要注入 transaction marker 到消息对应的 Partition, 这也涉及到 rpc 调用； 事务进行过程中，transaction coordinator 需要将事务状态如“prepare_commit” “complete_commit”等持久化到 transaction log，这涉及到磁盘写；但由于以上写操作的时间复杂度，更多是跟消息涉及到的分区个数相关，而不是跟消息的具体条数相关，且 KAFKA 通过 batch 机制尽量减小了 RPC 调用次数，所以对 Producer 的性能影响并不大； Confluent 官网有性能测试相关博文，其中讲到“for a producer producing 1KB records at maximum throughput, committing messages every 100ms results in only a 3% degradation in throughput. Smaller messages or shorter transaction commit intervals would result in more severe degradation.“，即在他们的测试场景下，开启事务后性能只有 3% 的下降； 3.2 对 Consumer 的影响 # 开启事务后，对 consumer 的性能影响相对对 producer 的性能影响更小，consumer 仍然是轻量级高吞吐的，几乎没有性能影响：\nconsumer 在 read_committed 模式下，只需要额外做一些消息的过滤，即过滤掉回滚了的事务的 message，和 open 状态的事务的 message；但过滤这些消息时，因为 topic 中存储的消息包括正常的数据消息和控制消息，这些消息包含了足够的元数据信息来支持消息过滤，所以 Kafka consumer 不需要跟 transactional coordinator 进行 rpc 交互； consumer 也需要缓存读取的消息以等待事务的结束，而且由于底层仍然可以使用 zero-copy 机制来读取消息，所以相对于没有开启事务的 consumer，性能几乎没有影响。 4. 参考文献 # Kafka 设计解析（八）- Exactly Once 语义与事务机制原理 Kafka 事务「原理剖析」 "},{"id":75,"href":"/posts/apache-hbase/ch02/","title":"Ch02-HBase 之数据模型","section":"Blog","content":"逻辑上，HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从 HBase 的底层物理存储结构 (K-V) 来看，HBase 更像是一个 multi-dimensional map。\n1. 基本概念 # 术语 说明 NameSpace 类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。HBase 有两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表，default 表是用户默认使用的命名空间。 Table 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往 HBase 写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase 能够轻松应对字段变更的场景 Row 表中的每行数据都由一个 RowKey 和多个 Column(列) 组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重要。 Column HBase 中的每个列都由 Column Family(列族) 和 Column Qualifier(列限定符) 进行限 定，例如 info:name，info:age。建表时，只需指明列族，而列限定符无需预先定义 TimeStamp 用于标识数据的不同版本 (version)，每条数据写入时，如果不指定时间戳，系统会 自动为其加上该字段，其值为写入 HBase 的时间。 Cell 由{rowkey, column_family:column_qualifier, timeStamp}唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存储。 2. 逻辑结构 # 3. 物理结构 # "},{"id":76,"href":"/posts/apache-hbase/ch01/","title":"Ch01-HBase 介绍","section":"Blog","content":"HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL 数据库。\n1. 基本信息 # 条目 说明 官网 https://hbase.apache.org/ 下载地址 https://hbase.apache.org/downloads.html 2. 架构介绍 # 2.1 基本组成 # HBase 的核心架构由五部分组成，分别是 HBase Client、HBase HMaster、HBase Region Server、ZooKeeper 以及 HDFS。它的架构组成如下图所示。\n组件 说明 HBase Client 为用户提供了访问 HBase 的接口维护了对应的 cache 来加速 Hbase 的访问，比如缓存元数据的信息 HBase Master 分配 Region 负载均衡（将数据均匀分配到不同的 RegionServer；将用户请求分配到不同的 RegionServer）维护数据，发现失效的 Region，并将失效的 Region 分配到正常的 RegionServer 上 HBase RegionServer 管理 HMaster 为其分配的 Region 负责与底层的 HDFS 交互，存储数据到 HDFS 负责 Region 变大以后的拆分以及 StoreFile 的合并工作 Apache Zookeeper 选举 HMaster，当 HMaster 宕机后，通知其他的 Master 参与选举，直到选举出 HMaster 监控 Region Server，当 RegionServer 宕机后，通过回调的形式通知 HMaster 有关 Region Server 上下线的信息维护元数据和集群配置 Apache HDFS 提供底层数据存储服务，同时为 HBase 提供高可用的支持 2.2 相互联系 # ZooKeeper 用来协调分布式系统的成员之间共享的状态信息。Region Server 及 HMaster 也与 ZooKeeper 连接。ZooKeeper 通过心跳信息为活跃的连接维持相应的 ephemeral node。\n每一个 Region server 都在 ZooKeeper 中创建相应的 ephemeral node，HMaster 通过监控这些 ephemeral node 的状态来发现正常工作的或发生故障下线的 Region server。\nHMaster 之间通过互相竞争创建 ephemeral node 进行 Master 选举。ZooKeeper 会选出区中第一个创建成功的作为唯一一个活跃的 HMaster。活跃的 HMaster 向 ZooKeeper 发送心跳信息来表明自己在线的状态。不活跃的 HMaster 则监听活跃 HMaster 的状态，并在活跃 HMaster 发生故障下线之后重新选举，从而实现了 HBase 的高可用性。\n如果 Region server 或者 HMaster 不能成功向 ZooKeeper 发送心跳信息，则其与 ZooKeeper 的连接超时之后与之相应的 ephemeral node 就会被删除。监听这些 ephemeral node 状态的其他节点就会得到相应 node 不存在的信息，从而进行相应的处理。比如活跃的 HMaster 监听 Region Server 的信息，并在其下线后重新分配 Region server 来恢复相应的服务。不活跃的 HMaster 监听活跃 HMaster 的信息，并在下线后重新选出活跃的 HMaster 进行服务。\n2.3 Region # 一个 Region Server 包含多个 Region。\nHLog：即 Write Ahead Log。负责记录着数据的操作日志，当 HBase 出现故障时可以进行日志重放、故障恢复。例如，磁盘掉电导致 MemStore 中的数据没有持久化存储到 StoreFile，这时就可以通过 HLog 日志重放来恢复数据。 Region：每一个 Region 都有起始 RowKey 和结束 RowKey，代表了存储的 Row 的范围，保存着表中某段连续的数据。一开始每个表都只有一个 Region，随着数据量不断增加，当 Region 大小达到一个阈值时，Region 就会被 Regio Server 水平切分成两个新的 Region。当 Region 很多时，HMaster 会将 Region 保存到其他 Region Server 上。 Store：一个 Region 由多个 Store 组成，每个 Store 都对应一个 Column Family, Store 包含 MemStore 和 StoreFile。 MemStore：作为 HBase 的内存数据存储，数据的写操作会先写到 MemStore 中，当 MemStore 中的数据增长到一个阈值（默认 64M）后，Region Server 会启动 flasheatch 进程将 MemStore 中的数据写人 StoreFile 持久化存储，每次写入后都形成一个单独的 StoreFile。当客户端检索数据时，先在 MemStore 中查找，如果 MemStore 中不存在，则会在 StoreFile 中继续查找。 StoreFile：MemStore 内存中的数据写到文件后就是 StoreFile，StoreFile 底层是以 HFile 的格式保存。HBase 以 Store 的大小来判断是否需要切分 Region。当一个 Region 中所有 StoreFile 的大小和数量都增长到超过一个阈值时，HMaster 会把当前 Region 分割为两个，并分配到其他 Region Server 上，实现负载均衡。 HFile：HFile 和 StoreFile 是同一个文件，只不过站在 HDFS 的角度称这个文件为 HFile，站在 HBase 的角度就称这个文件为 StoreFile。\n3. 数据模型 # HBase 是一个面向列式存储的分布式数据库，它的数据模型与 BigTable 十分相似。在 HBase 表中，一条数据拥有一个全局唯一的键 (RowKey) 和任意数量的列 (Column)，一列或多列组成一个列族 (Column Family)，同一个列族中列的数据在物理上都存储在同一个 HFile 中，这样基于列存储的数据结构有利于数据缓存和查询。\n4. 参考文献 # An In-Depth Look at the HBase Architecture Hbase 的架构详解 "},{"id":77,"href":"/posts/apache-kafka/ch09/","title":"Ch09-Kafka 之高可用","section":"Blog","content":"kafka HA\n1. 分区副本机制 # 同一个 partition 使用多个 replica，并从中选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replica 作为 follower 从 leader 中复制数据。如果 replica 挂了，那么 leader 继续对外保证服务。如果 leader 挂了，那么尝试这从 replica 中选举 leader 对外提供服务。\n2. Broker # 如果 Broker 宕机，那么 Controller 借助 Zookeeper 感知到该 Broker 宕机，然后 Controller 会对该 Broker 上的所有 partition 进行处理（比如重新选举 leader 等）\n3. Controller # 当 Controller 宕机时会触发 Controller failover，重新开始 Controller 的选举，选举成功后触发 KafkaController##onControllerFailover 完成 epoch id 自增，zookeeper watcher 注册等。\n4. 参考文献 # kafka——原理解析 Kafka 设计解析（二）- Kafka High Availability（上） Kafka 设计解析（三）- Kafka High Availability（下） "},{"id":78,"href":"/posts/apache-kafka/ch08/","title":"Ch08-Kafka 之选举","section":"Blog","content":"Kafka 选举主要体现在两个地方，一个是 Broker 的选举，另一个是 Partition 的选举。\n1. Controller 选举 # Kafka 中 Controller 的选举的工作依赖于 Zookeeper，所有的 broker 在 Zookeeper 中抢先注册 /controller 如果注册到了，那么该 broker 便是 Controller。\nController 被选举出来，作为整个 Broker 集群的管理者，管理所有的集群信息和元数据信息。它的职责包括下面几部分：\n处理 Broker 节点的上线和下线，包括自然下线、宕机和网络不可达导致的集群变动，Controller 需要及时更新集群元数据，并将集群变化通知到所有的 Broker 集群节点； 创建 Topic 或者 Topic 扩容分区，Controller 需要负责分区副本的分配工作，并主导 Topic 分区副本的 Leader 选举。 管理集群中所有的副本和分区的状态机，监听状态机变化事件，并作出相应的处理。Kafka 分区和副本数据采用状态机的方式管理，分区和副本的变化都在状态机内会引起状态机状态的变更，从而触发相应的变化事件。 2. Partition 选举 # 由 Controller 从 Zookeeper 中读取当前分区的所有 ISR(in-sync replicas) 集合，然后选择一个 Replica 升级为 Leader。目前支持的选择算法有 NoOpLeaderSelector，OfflinePartitionLeader，ReassignedPartitionLeader，PreferredReplicaPartitionLeader，ControlledShutdownLeader。\n"},{"id":79,"href":"/posts/apache-kafka/ch07/","title":"Ch07-Kafka 之数据可靠性","section":"Blog","content":"数据可靠性值指数据不会轻易丢失，数据一定会被可靠存储。\n1. Topic 分区副本机制 # Kafka 的分区多副本机制是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。 当消息写入所有 in-sync 状态的副本后，消息才会认为已提交（committed）。这里的写入有可能只是写入到文件系统的缓存，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本写入即返回，后者等待所有 in-sync 状态副本写入才返回。一旦消息已提交，那么只要有一个副本存活，数据不会丢失。\n2. Ack 确认机制 # 通过 Ack 确认机制保证数据一定可靠的存储成功，简单的来说就是一定要得到确定答复才会进行下一次数据的处理。大体上 Ack 确认分为可以分为两种，第一种是 producer 与 broker 之间的确认，另一种是 partition 的 leader 和 follower 之间的确认。\n"},{"id":80,"href":"/posts/apache-kafka/ch06/","title":"Ch06-Kafka 之数据一致性","section":"Blog","content":"数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？\n其实主要还是借助 HW（High Water Mark）实现。\n假设分区的副本为 3，其中副本 0 是 Leader，副本 1 和副本 2 是 follower，并且在 ISR 列表里面。虽然副本 0 已经写入了 Message4，但是 Consumer 只能读取到 Message2。因为所有的 ISR 都同步了 Message2，只有 High Water Mark 以上的消息才支持 Consumer 读取，而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本 2，这个很类似于木桶原理。\n这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前 Leader（副本 0）读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本 1 为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。\n当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。\n"},{"id":81,"href":"/posts/apache-spark/ch02/","title":"Ch02-Spark 应用执行模式","section":"Blog","content":"Spark 应用执行模式的不同主要体现在 Cluster Manager 使用的是哪个。如果是单独的进程，那么就是 Standalone 模式；如果是 Hadoop Yarn，那就是 Hadoop Yarn 模式。\n1. Cluster Mananger # 类型 说明 Standalone a simple cluster manager included with Spark that makes it easy to set up a cluster. Hadoop YARN the resource manager in Hadoop 2. Apache Mesos a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated) Kubernetes an open-source system for automating deployment, scaling, and management of containerized applications. 2. Yarn 部署模式 # Cluster Mananger 选择 Yarn 的时候存在两种部署模式，一种是 client，一种是 cluster。\nbin/spark-submit --master yarn --deploy-mode client bin/spark-submit --master yarn --deploy-mode cluster client 和 Cluster 唯一的不同点是 Spark Context 运行在哪里。如果运行在 Spark Yarn Client 中，那便是 Client 模式，如下图左所示；如果运行在 Application Master 中，那便是 Cluster 模式，如下图右所示。\n3. 参考文献 # spark 四种运行模式 "},{"id":82,"href":"/posts/apache-kafka/ch05/","title":"Ch05-Kafka 之 Partition","section":"Blog","content":"Kafka Partition 相关的机制是比较复杂的，它自身保留了一个 leader 来对外提供消息操作的能力，若干 follower 通过 leader 同步消息保证数据可靠性。\n1. 基础概念 # Term Name 说明 AR Assigned Replicas 所有的副本统称为 AR，AR=ISR+OSR ISR In-Sync Replicas ISR 是 AR 的一个子集，即所有和主副本保持同步的副本集合 OSR Out-of-Sync Replicas OSR 也是 AR 的一个子集，所有和主副本未保持一致的副本集合。Kafka 通过一些算法来判定从副本是否保持同步，处于失效的副本也可以通过追上主副本来重新进入 ISR。 LEO Log End Offset LEO 是下一个消息将要写入的 offset 偏移，在 LEO 之前的消息都已经写入日志了，每一个副本都有一个自己的 LEO。 HW High Watermark 所有和主副本保持同步的副本中，最小的那个 LEO 就是 HW，这个 offset 意味着在这之前的消息都已经被所有的 ISR 写入日志了，消费者可以拉取了，这时即使主副本失效其中一个 ISR 副本成为主副本消息也不会丢失。 2. 主副本 HW 与 LEO # LEO 和 HW 都是消息的偏移量，其中 HW 是所有 ISR 中最小的那个 LEO。\n生产者将消息发送给 leader； leader 追加消息到日志中，并更新自己的偏移量信息，同时 leader 也维护着 follower 的信息（比如 LEO 等）； follower 向 leader 请求同步，同时携带自己的 LEO 等信息； leader 读取日志，拉取保存的每个 follower 的信息（LEO）； leader 将数据返回给 follower，同时还有自己的 HW； follower 拿到数据之后追加到自己的日志中，同时根据返回的 HW 更新自己的 HW，方法就是取自己的 LEO 和 HW 的最小值。 从上面这个过程可以看出，一次同步过程之后 leader 的 HW 并没有增长，只有再经历一次同步，follower 携带上一次更新的 LEO 给 leader 之后，leader 才能更新 HW，这个时候才能确认消息确实是被所有的 ISR 副本写入成功了。所以 leader 的 HW 很重要，因为这个值直接决定了消费者可以消费的数据。\n3. 故障恢复 # 3.1 Follower 故障 # follower 发生故障后会被临时踢出 ISR 集合，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步数据操作。等该 follower 的 LEO 大于等于该 partition 的 HW，即 follower 追上 leader 后，就可以重新加入 ISR 了。\n3.2 Leader 故障 # leader 发生故障后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。\n4. 参考文献 # Kafka 概述：深入理解架构 "},{"id":83,"href":"/posts/apache-spark/ch01/","title":"Ch01-Spark 介绍","section":"Blog","content":"Apache Spark 是用于大规模数据处理的统一分析引擎，基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将 Spark 部署在大量硬件之上，形成集群。\n1. 基本信息 # 条目 说明 官网 https://spark.apache.org/ 下载地址 https://spark.apache.org/downloads.html 2. 架构介绍 # Spark 支持 Standalone、Yarn、Mesos、Kubernetes 等多种部署方案，几种部署方案原理也都一样，只是不同组件角色命名不同，但是核心功能和运行流程都差不多。\nSpark 应用程序启动在自己的 JVM 进程里，即 Driver 进程，启动后调用 SparkContext 初始化执行配置和输入数据。SparkContext 启动 DAGScheduler 构造执行的 DAG 图，切分成最小的执行单位也就是计算任务。 Driver 向 Cluster Manager 请求计算资源用于 DAG 的分布式计算。Cluster Manager 收到请求以后，将 Driver 的主机地址等信息通知给集群的所有计算节点 Worker。 Worker 收到信息以后，根据 Driver 的主机地址，跟 Driver 通信并注册，然后根据自己的空闲资源向 Driver 通报自己可以领用的任务数。Driver 根据 DAG 图开始向注册的 Worker 分配任务。 Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。 3. 基本概念 # 术语 说明 Driver Program 运行 Application 的 main() 函数并创建 SparkContext Cluster Mananger 集群资源管理中心，负责分配计算资源，目前支持 Standalone，Apache Mesos，Kubernetes，Hadoop YARN 这四种。 Worker Node 工作节点，负责完成具体计算 Executor 是运行在工作节点（Worker Node）上的一个进程，负责运行 Task，并为应用程序存储数据 Task 运行在 Executor 上的工作单元，是 Executor 中的一个线程 Application 用户编写的 Spark 应用程序，一个 Application 包含多个 Job Job 一个 Job 包含多个 RDD 及作用于相应 RDD 上的各种操作 Stage 作业的基本调度单位，一个作业会分为多组任务，每组任务就是一个 Stage DAG Directed Acyclic Graph，反映 RDD 之间的依赖关系 RDD Resilient Distributed Dataset，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 "},{"id":84,"href":"/posts/apache-kafka/ch04/","title":"Ch04-Kafka 之 ZeroCopy","section":"Blog","content":"Kafka 中存在大量的网络数据持久化到磁盘（Producer 到 Broker）和磁盘文件通过网络发送（Broker 到 Consumer）的过程。这一过程的性能直接影响 Kafka 的整体吞吐量，于是 Kafka 便引入了 ZeroCopy 技术来提升性能。\njava.nio.channels.FileChannel 的 transferTo() 方法允许字节从源通道传输到接收通道，而不需要将应用程序作为传输中介。这便是 ZeroCopy 的核心代码。\n1. 传统方式磁盘到网络处理过程 # 传统的方式是将源通道读入字节缓冲区，然后作为两个独立的操作写入接收器通道：\nFile.read(fileDesc, buf, len); Socket.send(socket, buf, len); 要完成将数据从磁盘读取出来，然后通过网络发送出去，传统方式为了完成此步骤，会在用户态和内核态之间进行四次上下文切换，并且在操作完成之前会复制四次数据。\n如上作图所示，复制分为 4 次。\n首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝）。 然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝）。 接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝）。 最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。 2. ZeroCopy 方式磁盘到网络处理过程 # Linux 2.4+内核通过 sendfile 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。除了减少数据拷贝外，因为整个读文件 - 网络发送由一个 sendfile 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。\n@Overridepublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { return fileChannel.transferTo(position, count, socketChannel); } transferTo 和 transferFrom 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 sendfile 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。\n"},{"id":85,"href":"/posts/apache-kafka/ch03/","title":"Ch03-Kafka 之消息存储","section":"Blog","content":"Kafka 生产者和消费者工作流程比较复杂，需要各个组件参与才能完成。\n1. 日志存储 # Kafka 中的消息在每个 Partition 中是以分段的形式进行存储的，即每 1GB 消息新建一个 Segment，每个 Segment 包含两个文件：.log 文件和.index 文件。.log 文件就是 Kafka 实际存储 Producer 生产的消息，而 .index 文件采用稀疏索引的方式存储 .log 文件中对应消息的逻辑编号（offset）和物理偏移地址（position），以便于加快数据的查询速度。.log 文件和.index 文件是一一对应，成对出现的。下图展示了.log 文件和.index 文件在 Partition 中的存在方式。\n2. 日志索引 # 日志追加提高了写的性能，但是对于读就不是很友好了。为了提高读的性能，就需要降低一点写的性能，在读写之间做一点平衡。也就是在写的时候维护一个索引。kafka 维护了两种索引：偏移量索引和时间戳索引。\n2.1 偏移量索引 # 为了能够快速定位给定消息在日志文件中的位置，一个简单的办法就是维护一个映射，key 就是消息的偏移量，value 就是在日志文件中的偏移量，这样只需要一次文件读取就可以找到对应的消息了。 不过当消息量巨大的时候这个映射也会变很大，kafka 维护的是一个稀疏索引（sparse index），即不是所有的消息都有一个对应的位置，对于没有位置映射的消息来说，一个二分查找就可以解决了。\n比如要找 offset 是 37 的消息所在的位置，先看索引中没有对应的记录，先找到不大于 37 的最大 offset 是 31，然后在日志中从 1050 开始按序查找 37 的消息。\n2.2 时间戳索引 # 时间戳索引就是可以根据时间戳找到对应的偏移量。时间戳索引是一个二级索引，现根据时间戳找到偏移量，然后就可以使用偏移量索引找到对应的消息位置了。\n3. 日志压缩 # 日志压缩针对的是 key，具有相同 key 的多个 value 值只保留最近的一个。同时，日志压缩会产生小文件，为了避免小文件过多，kafka 在清理的时候还会对其进行合并。\n4. 日志删除 # 日志删除策略有过期时间和日志大小。默认保留时间是 7 天，默认大小是 1GB。 虽然默认保留时间是 7 天，但是也有可能保留时间更长。因为当前活跃的日志分段是不会删除的，如果数据量很少，当前活跃日志分段一直没能继续拆分，那么就不会删除。Kafka 会有一个任务周期性地执行，对满足删除条件的日志进行删除。\n"},{"id":86,"href":"/posts/apache-kafka/ch02/","title":"Ch02-Kafka 生产消费流程","section":"Blog","content":"Kafka 生产者和消费者工作流程比较复杂，需要各个组件参与才能完成。\n1. 生产者 # 1.1 生产流程 # 在生产端主要有两个线程：main 和 sender，两者通过共享内存 RecordAccumulator 通信。\nKafkaProducer 创建消息； 生产者拦截器在消息发送之前做一些准备工作，比如过滤不符合要求的消息、修改消息的内容等； 序列化器将消息转换成字节数组的形式； 分区器计算该消息的目标分区，然后数据会存储在 RecordAccumulator 中； 发送线程获取数据进行发送； 创建具体的请求； 如果请求过多，会将部分请求缓存起来； 将准备好的请求进行发送； 发送到 kafka 集群； 接收响应； 清理数据。 1.2 消息发送模式 # 这种模式并不是一种配置参数，而是一种代码处理方式。\n发送模式 说明 代码示例 发后即忘 (fire and forget) 只管发送不管结果性能最高，可靠性也最差 producer.send(record) 同步 (sync) 等集群确认消息写入成功再返回可靠性最高，性能差很多 Future f = producer.send(record);T meta = f.get(); 异步 (async) 指定一个 callback，kafka 返回响应后调用来实现异步发送的确认 producer.send(record, new Callback() {} 与之有一个容易混淆的参数 producer.type，它表示消息发送到 broker 之后，要不要调用 flush 直接刷入到磁盘，如果设置为 sync，则表示 broker 接收到消息后直接刷盘。而 async 则表示接收到消息后不刷盘，过段时间后异步刷入。\n1.3 Acks # acks 说明 0 把消息发送出去，那么就认为消息已经写入成功。 1 Leader 成功写入即表示写入成功。 -1 Leader + Replica 全部成功写入即表示写入成功。 2. 消费者 # 2.1 消费流程 # kafka consumer 采用 pull（拉）模式从 broker 中读取数据。kafka 中的消息都有一个 offset 唯一标识，对于消费者来说，每消费完一个消息需要通知 kafka，这样下次拉取消息的时候才不会拉到已消费的数据（不考虑重复消费的情况）。这个消费者已消费的消息位置就是消费位移，这个 offset 被保存在名为 __consumer_offsets 的 topic 中。\n与消费组相关有两个组件，一个是消费者客户端的 ConsumerCoordinator（线程），一个是 Kafka Broker 服务端的 GroupCoordinator（线程），ConsumerCoordinator 负责与 GroupCoordinator 通信确定 consumer leader，进而确定消费方案，然后由 GroupCoordinator 将消费方案下发到每个 consumer，各个 consumer 按照对应的消费方案进行消费。\n每个 ConsumerCoordinator 通过对 groupid 取 hash 计算得到指定的 GroupCoordinator； 然后 GroupCoordinator 从一个 consumer group 中取出一个 consumer 作为 leader； GroupCoordinator 把 consumer group 情况发送给这个 consumer leader； 接着 consumer leader 会负责制定消费方案； 通过 SyncGroup 发送给 GroupCoordinator； 接着 GroupCoordinator 就把消费方案下发给所有的 consumer，他们会从指定的分区的 leader broker 开始进行 socket 连接和进行消息的消费； 2.2 分区分配策略 # 一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定哪个 partition 由哪个 consumer 来消费。\n分区分配策略 说明 Range 按照消费者的总数和分区总数进行整除运算来分配，不过是按照主题粒度的，所以可能会不均匀 RoundRobin 将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式这个将分区一次分配给每个消费者。 Sticky 这个策略比较复杂，目的是分区的分配尽可能均匀，以及分配要尽可能和上次保持一致。 2.3 分区分配再均衡 # 以下三种现象会使 partition 的所有权在 consumer 之间转移，这样的行为叫作再均衡。\n消费者组中新添加消费者读取到原本是其他消费者读取的消息。 消费者关闭或崩溃之后离开群组，原本由他读取的 partition 将由群组里其他消费者读取。 当向一个 Topic 添加新的 partition，会发生 partition 在消费者中的重新分配。 优缺点 说明 优点 给消费者组带来了高可用性和伸缩性 缺点 再均衡期间消费者无法读取消息，整个群组有一小段时间不可用 partition 被重新分配给一个消费者时，消费者当前的读取状态会丢失，有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。因此需要进行安全的再均衡和避免不必要的再均衡 3 参考文献 # kafka——消费者原理解析\n"},{"id":87,"href":"/posts/apache-kafka/ch01/","title":"Ch01-Kafka 介绍","section":"Blog","content":"Kafka 是一款开源的、轻量级的、分布式、可分区和具有复制备份的 (Replicated)、基于 ZooKeeper 协调管理的分布式流平台的功能强大的消息系统。与传统的消息系统相比，Kafka 能够很好地处理活跃的流数据，使得数据在各个子系统中高性能、低延迟地不停流转。\n1. 基本信息 # 条目 说明 官网 https://kafka.apache.org/ 下载地址 https://kafka.apache.org/downloads 2. 术语介绍 # 术语 说明 Broker 一台 Kafka 机器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。 Producer 消息生产者，即向 Kafka Broker 发消息的客户端 Consumer 消息消费者，从 Kafka Broker 取消息的客户端 Consumer Group 消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内 消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 Topic Partition 一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。 Replica 一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 Leader 每个 partition 多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader（负责读写） Follower 每个 partition 多个副本中的“从”，实时从 Leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 leader（只负责备份）。 Offset 每一条数据都有一个 offset，是数据在该 partition 中的唯一标识（其实就是消息的索引号），各个 consumer 会保存其消费到的 offset 位置，这样下次可以从该 offset 位置开始继续消费 3. 架构介绍 # 3.1 整体架构 # Kafka 主要由 Broker，Producer，Consumer 组成，用户可以使用 Producer 和 Consumer 插入和查询数据。Broker 是 Kafka 的核心服务部分，它将所有的数据组织起来，以 Topic 这个概念对外提供服务。\nKafka 2.8 引入了 KRaft 机制来摆脱对 ZooKeeper 的依赖，不过截止 2022 年 4 月份，仍然没有达到生产环境使用标准。\n3.2 Topic 架构 # ​topic 和 partition 是逻辑上的概念，partition 对应的 replica 则是物理概念，一个 replica 对应一个 log 文件，producer 每次生产的消息都会追加到该文件的末尾。 ​由于每次生产者生产消息都会追加到 log 文件中，为了防止日志文件过大影响消息的查找定位效率，kafka 引入了分片和索引的机制，将一个 replica 又分为若干个 segment，每个 segment 包含一个存储数据的 log 文件和查找的 index 索引文件，这些文件位于一个文件夹下，文件夹的命名规则为 topic 名称 + 分区序号。\n在 Kafka 中限定 replica 的数量不能大于 broker。\n3.3 ZK 节点组织 # 4. 参考文献 # Kafka 概述：深入理解架构 图说 Kafka 基本概念 "},{"id":88,"href":"/posts/apache-hive/ch03/","title":"Ch03-Hive 之底层数据存储","section":"Blog","content":"Hive 底层数据存储格式\n1. 存储格式 # 分类 格式 说明 行式存储 textfile sequencefile 列式存储 rcfile orc parquet 2. 参考文献 # "},{"id":89,"href":"/posts/apache-hive/ch02/","title":"Ch02-Hive 之 SQL 执行","section":"Blog","content":"hive sql 执行流程\n参考文献 # 基于 calcite 做傻瓜式的 sql 优化 (一) 基于 calcite 做傻瓜式的 sql 优化 (二) 基于 calcite 做傻瓜式的 sql 优化 (三) Hive SQL 语句的正确执行顺序 Hive 优化器原理与源码解析系列—统计信息带谓词选择率 Selectivity Hive 优化器原理与源码解析系列—CBO 成本模型 CostModel(一) Hive 优化器原理与源码解析系列—CBO 成本模型 CostModel(二) "},{"id":90,"href":"/posts/apache-hive/ch01/","title":"Ch01-Hive 介绍","section":"Blog","content":"Apache Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，其基本原理是将 SQL 语句转换为 MapReduce 任务进行数据处理功能。所以从代码层面来看，整个 Hive 就是将 SQL 语句转换成 MapReduce 代码的一款软件。\n1. 基本信息 # 条目 说明 官网 https://hive.apache.org/ 下载地址 https://hive.apache.org/downloads.html 2. 架构介绍 # Hive 由 Hive Server 和 Hive Client 组成。Hive Server 对外提供了 Thrift，JDBC 接口，用户可以自行进行选择使用。Hive Client 比较简单，命令行工具 beeline 是对 thrift 的一层再包装，Client 工具又是对 jdbc 接口的再包装。\n2.1 Hive Client # 组件 说明 JDBC/ODBC 以 jar 包的形式对外用户使用 Thrift 2.2 Hive Server # 组件 说明 Driver 实现了 session handler，在 JDBC/ODBC 接口上实现了执行和获取信息的 API。 Compiler 对查询语句做词法解析，语义分析，查询 MetaStore 元数据生成执行计划，借助 RBO/CBO 优化执行计划。 Execution Egine 提交执行计划到 Yarn、Mesos、Tez 等 2.3 Hive MetaStore # 组件 说明 Database 存储了表中列和列类型等结构化的信息以及数据仓库中的分区信息，支持使用 Derby，MySQL 等替代 Services 用于与 Hive Server 交互 3. 流程介绍 # 这里以官网文档上的例子来说明下 Hive 各个组件是如何配合完成 DML 操作的。\n这张图看起来是比较老旧的，右半边图中的 JOB TRACKER 属于 Hadoop 1.0 时代的组件，自 2.0 开始以 Resource Mananger 替代；同理 TASK TRACKERS 现在已经以 Node Manager 替代。\n用户提交查询等任务给 Driver。 Driver 为查询操作创建一个 session handler，接着 dirver 会发送查询操作到 compiler 去生成一个 execute plan Compiler 根据用户任务去 MetaStore 中获取需要的 Hive 的元数据信息。这些元数据在后续 stage 中用作抽象语法树的类型检测和修剪。 Compiler 得到元数据信息，对 task 进行编译，先将 HiveQL 转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询 plan，重写逻辑查询 plan，将逻辑 plan 转化为物理的 plan（MapReduce）, 最后选择最佳策略（借助 Apache Calcite 完成）。 将最终的 plan 提交给 Driver。 Driver 将 plan 转交给 Execution Engine 去执行，将获取到的元数据信息，提交到 Resource Manager 执行。 取得并返回执行结果。 "},{"id":91,"href":"/posts/apache-zookeeper/ch02/","title":"Ch02-Zookeeper 之 ZAB 协议","section":"Blog","content":"Zab 协议的全称是 Zookeeper Atomic Broadcast（Zookeeper 原子广播），Zab 是特别为 Zookeeper 设计的支持崩溃恢的原子广播协议，在 Zookeeper 中主要依赖 Zab 协议实现数据一致性，基于该协议，Zookeeper 实现了主备模型（Leader 与 Follower）的系统架构保证集群中各个副本之间的数据一致性。\n1. 工作原理 # 在 Zookeeper 中只有一个 Leader，并且只有 Leader 可以处理外部客户端的事务请求，并将其转换成一个事务 Proposal（写操作），然后 Leader 服务器再将事务 Proposal 操作的数据同步到所有 Follower（数据广播/数据复制）。ZAB 协议规定，只要超过半数 Follower 节点反馈 OK，Leader 节点就会向所有的 Follower 服务器发送 Commit 消息。即将 Leader 节点上的数据同步到 Follower 节点之上。\n2. 工作模式 # Zookeeper 正常情况下，会工作在消息广播模式，但是如果 Leader 宕机或者过半数 Follower 失联，那么会工作在崩溃恢复模式。\n2.1 消息广播模式 # Leader 服务器将客户端的 request 请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局唯一的 ID，即 ZXID； Leader 服务器与每个 Follower 之间都有一个队列，Leader 将消息发送到该队列； Follower 机器从队列中取出消息处理 (写入本地事务日志中) 完成后，向 Leader 服务器发送 ACK 确认； Leader 服务器收到半数以上的 Follower 的 ACK 后，即认为可以发送 commit； Leader 向所有的 Follower 服务器发送 commit 消息。 2.2 崩溃恢模式 # 2.2.1 需要遵循的原则 # 该模式做数据恢的时候主要遵循两个原则，已经处理过的消息不能丢，被丢弃的消息不能再现。\n对于第一个原则，Leader 收到超过半数 Follower 的 ACKs 后就向各个 Follower 广播 Commit 消息，如果在此期间 Leader 宕机了，那么后续新选举出来的 Leader 在数据恢复完成后要保证所有的 Server 都成功执行前面部分 Follower 完成的事务。 对于第二个原则，Leader 已经生成 Proposal 提案但是还没有发给 Follower 的时候，Leader 宕机了。新的 Leader 选取成功，旧的 Leader 注册称为 Follower 之后必须放弃先前的 Proposal 提案。 2.2.2 执行过程 # 崩溃恢复模式一般会经过两个执行过程，Leader 选举和初始化同步\nLeader 选举\n节点刚启动时，默认为 Following 状态。 节点启动后，状态切换为 Looking 状态，先投票给自己，然后将投票消息发送给其他节点。投票消息内容为 Vote(epoch, zxID, serverID)。 节点收到其他节点的投票消息后，比较 ZXID 和 ServerID 选出主节点，将选出主节点的投票消息广播给其他节点。 选出的主节点计算得票数，如果超过集群中节点半数，则切换为 Leading 状态，并向其他节点发送心跳消息。 其他节点切换为 Following 状态 对于节点已经启动后，Leader 宕机后的 Leader 选举过程跟上述过程类似，唯一不同的是第 1 步中所有的节点不是 Following 状态，会先将自己的状态改为 Looking。剩余过程依旧一致。\n初始化同步\n当完成 Leader 选 举后，此时的 Leader 还是一个准 Leader，其要经过初始化同步后才能变为真正的 Leader。\n为了保证 Leader 向 Learner 发送提案的有序，Leader 会为每一个 Learner 服务器准备一 个队列； Leader 将那些没有被各个 Learner 同步的事务封装为 Proposal； Leader 将这些 Proposal 逐条发给各个 Learner，并在每一个 Proposal 后都紧跟一个 COMMIT 消息，表示该事务已经被提交，Learner 可以直接接收并执行 Learner 接收来自于 Leader 的 Proposal，并将其更新到本地； 当 Learner 更新成功后，会向准 Leader 发送 ACK 信息； Leader 服务器在收到来自 Learner 的 ACK 后就会将该 Learner 加入到真正可用的 Follower 列表或 Observer 列表。没有反馈 ACK，或反馈了但 Leader 没有收到的 Learner，Leader 不会将其加入到相应列表。 参考文献 # 分布式选举-ZAB 算法 -1 Leader 选举 原理\n其他问题 # 0x01 ZAB 协议与 2PC 有何不同？\nZAB 协议简化了 2PC 事务提交：\n去除中断逻辑移除，Follower 要么 Ack，要么抛弃 Leader； Leader 不需要所有的 Follower 都响应成功，只要一个多数派 ACK 即可。 0x02 ZXID 长什么样？\nZAB 中每个事务对应一个 ZXID，它由两部分组成：\u0026lt;e, c\u0026gt;，e 即 Leader 选举时生成的 epoch，c 表示当次 epoch 内事务的编号、依次递增。 假设有两个事务的 ZXID 分别是 z、z’，当满足 z.e \u0026lt; z’.e 或者 z.e = z’.e \u0026amp;\u0026amp; z.c \u0026lt; z’.c 时，定义 z 先于 z’发生 (z \u0026lt; z’)。\n0x03 Learner 是什么？\nLearner 跟 Leader 和 Follower 不同，它并不是 ZooKeeper 的一种实际角色，属于一种逻辑概念，一般指 Follower 和 Obverser。\n"},{"id":92,"href":"/posts/apache-zookeeper/ch01/","title":"Ch01-Zookeeper 介绍","section":"Blog","content":"Apache ZooKeeper 是由 Apache Hadoop 的子项目发展而来，于 2010 年 11 月正式成为了 Apache 的顶级项目。ZooKeeper 是一个正式源代码的分布式协调服务，由知名互联网公司雅虎创建，是 Google Chubby 的开源实现。\nZooKpeer 是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。\n1. 基本信息 # 条目 说明 官网 https://zookeeper.apache.org/ 下载地址 https://zookeeper.apache.org/releases.html 2. 架构介绍 # 2.1 相关角色 # ZooKeeper 集群中包含 Leader、Follower 以及 Observer 三个角色，每个节点只能是其中的某一个角色。\n角色 说明 Leader 负责进行投票的发起和决议，更新系统状态，Leader 是由选举产生。 Follower 用于接受客户端请求并向客户端返回结果，在选主过程中参与投票。 Observer 可以接受客户端连接，接受读写请求，写请求转发给 Leader。但 Observer 不参加投票过程，只同步 Leader 的状态，Observer 的目的是为了扩展系统，提高读取速度。 2.2 服务状态 # ZooKeeper 的每个节点在某个时间段会处于某个指定状态，目前常见的有下述 4 种状态。\n状态 说明 Looking 寻找 Leader 状态。当服务器处于该状态时，它会认为集群中没有 Leader，因此需要进入 Leader 选举流程 Following 跟随者状态，表明当前服务器角色是 Follower。Follower 与 Leader 处于数据同步阶段 Leading 领导者状态，表明当前服务器角色是 Leader。前集群中有一个 Leader 为主进程 Observing 观察者状态，表明当前服务器角色是 Observer。 2.3 数据模型 # Zookeeper 通过一个共享的、树型结构的名字空间来进行相互协调。这个命名空间由若干个 ZNode 组成。\nZNode 可以分为持久节点(PERSISTENT)和临时节点(EPHEMERAL)。\n持久节点是指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则 ZNode 将一直保存在 ZooKeeper 上。 临时节点的生命周期会和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的临时节点就会被移除。 如果给持久节点和临时节点加上顺序特性，又可以形成持久顺序节点和临时顺序节点。 3. 机制介绍 # 3.1 事务 # 在 ZooKeeper 中，事务是指能够改变 ZooKeeper 服务器状态的操作，我们也称为事务操作和更新操作，一般包括数据节点创建与删除、数据节点内容更新和客户端会话失效操作等。 对于每一个事务请求，ZooKeeper 都会为其分配一个全局唯一的事务 ID，用 ZXID 表示，每一个 ZXID 对应一次更新操作，从这些 ZXID 中可以间接地识别出 ZooKeeper 处理这些更新操作请求的全局顺序。\n3.2 状态信息 # 每个数据节点除了存储了数据内容之外，还存储了数据节点本身的一些状态信息。使用 get 命令便可以查看。\n状态属性 说明 czxid 即 Created ZXID，表示该数据节点被创建时的事务 ID mzxid 即 Modified ZXID，表示该节点最后一次被更新时的事务 ID ctime 即 Created Time，表示节点被创建的时候 mtime 即 Modified Time，表示该节点最后一次被更新的时间 version 数据节点的版本号 cversion 子节点的版本号 aversion 节点的 ACL 版本号 ephemeralOwner 创建该临时节点的会话的 sessionID。如果该节点是持久节点，那么这个属性值为 0。 dataLength 数据内容的长度 numChildren 当前节点的子节点个数 pzxid 表示该节点的子节点列表最后一次被修改时的事务 ID。注意，只有子节点列表变更了才会变更 pzxid，子节点内容变更不会影响 pzxid 3.3 版本机制 # 每个 ZNode 节点都有三种类型的版本信息，对数据的任何操作都会引起版本的变化。不过这里要注意，虽然称为版本，但是叫做次数可能更加符合实际用途，它表示的是 数据内容，子节点，ACL 信息的修改次数。\n版本类型 说明 version 当前数据节点数据内容的版本号 cversion 当前数据节点子节点的版本号 aversion 当前数据节点 ACL 变更版本号 3.4 Watcher 机制 # ZooKeeper 提供了分布式数据的发布/订阅功能，Watcher 机制则完成了分布式通知功能。它允许客户端向服务器端注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知，这样就实现了分布式的通知功能。\n4. 参考文献 # Zookeeper 集群如何高可用部署？\n"},{"id":93,"href":"/posts/mysql/ch07/","title":"Ch07-MySQL 之 事务","section":"Blog","content":"数据库事务 (Database Transaction)，是指作为单个逻辑工作单元执行的一系列操作，要么完全执行，要么完全地不执行。要么完全地不执行。一般来说，事务是必须满足 4 个条件 (ACID)：原子性 (Atomicity)、一致性 (Consistency)、隔离性 (Isolation)、持久性 (Durability)。\n1. 原子性 (Atomicity) # MySQL 实现回滚操作完全依赖于 undo log。使用 undo 实现原子性在操作任何数据之前，首先会将修改前的数据记录到 undo log 中，再进行实际修改。如果出现异常需要回滚，系统可以利用 undo 中的备份将数据恢复到事务开始之前的状态。\n上图是 MySQL 中表示事务的基本数据结构，其中与 undo 相关的字段为 insert_undo 和 update_undo，分别指向本次事务所产生的 undo log。\n事务回滚根据 update_undo（或者 insert_undo）找到对应的 undo log，做逆向操作即可。对于已经标记删除的数据清理删除标记，对于更新数据直接回滚更新；插入操作稍微复杂一些，不仅需要删除数据，还需要删除相关的聚集索引以及二级索引记录。\n2. 一致性 (Consistency) # 3. 隔离性 (Isolation) # 3.1 说明 # 隔离级别 说明 Read uncommitted 一个会话可以读取其他事务未提交的更新结果，如果这个事务最后以回滚结束，这时的读取结果就可能是错误的，所以多数的数据库应用都不会使用这种隔离级别。 Read commited 这种隔离级别的事务只能读取其他事务已经提交的更新结果，否则，发生等待，但是其他会话可以修改这个事务中被读取的记录，而不必等待事务结束，显然，在这种隔离级别下，一个事务中的两个相同的读取操作，其结果可能不同。 Repeatable read 在一个事务中，如果在两次相同条件的读取操作之间没有添加记录的操作，也没有其他更新操作导致在这个查询条件下记录数增多，则两次读取结果相同。（该级别是 InnoDB 默认的隔离等级）。 Serializable 在一个事务中，读取操作的结果是在这个事务开始之前其他事务就已经提交的记录。 InnoDB 默认的隔离级别是 REPEATABLE READ（可重复读），并且通过间隙锁算法 (next-key locking) 策略防止 Phantom Read 的出现\n问题 说明 Dirty Read 事务 A 读取了事务 B 未提交的数据，然而事务 B 最后却进行了回滚。 NonRepeatable Read 事务 A 读取了事务 B 已提交的数据，然后事务 B 又进行了修改操作。 Phantom Read 事务 A 读取了事务 B 已提交的数据，然后事务 B 又进行了插入操作。 3.2 问题 # 不同的隔离级别面临的问题。\nIsolation/Question Dirty Read NonRepeatable Read Phantom Read Read uncommitted Y Y Y Read commited N Y Y Repeatable read N N Y Serializable N N N 4. 持久性 (Durability) # MySQL 事务持久化涉及的组件相对比较多，主要有 doublewrite、redo log 以及 binlog。\n4.1 MySQL 数据持久化（DoubleWrite） # 实际上 MySQL 的真实数据写入分为两次写入，一次写入到一个称为 DoubleWrite 的地方，写成功之后再真实写入数据所在磁盘。为什么要写两次？这是因为 MySQL 数据页大小与磁盘一次原子操作大小不一致，有可能会出现部分写入的情况，比如默认 InnoDB 数据页大小为 16K，而磁盘一次原子写入大小为 512 字节（扇区大小），这样一个数据页写入需要多次 IO，这样一旦中间发生异常就会出现数据丢失。另外需要注意的是 DoubleWrite 性能并不会影响太大，因为写入 DoubleWrite 是顺序写入，对性能影响来说不是很大。\n4.2 redolog 持久化策略（innodb_flush_log_at_trx_commit） # redolog 是 InnoDB 的 WAL，数据先写入 redolog 并落盘，再写入更新到 bufferpool。redolog 的持久化策略默认为 1，表示每次事务提交之后 log 就会持久化到磁盘；该值为 0 表示每隔 1 秒钟左右由异步线程持久化到磁盘，这种情况下 MySQL 发生宕机有可能会丢失部分数据。该值为 2 表示每次事务提交之后 log 会 flush 到操作系统缓冲区，再由操作系统异步 flush 到磁盘，这种情况下 MySQL 发生宕机不会丢失数据，但机器宕机有可能会丢失部分数据。\n4.3 binlog 持久化策略（sync_binlog） # binlog 作为 Server 层的日志系统，主要以 events 的形式顺序纪录了数据库的各种操作，同时可以纪录每次操作所花费的时间。在 MySQL 官方文档上，主要介绍了 Binlog 的两个最基本核心作用：备份和复制，因此 binlog 的持久化会一定程度影响数据备份和复制的完整性。和 redo 持久化策略相同，可取值有 0，1，N。默认为 0，表示写入操作系统缓冲区，异步 flush 到磁盘。该值为 1 表示同步写入磁盘。为 N 则表示每写 N 次操作系统缓冲就执行一次刷新操作。\n5. 参考文献 # 数据库事务系列－MySQL 跨行事务模型 "},{"id":94,"href":"/posts/mysql/ch05/","title":"Ch05-MySQL 之 索引","section":"Blog","content":"InnoDB 索引选择B+树作为其内存数据结构，选择聚簇索引作为数据存储方式。\n1. 索引数据组织形式 # 常见的索引数据组织形式有聚簇索引和非聚簇索引。\nInnoDB 使用的是聚簇索引（clustered index），将主键组织到一棵 B+树中，而行数据就储存在叶子节点上，若使用”where id = 14”这样的条件查找主键，则按照 B+树的检索算法即可查找到对应的叶节点，之后获得行数据。若对 Name 列进行条件搜索，则需要两个步骤：第一步在辅助索引 B+树中检索 Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引 B+树种再执行一次 B+树检索操作，最终到达叶子节点即可获取整行数据。\nMyISM 使用的是非聚簇索引（secondary index），非聚簇索引的两棵 B+树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引 B+树的节点存储了主键，辅助键索引 B+树存储了辅助键。表数据存储在独立的地方，这两颗 B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树。\n下图左是聚簇索引的示例，右是非聚簇索引的示例。\n2. 索引数据结构 # MySQL 中一个 B+ 树节点是以 Segment 为单位加载进来的，而非 Page。叶子节点根据其功能从逻辑上可以划分为主索引树非叶节点，主索引树叶子节点，辅助索引树非叶节点，辅助索引树叶子节点。它们的关系如下图所示。\n3. 索引分类 # 3.1 按照字段特性 # 条目 说明 主键索引 建立在主键上的索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建 唯一索引 建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会发生重复冲突 普通索引 建立在普通字段上的索引 前缀索引 前缀索引是指对字符类型字段的前几个字符或对二进制类型字段的前几个 bytes 建立的索引，而不是在整个字段上建索引 3.2 按索引字段个数 # 条目 说明 单列索引 建立在单个列上的索引 联合索引 建立在多个列上的索引 "},{"id":95,"href":"/posts/apache-hadoop/ch05/","title":"Ch05-Hadoop 之 MapReduce","section":"Blog","content":"Mapreduce 是一个分布式运算程序的编程框架，是用户开发“基于 hadoop 的数据分析应用”的核心框架；Mapreduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上\n1. MapReduce 架构 # MapReduce 分为两个阶段：Map 映射、Reduce 归约。 Map 阶段：对数据进行处理。 Reduce 阶段：对 Map 结果进行汇总。 Shuffle 过程：map 方法之后，reducer 方法之前是 Map 阶段和 Reduce 阶段的中间过程 不同阶段对应的 Task： MapTask：负责 Map 阶段的整个数据处理流程。 ReduceTask：负责 Reduce 阶段的整个数据处理流程 AppMaster：负责整个 MR 的过程调度及状态协调 2. MapReduce 处理流程 # map() reduce() Input ---\u0026gt; Mapper ---\u0026gt; Reducer ---\u0026gt; Output | | | InputFormat Shuffle OutpuFormat | | FileInputFormat FileOutputFormat | | TextInputFormat TextOutputFormat | RecordReader | RecordReader | | | | new LineRecordReader new LineRecordReader 3. 参考文献 # Hadoop 之 MapReduce\n"},{"id":96,"href":"/posts/apache-hadoop/ch04/","title":"Ch04-Hadoop 之 Yarn","section":"Blog","content":"HDFS 读写过程需要 NameNode，DataNode，Client 等组件共同参与才能完成，所以 HDFS 的读写流程还是比较复杂的。\n1. Yarn 基本架构 # 组件 功能 Container Yarn 资源（cpu，mem）的一种抽象，可以简单看做一台单独的“机器”。 Resource Manager 简称 RM，负责整个 yarn 的资源调度与分配；处理客户端的任务请求；启动或者监控 AppMaster。 Node Manager 简称 NM，负责管理单个节点的资源分配与执行；处理来自 Resource Mananger 的命令；处理来自 AppMaster 的命令。 Application Master 简称 AM，或 AppMaster，负责 MapReduce 任务相关资源的申请和内部任务分配，监控。 如果某个节点的 NodeManager/Container 损坏，AppMaster 也会重新向 ResourceManager 申请资源，ResourceManager 分配一个除当前损坏节点外的空闲节点。 若 AppMaster 所在的节点损坏，会导致整个 MapReduce 任务失败。 1.1 Resource Mananger # Resource Mananger 是 Master 上一个独立运行的进程，负责集群统一的资源管理、调度、分配等，是一个全局的资源管理器。集群只有一个，它由应用程序管理器（Applications Manager，简称 ASM）和资源调度器（Resource Scheduler）组成。\n易混点：\nApplication Manager 缩写是 ASM，它是 Resource Mananger 里面的一个小模块 Application Master 缩写是 AM 或者 AppMaster，它是由 Application Manager 管理的一个小模块 资源调度器（Resource Scheduler），根据各个应用程序的资源需求进行资源分配，以 Container 集成的方式返回给 AM，其中调度器 Scheduler 是一个“纯调度器”，它不从事任何与具体应用程序相关的工作，只做资源的调度。它是一个可插拔的组件，有三种常见的调度器可供使用，分别是 FIFO Scheduler，Capacity Scheduler 和 Fair Scheduler，可以通过 yarn-site.xml 配置。 应用程序管理器（Applications Manager），主要负责管理整个系统中所有 AM，RM 接收 job 的提交，其实是先提交给了 RM 的 ASM。它为应用分配第一个 Container 来启动运行 AM，这个 AM 是一个 pre-application ApplicationMaster，需要提前执行包括应用程序提交、与 调度器 Scheduler 协商资源以启动 AM、监控 AM 运行状态并在失败时重新启动它、监控 task 执行的进度等工作。 1.2 NodeMananger # NM 是 Slave 上一个独立运行的进程，通过心跳机制上报节点的状态（磁盘，内存，CPU 等使用信息）给 RM，它是真正提供计算资源的，除了节点的状态还会上报节点上 Container 的使用信息。\n1.3 AppMaster # AM 是 Application 的监护人和管理者，负责监控，管理这个 Application 的所有 Attempt(task) 在集群中各个节点上的运行情况，同时负责向 RM 申请资源运行 task，task 执行完成后返还资源。AM 只有在集群运行 job 时，有任务在执行时，才会出现这个进程。\n1.4 Container # Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了 Linux 中轻量级资源隔离机制 Cgroups 进行资源隔离。Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的，默认是 1G 内存 +1 核 CPU。 当 AM 向 RM 申请资源时，AM 向 RM 询问 AM 有哪些 Container 可以用，向 NM 请求分配具体的 Container。\n2. Yarn 提交流程 # 用户向 YARN 中提交应用程序，其中包括 AppMaster 程序、启动 AppMaster 的命令、用户程序等。 ResourceManager 为该应用程序分配第一个 Container，并与对应的 NodeManager 通信，要求它在这个 Container 中启动应用程序的 AppMaster。 AppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManage 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤 4~7。 AppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 一旦 AppMaster 申请到资源后，便与对应的 NodeManager 通信，要求它启动任务。 NodeManager 为任务设置好运行环境 (包括环境变量、JAR 包、二进制程序等) 后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个 RPC 协议向 AppMaster 汇报自己的状态和进度，以让 AppMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过 RPC 向 AppMaster 查询应用程序的当前运行状态。 应用程序运行完成后，AppMaster 向 ResourceManager 注销并关闭自己。 3. 参考文献 # Yarn 基础组件及提交流程 "},{"id":97,"href":"/posts/apache-hadoop/ch03/","title":"Ch03-Hadoop 之 HDFS 读写流程","section":"Blog","content":"HDFS 读写过程需要 NameNode，DataNode，Client 等组件共同参与才能完成，所以 HDFS 的读写流程还是比较复杂的。\n1. 基本概念 # 在 HDFS 中，文件存储是按照数据块（Block）为单位进行存储的，在读写数据时，DFSOutputStream 使用 Packet 类来封装一个数据包。每个 Packet 包含了若干个 chunk 和对应的 checksum。\nBlock: HDFS 上的文件都是分块存储的，即把一个文件物理划分为一个 Block 块存储。Hadoop 2.X/3.X 默认块大小为 128 M，1.X 为 64M。 Packet: 是 Client 端向 DataNode 或 DataNode 的 Pipline 之间传输数据的基本单位，默认 64 KB Chunk: Chunk 是最小的单位，它是 Client 向 DataNode 或 DataNode PipLine 之间进行数据校验的基本单位，默认 512 Byte，因为用作校验，所以每个 Chunk 需要带有 4 Byte 的校验位，实际上每个 Chunk 写入 Packtet 的大小为 516 Byte。 2. 写入流程 # 客户端通过分布式文件系统 DistributeFileSystem 向 NameNode 发送上传文件请求，NameNode 收到请求后检查目标文件是否存，父目录是否存在，之后响应客户端可以上传文件。 客户端收到响应后向 NameNode 请求上传第一个块信息，并请求返回 DataNode，NameNode 收到请求后根据文件的备份数量返回相应数量的 DataNode 节点。 客户端收到来自 NameNode 返回的节点信息，通过 FSDataOutPutStream 向离客户端最近的节点上传数据，第一个节点收到请求后会继续调用下一个的 DataNode 节点，然后下一个节点去调用自己的下一个节点，直到最后一个节点，将通信管道 PipeLine 建立，然后各个节点从后向前逐级应答客户端。 通信管道 PipeLine 建立完成后，客户端先将磁盘中的块信息读到内存，然后通过 FSDataOutPutStream 开始以 Packet 为单位传送块数据，其中 Packet 为 64K 由 Chunk 组成，每个 Chunk 为 512B，并且每个 Chunk 还包含了 4B 的 crc 校验信息，共 516B。当 Packet 被 Chunk 填满以后就将 Packet 放入到应答队列 DataQueue 队列中等待，再从该队列中将 Packet 取出存入另一个队列 ACKQueue 队列中，然后将该 Patcket 发送给离客户端最近的 DataNode。 第一个 DataNode 收到数据后，会将该数据传递给下一个 DataNode，每个节点收到数据后都会逐级传递给下个节点直至 PipeLine 中的最后一个节点。并且在传输 Packet 的过程中，每向 DataNode 发送一个 Packet 数据客户端都会等待来自 DataNode 的应答响应，若响应则代表当前节点收来自客户端发送的数据，若没有响应时会将 ACKQueun 中的 Packet 数据重新放回到 DataQueue 队列中等待再次发送，避免了数据丢失。 当第一个 Block 块信息传输完成后，客户端会再次请求 NameNode 上传第其他个 Block 块信息。 3. 读取流程 # 客户端通过 DistributeFileSystem 向 NameNode 申请下载文件请求，NameNode 收到客户端下载请求后通过 查询文件的元数据找到文件所在的 DataNode，然后返回包含所有该下载文件的 DataNode 给客户端。 客户端收到 NameNode 发送回来的 DataNode 信息后，会寻找距当前节点最近的 DataNode 节点请求下载文件。 DataNode 收到客户端下载请求后开始向客户端发送数据，先将磁盘中的块信息通过输入流读出到内存，然后以 Packet 作为单位校验，传输给客户端。 客户端收到 Packet 后先将文件读入到内存然后通过输出流写出到磁盘存储。 4. 参考文献 # 详解 HDFS 底层交互原理 Hadoop 之 HDFS "},{"id":98,"href":"/posts/apache-hadoop/ch02/","title":"Ch02-Hadoop 之 HDFS 架构","section":"Blog","content":"HDFS 分布式部署场景下最常见的为两种架构，一种是基本的分布式架构，另一种是 HA 架构。在生产环境中一般都会部署 HA 架构。\n1. HDFS 基础架构 # 1.1 基本组成 # HDFS 由 NameNode，Secondary NameNode，DataNode，Client 这四部分组成。NameNode 负责管理元数据和文件信息；Secondary NameNode 主要用于定期合并 Fsimage 文件和 Edits 文件；DataNode 用户完成数据的存储和读取等；Client 提供了各种操作 HDFS 的 API 接口。\nFsimage 文件：也称为 Checkpoint 文件，记录了 HDFS 文件系统所有目录以及文件相关信息（Block 数量，副本数量，权限等信息），它是 NameNode 内存中元数据序列化后形成的文件。\nEdits 文件：记录 Client 对指定 HDFS 文件的增删改操作\n1.2 CheckPoint 过程 # 第一阶段：NameNode 启动 # 第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载 Fsimage 和 Edits 文件到内存。 客户端对元数据进行增删改的请求。 NameNode 记录操作日志，更新滚动日志。 NameNode 在内存中对数据进行增删改。 第二阶段：Secondary NameNode 工作 # Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode 是否执行检查点操作结果。 Secondary NameNode 请求执行 CheckPoint。 NameNode 滚动正在写的 Edits 日志。 将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。 Secondary NameNode 加载编辑日志和镜像文件到内存并合并。 生成新的镜像文件 fsimage.ckpt。 拷贝 fsimage.ckpt 到 NameNode。 NameNode 将 fsimage.ckpt 重新命名成 fsimage。 2. HDFS HA 架构 # 2.1 基本组成 # HDFS 中，DataNode 自身保存了多个副本，所以某种意义上来说本身就是 HA 架构；NameNode 和 Secondary NameNode 本身却只是单点结构，如果 NameNode 宕机，那么 HDFS 将无法对外提供服务，所以社区就设计了 HA 架构（也叫做 QJM(Quorum Journal Manager) ）。其结构如下图所示。\n原理较为简单，将 NameNode 变成多个并将其划分不同角色（1 个 Active NameNode，n 个 Standy NameNode），只让 Active NameNode 提供 NameNode 的服务。接着设计了个名称为 ZKFC 的监控组件，不断的监控所有 NameNode 状态，如果发现 Active NameNode 宕机了，便从 Standby NameNode 中选一个并切换角色为 Active NameNode，同时也会保证只有一个 Active NameNode 在线。\n为了能够保证所有 NameNode 看到的 edit 文件一致，所以社区又设计了 JournalNode Cluster（基于 Poxos 算法保证 edit 一致性），这样只需要向 JournalNode Cluster 中写 edit 文件便可。 基础架构中的 NameNode 功能由 HA 架构中的 Active NameNode 完成，那 Secondary NameNode 岂不是没了？事实上 HA 架构中的 Standby NameNode 维护了一个线程，会不断的监听 JournalNode Cluster 中的 edit 文件变更，如果满足条件（比如时间到了）那么便拉取 edit 文件并与 fsimage 文件合并，最后重新发送回 Active NameNode。\n3. Fsmage 和 edit 小细节 # 文件名自带 txnid，可以用来确认 fsimage 和 edits 之间的关系。同目录中还有一个名为 seen_txid 的文件，保存了最后一个 edits 文件名的 txnid。\nedits 文件名称：edits_0000000000000000264-edits_0000000000000000267 fsimage 文件名称：fsimage_0000000000000000267 比如上述两个文件，fsimage (*-0267) 已经将 edits 文件 (*0264-*267) 合并进来了，此时可以删除 edits 文件 (*0264-*267) 文件。\n4. 参考文献 # Hadoop 之 HDFS Namenode 与 SecondaryNameNode 详解 HDFS 底层交互原理 "},{"id":99,"href":"/posts/apache-hadoop/ch01/","title":"Ch01-Hadoop 介绍","section":"Blog","content":"2012 年社区发布了 Hadoop 2.0-alpha，自此 Hadoop 开启了 2.0 时代，相比较与 1.0 引入了 Yarn，NameNode HA 等重要组件和功能。随后数年 Hadoop 1.0 也慢慢的退出了历史舞台，所以这里也仅仅讲述 Hadoop 2.0 相关。\n广义上来说 Hadoop ≈ 大数据，于是不少博文中在写 Hadoop 的过程中还会出现 Spark，Hive 等其他组件的介绍。个人并不是非常认同这种分类方式，Hadoop 只是大数据技术中的一个小小的处理工具/软件而已，作为一个软件应该有专属于自己的说明内容，在说明自己专属内容的时候再掺杂其他的软件说明是不妥的。\n1. 基本信息 # 条目 说明 官网 https://hadoop.apache.org/ 下载地址 https://hadoop.apache.org/releases.html 2. 架构介绍 # Hadoop 由三个部分组成，分别是 HDFS，Yarn，MapReduce。HDFS 提供存储功能，Yarn 提供调度功能，MapReduce 提供计算框架。 用不准确的表述将所有的功能串起来大概是 Yarn 调度使用 MapReduce 计算框架读取并计算存储在 HDFS 上的数据，最终又重新将计算结果写入到 HDFS 中。\n3. 参考文献 # hadoop 1.0 和 hadoop 2.0 的区别 "},{"id":100,"href":"/posts/mysql/ch02/","title":"Ch02-MySQL 之 InnoDB 内存结构和存储结构","section":"Blog","content":"InnoDB 的内存结构主要由 Buffer Pool, Change Buffer, Adaptive Hash Index, Log Buffer 这几个部分组成，而存储结构则是由若干种不同的 Tablespace 组成。如下图所示。\n不同版本的 Arch 见 MySQL-5.6, MySQL-5.7, MySQL-8.0\n1. InnoDB 内存结构 # 条目 说明 Buffer Pool 也称为读缓存，以 page 为单位缓存数据，使用改进过的 LRU 算法进行淘汰 Change Buffer 也称为写缓存，对 page 进行了修改，先缓存到这里，后续 merge 后统一写入到磁盘 Adaptive Hash Index 简写为AHI，它可以简单理解为定制版索引，当访问某些 Page 非常频繁的时候，则为该 Page 建立索引 Log Buffer 2. InnoDB 存储结构 # 上图描述的是 General Tablespaces 和 File-Per-Table Tablespaces 的内部组织形式。\n2.1 Tablespaces # 条目 说明 System Tablespaces 包含 innodb 相关对象的元数据，doublewrite buffer, change buffer, undo logs 等 File-Per-Table Tablespaces .idb 文件，是一个单表表空间，该表创建于自己的数据文件中，而非创建于系统表空间中 Undo Tablespaces .ibu 文件，由一个或多个包含 undo 日志的文件组成 General Tablespaces .idb 文件，通过 create tablespace 语法创建的共享表空间 Temporary Tablespaces .ibt 文件，用户创建的临时表和磁盘内部临时表创建于共享临时表空间中 2.2 Segment # 是一个逻辑概念，并不对应任何一个连续的空间物理区域，它主要被用来区分不同功能的 Extent，和碎片区中的 Page。常见的有“叶子节点段 (non-leaf node segment)，“非叶子节点段 (leaf node segment)”等。\n备注： 非叶子节点段 和 叶子节点段 对应 B+ 树中的非叶子节点和叶子节点。\n非叶子节点段：存储和管理索引树 叶子节点段：存储和管理实际数据 2.3 Extent # 固定大小，为 1MB，存放 64 个地址连续的 Page。\n这里有个小细节，在初次创建的表的时候，仅仅会创建包含 7 个 Page 的 Extent，而不是 64 个 Page 的完整 Extent，这些 Page 会被保存在碎片区中。随着数据的增加，则会申请包含 64 个 Page 的 新的 Extent。\n256 个 Extent 组成一个 Extent Group，其大小为 256MB。\n2.4 Page # Page 是 InnoDB 存储引擎磁盘管理的最小单位，固定大小，为 16 KB。页有很多不同的分类，常见的大概有如下几种。\n条目 说明 FIL_PAGE_TYPE_ALLOCATED 最新分配，还未使用 FIL_PAGE_UNDO_LOG undo 日志页 FIL_PAGE_INODE 存储 Segment 的信息 FIL_PAGE_IBUF_FREE_LIST Change Buffer 空闲列表 FIL_PAGE_IBUF_FREE_BITMAP Change Buffer 的一些属性 FIL_PAGE_TYPE_SYS 存储一些系统属性 FIL_PAGE_TYPE_XDES 存储 Extent 的信息 FIL_PAGE_TYPE_TRX_SYS 存储事务相关的系统数据 FIL_PAGE_TYPE_FSP_HDR 表空间头部信息 FIL_PAGE_INDEX 索引页，或者常说的数据页 2.5 Row # Row 支持 4 种格式，不定大小，最大长度为 8KB，\n条目 说明 REDUNDANT COMPACT DYNAMIC 默认类型， COMPRESSED "},{"id":101,"href":"/posts/mysql/ch01/","title":"Ch01-MySQL 介绍","section":"Blog","content":"MySQL 是一个关系型数据库管理系统，由瑞典 MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL 是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。\n1. 基本信息 # 条目 说明 官网 https://www.mysql.com/ 下载地址 https://dev.mysql.com/downloads/mysql/ 2. 架构组成 # 下图展示了 MySQL 的架构图，我们可以看出 MySQL 的架构自顶向下大致可以分为网络连接层、数据库服务层、存储引擎层和系统文件层四大部分。\n2.1 网络连接层 # 网络连接层位于整个 MySQL 体系架构的最上层，主要担任客户端连接器的角色。提供与 MySQL 服务器建立连接的能力，几乎支持所有主流的服务端语言，例如：Java、C、C++、Python 等，各语言都是通过各自的 API 接口与 MySQL 建立连接。\n2.2 数据库服务层 # 条目 说明 系统管理和控制工具 提供数据库系统的管理和控制功能，例如对数据库中的数据进行备份和恢复，保证整个数据库的安全性，提供安全管理，对整个数据库的集群进行协调和管理等。 连接池 主要负责存储和管理客户端与数据库的连接信息，连接池里的一个线程负责管理一个客户端到数据库的连接信息。 SQL 接口 主要负责接收客户端发送过来的各种 SQL 命令，并将 SQL 命令发送到其他部分，并接收其他部分返回的结果数据，将结果数据返回给客户端。 解析器 主要负责对请求的 SQL 解析成一棵“语法树”，然后根据 MySQL 中的一些规则对“语法树”做进一步的语法验证，确认其是否合法。 查询优化器 在 MySQL 中，如果“语法树”通过了解析器的语法检查，此时就会由优化器将其转化为执行计划，然后与存储引擎进行交互，通过存储引擎与底层的数据文件进行交互。 缓存 MySQL 的缓存是由一系列的小缓存组成的。例如：MySQL 的表缓存，记录缓存，MySQL 中的权限缓存，引擎缓存等。MySQL 中的缓存能够提高数据的查询性能，如果查询的结果能够命中缓存，则 MySQL 会直接返回缓存中的结果信息。 2.3 存储引擎层 # MySQL 中的存储引擎层主要负责数据的写入和读取，与底层的文件进行交互。值得一提的是，MySQL 中的存储引擎是插件式的，服务器中的查询执行引擎通过相关的接口与存储引擎进行通信，同时，接口屏蔽了不同存储引擎之间的差异。MySQL 中，最常用的存储引擎就是 InnoDB 和 MyISAM。\n条目 InnoDB MyISAM 事务支持 支持 不支持 存储结构 所有的表都保存在系统表空间，或者每张表各自的表空间 每张表在磁盘上存储成三个文件 存储空间 需要更多的内存和存储，在主内存中建立其专用的缓冲池用于高速缓冲数据和索引 可被压缩，存储空间较小 表锁差异 支持事务和行级锁 只支持表级锁 全文索引 不支持 (FULLTEXT 类型的) 全文索引，但是 innodb 可以使用 sphinx 插件支持全文索引，并且效果更好 支持 (FULLTEXT 类型的) 全文索引 主键 如果没有设定主键或者非空唯一索引，就会自动生成一个 6 字节的主键 (用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值 允许没有任何索引和主键的表存在，索引都是保存行的地址 外键 支持 不支持 2.4 系统文件层 # 系统文件层主要包括 MySQL 中存储数据的底层文件，与上层的存储引擎进行交互，是文件的物理存储层。其存储的文件主要有：日志文件、数据文件、配置文件、MySQL 的进行 pid 文件和 socke\n条目 说明 日志文件 包括错误日志、通用查询日志、二进制日志、慢查询日志等 数据文件 db.opt 文件、frm 文件 (MySQL 8.0 无此文件)、MYD 文件、MYI 文件、ibd 文件、ibdata 文件、ibdata1 文件、ib_logfile0 和 ib_logfile1 文件等。 配置文件 在Unix/Linux环境中是my.cnf文件，在 Windows 环境中是my.ini文件。 pid 文件 pid 文件是存放 MySQL 进程运行时的进程号的文件 socket 文件 socket 文件和 pid 文件一样，都是 MySQL 在 Unix/Linux 环境中运行才会有的文件。 "},{"id":102,"href":"/posts/database/ch01/","title":"Ch01-数据库理论 之 事务","section":"Blog","content":"数据库事务 (Database Transaction)，是指作为单个逻辑工作单元执行的一系列操作，要么完全执行，要么完全地不执行。要么完全地不执行。一般来说，事务是必须满足 4 个条件 (ACID)：原子性 (Atomicity)、一致性 (Consistency)、隔离性 (Isolation)、持久性 (Durability)。\n1. 原子性 (Atomicity) # 一个事务 (transaction) 中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚 (Rollback) 到事务开始前的状态，就像这个事务从来没有执行过一样。\n2. 一致性 (Consistency) # 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。\n3. 隔离性 (Isolation) # 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。\n4. 持久性 (Durability) # 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n5. 参考文献 # 数据库事务系列－事务模型基础 "},{"id":103,"href":"/posts/jdk/ch06/","title":"Ch06-Java 之 垃圾收集算法","section":"Blog","content":"在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。由于 Java 虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，所以在此只讨论几种常见的垃圾收集算法的核心思想。\n1. 判断一个对象是否可被回收 # 1.1 引用计数算法 # 引用计数算法给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。正因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。\npublic class ReferenceCountingGC { public Object instance = null; public static void main(String[] args) { ReferenceCountingGC objectA = new ReferenceCountingGC(); ReferenceCountingGC objectB = new ReferenceCountingGC(); objectA.instance = objectB; objectB.instance = objectA; } } 1.2 可达性分析算法 # 通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。\nJava 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容：\n虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 2. 垃圾回收算法 # 2.1 标记 - 清除算法 # 标记 - 清除算法 (Mark-Sweep) 这是最基础的垃圾回收算法，之所以说它是最基础的是因为它最容易实现，思想也是最简单的。标记 - 清除算法分为两个阶段：标记阶段和清除阶段。\n标记阶段的任务是标记出所有需要被回收的对象； 清除阶段就是回收被标记的对象所占用的空间； 具体过程如下图所示：\n但是它存在两点重要不足：\n效率问题：标记和清除两个过程的效率都不高； 空间问题：标记清除后会产生大量不连续的内存碎片，空间碎片太多可能导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不再次发出一次垃圾收集动作； 2.2 复制算法 # 复制算法 (Copying) 为了解决标记 - 清除算法中效率问题的缺陷。\n它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块； 当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用的内存空间一次清理掉； 这样一来就不容易出现内存碎片的问题。\n具体过程如下图所示：\n这种算法虽然实现简单，运行高效且不容易产生内存碎片，但是却对内存空间的使用做出了高昂的代价，因为能够使用的内存缩减到原来的一半。很显然，复制算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么 Copying 算法的效率将会大大降低。\n实际的商业虚拟机会采用这种算法来回收新生代，当然并不是按照 1:1 的比例划分内存空间，而是将内存空间划分为一块比较大的 Eden 空间和两块比较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地复制到另一块 Survivor 空间上（理论上，大部分的对象会被回收掉，因此这里会假设另一块 Survivor 空间足够），最后清理掉 Eden 和刚才用过的 Survivor 空间。\nHotSpot 虚拟机默认 Eden 和 Survivor 的大小比例是 8:1，当 Survivor 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保(Handle Promotion)。因此当 Survivor 空间真的不足以存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。\n2.3 标记 - 整理算法 # 复制算法在对象存活率比较高时就要进行比较多的复制操作，效率将会降低，而且需要额外的空间对其分配担保，以应对被使用内存中所有对象都 100% 存活的极端情况，所以老年代一般不能选择这种算法。\n标记 - 整理算法 (Mark-Compact) 过程与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。具体过程如下图所示：\n2.4 分代收集算法 # 分代收集算法 (Generational Collection) 是目前大部分虚拟机的垃圾收集器采用的算法。\n它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。\n对于新生代会采取复制算法，因为在新生代中，每次垃圾收集时都会发现大量对象死去，只有少量存活，也就是说仅需要少量的复制操作。 对于老年代会采用标记-清除算法或标记-整理算法，因为对象存活率高，也没有额外空间对它进行分配担保。 "},{"id":104,"href":"/posts/jdk/ch05/","title":"Ch05-Java 之 引用类型","section":"Blog","content":"无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。\nJava 具有四种强度不同的引用类型。\n1. 强引用 # 被强引用关联的对象不会被回收。\n使用 new 一个新对象的方式来创建强引用。\nObject obj = new Object(); 2. 软引用 # 被软引用关联的对象只有在内存不够的情况下才会被回收。\n使用 SoftReference 类来创建软引用。\nObject obj = new Object(); SoftReference\u0026lt;Object\u0026gt; sf = new SoftReference\u0026lt;Object\u0026gt;(obj); obj = null; // 使对象只被软引用关联 3. 弱引用 # 被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。\n使用 WeakReference 类来实现弱引用。\nObject obj = new Object(); WeakReference\u0026lt;Object\u0026gt; wf = new WeakReference\u0026lt;Object\u0026gt;(obj); obj = null; 4. 虚引用 # 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象。\n为一个对象设置虚引用关联的唯一目的就是能在这个对象被回收时收到一个系统通知。\n使用 PhantomReference 来实现虚引用。\nObject obj = new Object(); PhantomReference\u0026lt;Object\u0026gt; pf = new PhantomReference\u0026lt;Object\u0026gt;(obj); obj = null; "},{"id":105,"href":"/posts/jdk/ch04/","title":"Ch04-Java 之 类的整个生命周期","section":"Blog","content":"其中类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持 Java 语言的运行时绑定 (也成为动态绑定或晚期绑定)。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。\n1. 类的生命周期 # 1.1 加载 # 查找并加载类的二进制数据，加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情：\n通过一个类的全限定名来获取其定义的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象，作为对方法区中这些数据的访问入口。 加载.class 文件的方式\n从本地系统中直接加载 通过网络下载.class 文件 从 zip，jar 等归档文件中加载.class 文件 从专有数据库中提取.class 文件 将 Java 源文件动态编译为.class 文件\n1.2 验证 # 确保被加载的类的正确性，这一阶段的目的是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。\n验证阶段大致会完成 4 个阶段的检验动作：\n文件格式验证：验证字节流是否符合 Class 文件格式的规范；例如：是否以 0xCAFEBABE 开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析 (注意：对比 javac 编译阶段的语义分析)，以保证其描述的信息符合 Java 语言规范的要求；例如：这个类是否有父类，除了 java.lang.Object 之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 1.3 准备 # 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。\n该阶段有以下几点需要注意\n这时候进行内存分配的仅包括类变量 (static)，而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值通常情况下是数据类型默认的零值 (如 0、0L、null、false 等)，而不是被在 Java 代码中被显式地赋予的值。 对基本数据类型来说，对于类变量 (static) 和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。 对于同时被 static 和 final 修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被 final 修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，系统不会为其赋予默认零值。 对于引用数据类型 reference 来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即 null。 如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。 如果类字段的字段属性表中存在 ConstantValue 属性，即同时被 final 和 static 修饰，那么在准备阶段变量 value 就会被初始化为 ConstValue 属性所指定的值。 1.4 解析 # 把类中的符号引用转换为直接引用，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符 7 类符号引用进行。符号引用就是一组符号来描述目标，可以是任何字面量。\n1.5 初始化 # 初始化，为类的静态变量赋予正确的初始值，JVM 负责对类进行初始化，主要对类变量进行初始化。在 Java 中对类变量进行初始值设定有两种方式，声明类变量是指定初始值，使用静态代码块为类变量指定初始值。\nJVM 初始化步骤\n假如这个类还没有被加载和连接，则程序先加载并连接该类 假如该类的直接父类还没有被初始化，则先初始化其直接父类 假如类中有初始化语句，则系统依次执行这些初始化语句 类初始化时机\n只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种\n创建类的实例，也就是 new 的方式 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射 (如 Class.forName(\u0026ldquo;com.pdai.jvm.Test\u0026rdquo;)) 初始化某个类的子类，则其父类也会被初始化 Java 虚拟机启动时被标明为启动类的类 (Java Test)，直接使用 java.exe 命 1.6 使用 # 类访问方法区内的数据结构的接口，对象是 Heap 区的数据。\n1.7 卸载 # Java 虚拟机将结束生命周期的几种情况\n执行了 System.exit() 方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致 Java 虚拟机进程终止 2. 类加载 # 2.1 JVM 类加载器 # 类加载器 说明 启动类加载器 Bootstrap ClassLoader，负责加载存放在 JDK\\jre\\lib(JDK 代表 JDK 的安装目录，下同) 下，或被-Xbootclasspath 参数指定的路径中的，并且能被虚拟机识别的类库 (如 rt.jar，所有的 java.*开头的类均被 Bootstrap ClassLoader 加载)。启动类加载器是无法被 Java 程序直接引用的 扩展类加载器 Extension ClassLoader，该加载器由 sun.misc.Launcher$ExtClassLoader 实现，它负责加载 JDK\\jre\\lib\\ext 目录中，或者由 java.ext.dirs 系统变量指定的路径中的所有类库 (如 javax.*开头的类)，开发者可以直接使用扩展类加载器 应用程序类加载器 Application ClassLoader，该类加载器由 sun.misc.Launcher$AppClassLoader 来实现，它负责加载用户类路径 (ClassPath) 所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器 2.1 JVM 类加载机制 # 加载机制 说明 全盘负责 当一个类加载器负责加载某个 Class 时，该 Class 所依赖的和引用的其他 Class 也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托 先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制 缓存机制将会保证所有加载过的 Class 都会被缓存，当程序中需要使用某个 Class 时，类加载器先从缓存区寻找该 Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成 Class 对象，存入缓存区。这就是为什么修改了 Class 后，必须重启 JVM，程序的修改才会生效 双亲委派机制 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类 "},{"id":106,"href":"/posts/jdk/ch03/","title":"Ch03-Java 之 Java 对象结构","section":"Blog","content":"在 HotSpot 虚拟机中，对象在内存中存储的布局可以分为三块区域：对象头 (Header)、 实例数据 (Instance Data)、对齐填充 (Padding)。\n1. 内存布局 # 内存布局 说明 对象头 HotSpot 虚拟机的对象头包括两部分信息，Mark Word（标记字段）和 Klass Pointer（类型指针） 实例数据 即对象真正存储的有效信息，也是程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是子类中定义的，都需要记录起来。 对齐填充 对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。会将对象补充为 8 的倍数。 2. 对象头 # 在 JVM 中，Hotspot 并没有将 Java 对象映射成 C++对象，而是实现了 Java 的对象模型（OOP-Klass）。JVM 不希望每个对象中都包含一份虚函数表，所以就把对象模型拆成 klass 和 oop，其中 oop 中不含有任何虚函数，而 klass 就含有虚函数表，可以进行 method dispatch。\nOOP 英文全称是 Ordinary Object Pointer，即普通对象指针，看起来像个指针实际上是藏在指针里的对象，表示对象的实例信息。 Klass 元数据和方法信息，用来描述 Java。是 Java 类的在 C++中的表示形式，用来描述 Java 类的信息。 当加载一个 Class 时，会创建一个 InstanceKlass 对象，实例化的对象则对应 instanceOopDesc，它继承自 oopDesc，用于表示普通的 Java 对象，每次 new 一个 Java 对象就会创建一个新的 instanceOopDesc 实例，其中 InstanceKlass instanceOopDesc 存放在堆中。\nclass oopDesc { private: volatile markWord _mark; union _metadata { Klass* _klass; narrowKlass _compressed_klass; } _metadata; } class instanceOopDesc : public oopDesc { public: static int header_size() { return sizeof(instanceOopDesc)/HeapWordSize; } } 2.1 markWord # 字节分配 说明 无锁 对象头开辟 25bit 的空间用来存储对象的 hashcode，4bit 用于存放对象分代年龄，1bit 用来存放是否偏向锁的标识位，2bit 用来存放锁标识位为 0，其标识为 01 偏向锁 开辟 25bit 的空间，其中 23bit 用来存放线程 ID，2bit 用来存放 Epoch，4bit 存放对象分代年龄，1bit 存放是否偏向锁标识，0 表示无锁，1 表示偏向锁，锁的标识为 01 轻量级锁 开辟 30bit 的空间存放指向栈中锁记录的指针，2bit 存放锁的标志位，其标识为 00 重量级锁 开辟 30bit 的空间用来存放指向重量级锁的指针，2bit 存放锁的标识位，其标识为 11 GC 标记 开辟 30bit 的内存空间却没有占用，2bit 空间存放锁标识为 11 2.2 _metadata # 多个对象的 ModelA 和 ModelB 的定义如下\nclass Model { public static int a = 1; public int b; public Model(int b) { this.b = b; } public static void main(String[] args) { int c = 10; Model modelA = new Model(2); Model modelB = new Model(3); } } 3. 对象的访问定位 # 3.1 使用句柄访问 # 如果使用句柄访问方式，Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。\n3.2 使用直接指针访问 # 如果使用直接指针访问方式，java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，reference 中直接存储的就是对象地址。\n4. 参考文献 # Java 对象模型（OOP-Klass 模型） "},{"id":107,"href":"/posts/jdk/ch02/","title":"Ch02-Java 之 JVM 内存结构 (JVM)","section":"Blog","content":"Java 虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中有一些会随着虚拟机启动而创建，随着虚拟机退出而销毁。另外一些则是与线程一一对应的，这些与线程一一对应的数据区域会随着线程开始和结束而创建和销毁。\n线程私有：程序计数器、虚拟机栈、本地方法区 线程共享：堆、方法区，堆外内存（Java7 的永久代或 JDK8 的元空间、代码缓存） 1. 堆 # 堆内存主要用于存放对象和数组，它是 JVM 管理的内存中最大的一块区域，堆内存和方法区都被所有线程共享，在虚拟机启动时创建。在垃圾收集的层面上来看，由于现在收集器基本上都采用分代收集算法，因此堆还可以分为新生代（YoungGeneration）和老年代（OldGeneration），新生代还可以分为 Eden、From Survivor、To Survivor。\n2. 虚拟机栈 # 每个线程运行时所需要的内存，称为虚拟机栈。每个栈由多个栈帧（Frame）组成，对应着每次方法调用时所占用的内存。每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法\n3. 本地方法栈 # 虚拟机栈为虚拟机执行 Java 方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。\n4. 方法区 # 方法区在虚拟机启动时被创建，逻辑上是堆的组成部分，这里的方法区指的是规范，永久代（jdk1.6，占用的是堆内存）和元空间（jdk1.8，占用的是系统的内存）只是其实现。\nStringTable 在 jdk1.6 实现在永久代（方法区）中，jdk 1.8 后是现在堆中，majorGC 就可以回收掉，节约空间。 运行时常量池是方法区的一部分，编译期生成的各种字面量和符号引用在类加载后会被存放到运行时常量池中 5. 程序计数器 # 又称 PC 寄存器，记住下一条 jvm 指令的执行地址。字节码解释器工作是就是通过改变这个计数器的值来选取下一条需要执行指令的字节码指令，分支、循环、跳转、异常处理、线程恢复等。\n"},{"id":108,"href":"/posts/jdk/ch01/","title":"Ch01-Java 之 Java 内存模型 (JMM)","section":"Blog","content":"JMM 定义了 Java 虚拟机 (JVM) 在计算机内存 (RAM) 中的工作方式，JVM 是 java 整个计算虚拟模型。\n1. JMM 介绍 # 根据 JMM 模型，JVM 把内存分成了线程栈区（工作内存）和堆区（主内存）。\n一个变量如何从主内存拷贝到工作内存，如何从工作内存同步回主内存之类的实现细节，Java 内存模型中定义了以下 8 种操作来完成。这 8 种操作都是原子性的、不可再分的。\n操作 说明 lock 作用于主内存变量，它把一个变量标识为一条线程独占的状态。 unlock 作用于主内存变量，它把一个处理锁定的状态的变量释放出来，释放后的变量才可以被其它线程锁定，unlock 之前必须将变量值同步回主内存。 read 作用于主内存变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用。 load 作用于工作内存变量，它把 read 操作从主内存中得到的值放入工作内存的变量副本中。 use 作用于工作内存中的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的字节码指令时将会执行这个操作。 assign 作用于工作内存变量，它把一个从执行引擎接到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store 作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的 write 操作使用。 write 作用于主内存的变量，它把 store 操作从工作内存中得到的值放入主内存的变量中。 2. JMM 与 JVM 之间的关系 # 3. JMM 与 硬件内存的关系 # 参考文献 # Java 内存模型引入 "},{"id":109,"href":"/posts/java-juc/ch05/","title":"Ch5-Java 之 AQS","section":"Blog","content":"AQS 即 java.util.concurrent.locks.AbstractQueuedSynchronizer，可以用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器。\n1. 介绍 # AbstractQueuedSynchronizer 主要维护了两个队列（Sync Queue, Condition Queue）和一个临界变量（state）。\nSync Queue 采用的是 CLH(Craig,Landin,and Hagersten) 虚拟双向队列，AQS 将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点 (Node) 来实现锁的分配。 Condition Queue 是一个单向链表，只有当使用 Condition 时，才会存在此单向链表，并且可能会有多个。 state!=0 代表当前对象锁已经被占有，其他线程来加锁时则会失败，失败的线程会被放到 Sync Queue 中。state 的操作都是通过 CAS 机制来保证修改的安全性。 CLH 锁 Craig、Landin and Hagersten 是一种基于链表的可扩展、高性能、公平的自旋锁，申请线程只在本地变量上自旋，它不断轮询前驱的状态，如果发现前驱释放了锁就结束自旋。\n2. 实现的同步工具 # 同步工具 同步工具与 AQS 的关联 ReentrantLock 使用 state 保存锁重复持有的次数。当一个线程获取锁时，ReentrantLock 记录当前获得锁的线程标识，用于检测是否重复获取，以及错误线程试图解锁操作时异常情况的处理。 Semaphore 使用 state 来保存信号量的当前计数。tryRelease 会增加计数，acquireShared 会减少计数。 CountDownLatch 使用 state 来表示计数。计数为 0 时，所有的 Acquire 操作（CountDownLatch 的 await 方法）才可以通过。 ReentrantReadWriteLock 使用 state 的前 16 位保存写锁持有的次数，后 16 位用于保存读锁的持有次数。 ThreadPoolExecutor Worker 利用 state 同步状态实现对独占线程变量的设置（tryAcquire 和 tryRelease）。 3. 参考文献 # 【深入 AQS 原理】我画了 35 张图就是为了让你深入 AQS Java 并发编程之锁机制之 AQS(AbstractQueuedSynchronizer) AQS 源码分析 (二) [tryAcquire 代码详解与 addWaiter 动画演示] AQS 源码分析 (三) [acquireQueued 动画演示] "},{"id":110,"href":"/posts/java-juc/ch04/","title":"Ch4-Java 之 JUC","section":"Blog","content":"JUC 即 java.util.concurrent，是 Java 并发编程中很常用的实用工具类。它包含了 Lock 框架，Collections 并发集合，Executors 线程池，Atomic 原子类，Tools。\nJUC 说明 Lock 框架 AbstractQueuedSynchronizer, ReentrantLock, ReentrantReadWriteLock Collections 并发集合 ArrayBlockingQueue, LinkedBlockingQueue, LinkedBlockingDequeue, ConcurrentLinkedQueue, ConcurrentLinkedDequeue, DelayQueue Executors 线程池 Executors, ThreadPoolExecutor, RejectedExecutionHandler Atomic 原子类 AtomicInteger, AtomicIntegerArray, AtomicIntegerFieldUpdater, AtomicStampedReference, ... Tools CyclicBarrier, Semaphore, CountDownLatch, Phaser, Exchanger "},{"id":111,"href":"/posts/java-juc/ch03/","title":"Ch3-Java 之 ThreadLocal","section":"Blog","content":"java.lang.ThreadLocal是一个将在多线程中为每一个线程创建单独的变量副本的类; 当使用 ThreadLocal 来维护变量时，ThreadLocal 会为每个线程创建单独的变量副本，避免因多线程操作共享变量而导致的数据不一致的情况。\n1. 基本原理 # 如下图所示，每个 Thread 中都维护了一个类型为 ThreadLocal.ThreadLocalMap 的属性 threadLocals，这就意味着每个线程都有独属于自己的一段小空间（内存）。当线程向 ThreadLocal 添加数据时本质上都是添加到每个线程自己的内存空间中，这就容易理解为什么说 ThreadLocal 可以实现线程间变量隔离了。\n2. 存在的问题 # ThreadLocalMap 这个类定义的 Entry（用于存储每个\u0026lt;K, V\u0026gt;）是继承自 WeakReference。那也就是说，发生一次 GC 后，这个值可能会变成 null。但是 Entry 里面的值仍然有可能不被释放，于是就有可能发生内存泄露，所以最好的做法是将调用 threadlocal 的 remove 方法。\nJava 为了最小化减少内存泄露的可能性和影响，在 ThreadLocal 的 get,set 的时候都会清除线程 Map 里所有 key 为 null 的 value。所以最怕的情况就是，threadLocal 对象设 null 了，开始发生“内存泄露”，然后使用线程池，这个线程结束，线程放回线程池中不销毁，这个线程一直不被使用，或者分配使用了又不再调用 get,set 方法，那么这个期间就会发生真正的内存泄露。\n3. Entry 为什么使用弱引用 # 下面我们分两种情况讨论\nkey 使用强引用：引用的 ThreadLocal 的对象被回收了，但是 ThreadLocalMap 还持有 ThreadLocal 的强引用，如果没有手动删除，ThreadLocal 不会被回收，导致 Entry 内存泄漏。 key 使用弱引用：引用的 ThreadLocal 的对象被回收了，由于 ThreadLocalMap 持有 ThreadLocal 的弱引用，即使没有手动删除，ThreadLocal 也会被回收。value 在下一次 ThreadLocalMap 调用 set,get，remove 的时候会被清除。 比较两种情况，我们可以发现：由于 ThreadLocalMap 的生命周期跟 Thread 一样长，如果都没有手动删除对应 key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用 ThreadLocal 不会内存泄漏，对应的 value 在下一次 ThreadLocalMap 调用 set,get,remove 的时候会被清除。\n因此，ThreadLocal 内存泄漏的根源是：由于 ThreadLocalMap 的生命周期跟 Thread 一样长，如果没有手动删除对应 key 就会导致内存泄漏，而不是因为弱引用。\n4.InheritableThreadLocal # ThreadLocal保证了对一个资源进行set/get操作，兄弟线程，父子线程互不影响。但是有时可能需要兄弟线程互不影响，但是子线程可以继承父线程的资源。那么就可以使用 InheritableThreadLocal 实现。\n每个 Thread 中都维护了一个类型为 ThreadLocal.ThreadLocalMap 的属性 inheritableThreadLocals，该属性被用来存储父线程的 ThreadLocalMap，这样就可以使用父线程的资源。\n"},{"id":112,"href":"/posts/java-juc/ch02/","title":"Ch02-Java 之 线程基础","section":"Blog","content":" 1. 线程状态 # state 状态 说明 New 新建 新创建了一个线程对象，还未调用线程的 start() 方法 Runnable 就绪 线程对象创建后，其他线程调用了该对象的 start() 方法，该状态的线程位于可运行线程池中，变得可运行，等待获取 CPU 的使用权 Running 运行 就绪状态的线程获取了 CPU，执行程序代码的状态，还有种可能就是这个线程正在等待其他的系统资源（IO 资源等），这种状态也称为 Running 状态 Blocked 阻塞 一个线程因为等待监视锁而被阻塞的状态，也称之为阻塞状态。阻塞的线程不会被分配 CPU 资源 WAITING 等待 一个正在等待的线程的状态，也称之为等待状态 TIMED_WAITING 超时等待 一个在限定时间内等待的线程的状态。也称之为限时等待状态 Dead 死亡 线程执行完了或者因异常退出了 run() 方法，该线程结束生命周期 (当时如果线程被持久持有，可能不会被回收) 注意：\nWAITING: 造成线程等待的原因有三种，分别是调用 Object.wait()、join() 以及 LockSupport.park() 方法。处于等待状态的线程，正在等待其他线程去执行一个特定的操作。例如：因为 wait() 而等待的线程正在等待另一个线程去调用 notify() 或 notifyAll()；一个因为 join() 而等待的线程正在等待另一个线程结束。处于等待状态的线程不会被分配 CPU 资源。 TIMED_WAITING: 造成线程限时等待状态的原因有五种，分别是调用 Thread.sleep(long)、Object.wait(long)、join(long)、LockSupport.parkNanos(obj,long) 和 LockSupport.parkUntil(obj,long)。 2. 线程状态转换 # "},{"id":113,"href":"/posts/java-juc/ch01/","title":"Ch01-Java 之 理论基础","section":"Blog","content":" Happens-Before 规则 # 规则 说明 单一线程原则 在一个线程内，在程序前面的操作先行发生于后面的操作 管程锁定规则 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作 volatile 变量规则 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作 线程启动规则 Thread 对象的 start() 方法调用先行发生于此线程的每一个动作 线程加入规则 Thread 对象的结束先行发生于 join() 方法返回 线程中断规则 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生 对象终结规则 一个对象的初始化完成 (构造函数执行结束) 先行发生于它的 finalize() 方法的开始 传递性 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C 参考文献 # Java 并发 - 理论基础 "},{"id":114,"href":"/posts/software-engineering/ch05/","title":"Ch05-锁","section":"Blog","content":"锁概念\nJava # 乐观锁 VS 悲观锁 自旋锁 VS 适应性自旋锁 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁 公平锁 VS 非公平锁 可重入锁 VS 非可重入锁 读写锁 | 独享锁 (排他锁) VS 共享锁 C++ # 乐观锁 VS 悲观锁 自旋锁 条件锁 读写锁 可重入锁 (递归锁) VS 非可重入锁 (非递归锁) 读写锁 | 独享锁 (排他锁) VS 共享锁 MySQL # 读写锁 | 独享锁 (排他锁) VS 共享锁 意向排他锁 VS 意向共享锁 行级锁 VS 表级锁 VS 页级锁记录所 VS 间隙锁 VS 临键锁 "},{"id":115,"href":"/posts/software-engineering/ch04/","title":"Ch04-并发问题","section":"Blog","content":"并发问题\n1. 线程安全要素 # 要素 说明 引入原因 可见性 一个线程对共享变量的修改，另外一个线程能够立刻看到 CPU 缓存 原子性 一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行 分时复用 有序性 程序执行的顺序按照代码的先后顺序执行 编译重排序，指令重排序 理想情况下 CPU 应该使用 MESI 协议来保证不同 CPU 之间的缓存一致性，实际上，为了保证 cpu 的效率，AMD 用 MOESI 协议，Intel 用 MESIF 协议来保证缓存一致性，代价就是无法真正的保证缓存强一致性，需要软件使用 memory barrier（内存屏障）或者 lock 指令前缀等来保证变量在不同 cpu cache 中的强一致性。\n2. 线程安全分类 # 线程安全不是一个非真即假的命题，可以将共享数据按照安全程度的强弱顺序分成以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立。\n安全程度 说明 不可变 不可变的对象一定是线程安全的，不需要再采取任何的线程安全保障措施 绝对线程安全 不管运行时环境如何，调用者都不需要任何额外的同步措施 相对线程安全 需要保证对这个对象单独的操作是线程安全的，在调用的时候不需要做额外的保障措施。但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。 线程兼容 线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用 线程对立 线程对立是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码 3. 线程安全的实现方法 # 实现方法 说明 阻塞同步 也称为互斥同步，无论共享数据是否真的会出现竞争，它都要进行加锁 非阻塞同步 先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施，比如 CAS 无同步方案 如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，比如栈封闭，线程本地存储，可重入代码等 4. 语言相关的解决方案 # 4.1 Java 解决方案 # 要素 解决方案 可见性 基本数据类型读取和赋值；synchronized；Lock；volatile 原子性 synchronized；Lock; Atomic 有序性 synchronized；Lock；volatile + Happen Before 原则 4.2 C++ 解决方案 # 要素 解决方案 可见性 mutex；lock_guard；unique_lock；atomic + memory_order 原子性 mutex；lock_guard；unique_lock；atomic 有序性 mutex；lock_guard；unique_lock；atomic + memory_order "},{"id":116,"href":"/posts/java-map/ch12/","title":"Ch12-Java Map 之 WeakHashMap","section":"Blog","content":"WeakHashMap 的特殊之处在于 WeakHashMap 里的 entry 可能会被 GC 自动删除，即使程序员没有调用 remove() 或者 clear() 方法。\n当使用 WeakHashMap 时，即使没有显示的添加或删除任何元素，也可能发生如下情况：\n调用两次 size() 方法返回不同的值； 两次调用 isEmpty() 方法，第一次返回 false，第二次返回 true； 两次调用 containsKey() 方法，第一次返回 true，第二次返回 false，尽管两次使用的是同一个 key； 两次调用 get() 方法，第一次返回一个 value，第二次返回 null，尽管两次使用的是同一个对象。 所以 WeakHashMap 的这个特点特别适用于需要缓存的场景。在缓存场景下，由于内存是有限的，不能缓存所有对象；对象缓存命中可以提高系统效率，但缓存 MISS 也不会造成错误，因为可以通过计算重新得到。\n1. 底层数据结构 # 类似与 HashMap。\n"},{"id":117,"href":"/posts/java-map/ch03/","title":"Ch03-Java Map 之 TreeMap","section":"Blog","content":"TreeMap 实现了 SortedMap 接口，也就是说会按照 key 的大小顺序对 Map 中的元素进行排序，key 大小的评判可以通过其本身的自然顺序 (natural ordering)，也可以通过构造时传入的比较器 (Comparator)。\n1. 底层数据结构 # TreeMap 底层通过红黑树 (Red-Black tree) 实现，也就意味着 containsKey(), get(), put(), remove() 都有着 log(n) 的时间复杂度。\n"},{"id":118,"href":"/posts/java-map/ch02/","title":"Ch02-Java Map 之 LinkedHashMap","section":"Blog","content":"LinkedHashMap 是 HashMap 的直接子类，二者唯一的区别是 LinkedHashMap 在 HashMap 的基础上，采用双向链表 (doubly-linked list) 的形式将所有 entry 连接起来，这样是为保证元素的迭代顺序跟插入顺序相同。\n1. 底层数据结构 # LinkedHashMap 实现了 Map 接口，即允许放入 key 为 null 的元素，也允许插入 value 为 null 的元素。其主体部分跟 HashMap 完全一样，多了 header 指向双向链表的头部 (是一个哑元)，该双向链表的迭代顺序就是 entry 的插入顺序。除了可以保迭代历顺序，这种结构还有一个好处 : 迭代 LinkedHashMap 时不需要像 HashMap 那样遍历整个 table，而只需要直接遍历 header 指向的双向链表即可，也就是说 LinkedHashMap 的迭代时间就只跟 entry 的个数相关，而跟 table 的大小无关。\n2. FIFO Cache # LinkedHashMap 除了可以保证迭代顺序外，还有一个非常有用的用法：可以轻松实现一个采用了 FIFO 替换策略的缓存。\nclass FIFOCache\u0026lt;K, V\u0026gt; extends LinkedHashMap\u0026lt;K, V\u0026gt;{ private final int cacheSize; public FIFOCache(int cacheSize){ this.cacheSize = cacheSize; } // 当 Entry 个数超过 cacheSize 时，删除最老的 Entry @Override protected boolean removeEldestEntry(Map.Entry\u0026lt;K,V\u0026gt; eldest) { return size() \u0026gt; cacheSize; } } "},{"id":119,"href":"/posts/java-map/ch01/","title":"Ch01-Java Map 之 HashMap","section":"Blog","content":" 1. 底层数据结构 # Java 1.7 中使用数组+链表这样的数据结构，自 Java 1.8 开始使用数组+链表+红黑树这样的数据结构。\n2. 数组扩容 # resize() 方法用于初始化数组或数组扩容，每次扩容后，容量为原来的 2 倍，并进行数据迁移。\n3.线程不安全 # HashMap 的线程不安全主要体现在下面两个方面：\n在 Java 1.7 中，当并发执行扩容操作时会造成环形链和数据丢失的情况。 在 Java 1.8 中，在并发执行 put 操作时会发生数据覆盖的情况。 4. 参考文献 # JDK1.7 和 JDK1.8 中 HashMap 为什么是线程不安全的？ "},{"id":120,"href":"/posts/java-collection/ch05/","title":"Ch05-Java Collection 之 PriorityQueue","section":"Blog","content":"优先队列的作用是能保证每次取出的元素都是队列中权值最小的，这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序，也可以通过构造时传入的比较器 Comparator。\n1. 底层数据结构 # 底层基于数组实现实现的小根堆。\n2. 参考文献 # PriorityQueue "},{"id":121,"href":"/posts/java-collection/ch04/","title":"Ch04-Java Collection 之 ArrayDeque","section":"Blog","content":"Java 里有一个叫做 Stack 的类，却没有叫做 Queue 的类 (它是个接口名字)。当需要使用栈时，Java 已不推荐使用 Stack，而是推荐使用更高效的 ArrayDeque；既然 Queue 只是一个接口，当需要使用队列时也就首选 ArrayDeque 了 (次选是 LinkedList)。\n1. 底层数据结构 # Deque 的含义是“double ended queue”，即双端队列，它既可以当作栈使用，也可以当作队列使用。它基于循环数组实现，也就是说数组的任何一点都可能被看作起点或者终点。ArrayDeque 是非线程安全的 (not thread-safe)，当多个线程同时使用的时候，需要程序员手动同步；另外，该容器不允许放入 null 元素。\nhead 指向首端第一个有效元素，tail 指向尾端第一个可以插入元素的空位。因为是循环数组，所以 head 不一定总等于 0，tail 也不一定总是比 head 大。\n2. 接口定义 # Deque 是\u0026quot;double ended queue\u0026quot;, 表示双向的队列，英文读作\u0026quot;deck\u0026quot;. Deque 继承自 Queue 接口，除了支持 Queue 的方法之外，还支持 insert, remove 和 examine 操作，由于 Deque 是双向的，所以可以对队列的头和尾都进行操作，它同时也支持两组格式，一组是抛出异常的实现；另外一组是返回值的实现 (没有则返回 null)。共 12 个方法如下：\nThrows exception(head) Special value(head) Throws exception(tail) Special value(tail) Insert addFirst(e) offerFirst(e) addLast(e) Remove removeFirst() pollFirst() removeLast() Examine getFirst() peekFirst() getLast() "},{"id":122,"href":"/posts/java-collection/ch03/","title":"Ch03-Java Collection 之 LinkedList","section":"Blog","content":"LinkedList 同时实现了 List 接口和 Deque 接口，也就是说它既可以看作一个顺序容器，又可以看作一个队列 (Queue)，同时又可以看作一个栈 (Stack)。所以当需要使用栈或者队列时，可以考虑使用 LinkedList。\n1. 底层数据结构 # 每个 ArrayList 都有一个容量 (capacity)，表示底层数组的实际大小，容器内存储元素的个数不能多于当前容量。当向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小。Java 泛型只是编译器提供的语法糖，所以这里的数组是一个 Object 数组，以便能够容纳任何类型的对象。\nLinkedList 基于双向链表实现，因此决定了所有跟下标相关的操作都是线性时间，而在首段或者末尾删除元素只需要常数时间。为追求效率 LinkedList 没有实现同步 (synchronized)，如果需要多个线程并发访问，可以先采用 Collections.synchronizedList() 方法对其进行包装。\n"},{"id":123,"href":"/posts/java-collection/ch02/","title":"Ch02-Java Collection 之 ArrayList","section":"Blog","content":"ArrayList 实现了 List 接口，是顺序容器，即元素存放的数据与放进去的顺序相同，允许放入 null 元素，底层通过数组实现。除该类未实现同步外，其余跟 Vector 大致相同。\n1. 底层数据结构 # ArrayList 基于数组实现，它有一个容量 (capacity) 表示底层数组的实际大小，容器内存储元素的个数不能多于当前容量。当向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小。Java 泛型只是编译器提供的语法糖，所以这里的数组是一个 Object 数组，以便能够容纳任何类型的对象。\nsize(), isEmpty(), get(), set() 方法均能在常数时间内完成，add() 方法的时间开销跟插入位置有关，addAll() 方法的时间开销跟添加元素的个数成正比。其余方法大都是线性时间。为追求效率，ArrayList 没有实现同步 (synchronized)，如果需要多个线程并发访问，用户可以手动同步，也可使用 Vector 替代。\n2. 自动扩容 # 每当向数组中添加元素时，都要去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的需求。\n数组扩容通过一个公开的方法 ensureCapacity(int minCapacity) 来实现。在实际添加大量元素前，也可以使用 ensureCapacity 来手动增加 ArrayList 实例的容量，以减少递增式再分配的数量。数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的 1.5 倍。\n"},{"id":124,"href":"/posts/java-collection/ch01/","title":"Ch01-Java 之 Collection 和 Map","section":"Blog","content":"\n集合类 说明 TreeSet 基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。 HashSet 基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。 LinkedHashSet 具有 HashSet 的查找效率，且内部使用双向链表维护元素的插入顺序。 ArrayList 基于动态数组实现，支持随机访问。 Vector 和 ArrayList 类似，但它是线程安全的。 LinkedList 基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 PriorityQueue 基于堆结构实现，可以用它来实现优先队列。 TreeMap 基于红黑树实现。 HashMap 基于哈希表实现。 HashTable 和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。 LinkedHashMap 使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用 (LRU) 顺序。 "},{"id":125,"href":"/posts/java-basic/ch04/","title":"Ch04-Java 之 synchronized","section":"Blog","content":"synchronized 是 Java 中的一个关键字，主要用于解决 Java 中常见的并发问题（原子性，可见性，有序性）。\n1. 常见用法 # 修饰对象 被修饰后的名称 作用范围 作用对象 代码块 同步语句块 大括号{}括起来的代码 调用这个代码块的对象 方法 同步方法 整个方法 调用这个方法的对象 静态的方法 整个静态方法 该类的所有对象 类 synchronized 后面括号括起来的部分 该类的所有对象 2. 实现原理 # 编译器会在修饰的代码块前面和后面显式添加 monitorenter，monitorexit。这两个字节码便实际完成加锁和释放锁的功能，这一切主要还是由 Monitor 提供。\n2.1 monitor 介绍 # Monitor 被翻译为监视器或管程，每个 Java 对象都可以关联一个 Monitor 对象，如果使用 synchronized 给对象上锁（重量级）之后，该对象头的 Mark Word 中就被设置指向 Monitor 对象的指针。如果不加 synchronized 的对象不会关联监视器\n2.2 monitor 工作原理 # 对于一个 synchronized 修饰的方法 (代码块) 来说：\n当多个线程同时访问该方法，那么这些线程会先被放进_EntryList 队列，此时线程处于 blocked 状态 当一个线程获取到了对象的 monitor 后，那么就可以进入 running 状态，执行方法块，此时，ObjectMonitor 对象的_owner 指向当前线程，_count 加 1 表示当前对象锁被一个线程获取。 当 running 状态的线程调用 wait() 方法，那么当前线程释放 monitor 对象，进入 waiting 状态，ObjectMonitor 对象的_owner 变为 null，_count 减 1，同时线程进入_WaitSet 队列，直到有线程调用 notify() 方法唤醒该线程，则该线程进入_EntryList 队列，竞争到锁再进入_owner 区。 如果当前线程执行完毕，那么也释放 monitor 对象，ObjectMonitor 对象的_owner 变为 null，_count 减 1。 因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。\n庆幸的是在 Java 1.6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化最终提升显著，Java 1.6 之后，为了减少获得锁和释放锁所带来的性能消耗，引入了锁升级的概念。\n3. 锁状态 # 锁状态 说明 无锁 无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 偏向锁 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。 轻量级锁 锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。 重量级锁 会对其他线程阻塞 锁可以从偏向锁升级到轻量级锁，再升级的重量级锁。但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。\n锁升级主要体现在 Java 的对象头中有个 MarkWord 字段，它无法手动配置，Java 会自行控制何时选择使用何种锁状态。每一个线程在准备获取共享资源时：\n检查 MarkWord 里面是不是放的自己的 ThreadId；如果是，表示当前线程是处于“偏向锁”。跳过轻量级锁直接执行同步体。 如果 MarkWord 不是自己的 ThreadId，锁升级，这时候，用 CAS 来执行切换，新的线程根据 MarkWord 里面现有的 ThreadId，通知之前线程暂停，之前线程将 Markword 的内容置为空。 两个线程都把对象的 HashCode 复制到自己新建的用于存储锁的记录空间，接着开始通过 CAS 操作，把共享对象的 MarKword 的内容修改为自己新建的记录空间的地址的方式竞争 MarkWord。 第 3 步中成功执行 CAS 的获得资源，失败的则进入自旋。 自旋的线程在自旋过程中，成功获得资源 (即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于轻量级锁的状态，如果自旋失败。 进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己。 4. 锁优化 # 锁状态 说明 自旋锁 当一个线程尝试获取某个锁时，如果该锁已被其他线程占用，就一直循环检测锁是否被释放，而不是进入线程挂起或睡眠状态。 适应性自旋锁 JDK 1.6 引入，自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 锁消除 为了保证数据的完整性，在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM 检测到不可能存在共享数据竞争，JVM 便会对这些同步锁进行锁消除。 锁粗化 将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。主要是为了解决一系列的连续加锁解锁操作，导致不必要的性能损耗而引入的优化方案。 5. 参考文献 # synchronized 原理 6. 附件 # ObjectMonitor() { _header = NULL; _count = 0; // 记录个数，size(_WaitSet) + size(_EntryList) _waiters = 0, _recursions = 0; // 记录重入次数 _object = NULL; _owner = NULL; // 指向持有 ObjectMonitor 对象的线程 _WaitSet = NULL; // 保存 ObjectWaiter 对象列表 _WaitSetLock = 0; _Responsible = NULL; _succ = NULL; _cxq = NULL; FreeNext = NULL; _EntryList = NULL; // 处于等待锁 block 状态的线程，会被加入到该列表 _SpinFreq = 0; _SpinClock = 0; OwnerIsThread = 0; } // 每个等待锁的线程都会被封装成 ObjectWaiter 对象 class ObjectWaiter : public StackObj { public: enum TStates { TS_UNDEF, TS_READY, TS_RUN, TS_WAIT, TS_ENTER, TS_CXQ } ; enum Sorted { PREPEND, APPEND, SORTED } ; ObjectWaiter * volatile _next; ObjectWaiter * volatile _prev; //等待的线程 Thread* _thread; jlong _notifier_tid; ParkEvent * _event; volatile int _notified ; volatile TStates TState ; Sorted _Sorted ; // List placement disposition bool _active ; // Contention monitoring is enabled }; "},{"id":126,"href":"/posts/java-basic/ch03/","title":"Ch03-Java 之 各种机制","section":"Blog","content":" 1. 泛型机制 # 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。\n2. 注解机制 # 注解是 JDK1.5 版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它主要的作用有以下四方面：\n生成文档，通过代码里标识的元数据生成 javadoc 文档。 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理，例如使用反射注入实例。 3. 异常机制 # 4. 反射机制 # RTTI（Run-Time Type Identification）运行时类型识别。利用反射技术可以对一个类进行解剖，把个个组成部分映射成一个个对象（比如成员变量，方法，构造方法等都可以映射成对象）。\n5. SPI 机制 # SPI（Service Provider Interface），是 JDK 内置的一种 服务提供发现机制，可以用来启用框架扩展和替换组件，主要是被框架的开发人员使用。Java 中 SPI 机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是解耦。\n"},{"id":127,"href":"/posts/java-basic/ch02/","title":"Ch02-Java 之 Q/A","section":"Blog","content":" 1. a==b和a.equals(b)有什么区别？ # 如果 a 和 b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。#\n2. a.hashCode() 有什么用？与 a.equals(b) 有什么关系？ # hashCode() 方法是相应对象整型的 hash 值，根据 Java 规范，两个使用 equals() 方法来判断相等的对象，其 hash code 必须相等。\n3. final、finalize 和 finally 的不同之处？ # 条目 说明 final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变。 finalize 允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。 finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常。 4. String、StringBuffer 与 StringBuilder 的区别 # 可变和适用范围。String 对象是不可变的，而 StringBuffer 和 StringBuilder 是可变字符序列。每次对 String 的操作相当于生成一个新的 String 对象，而对 StringBuffer 和 StringBuilder 的操作是对对象本身的操作，而不会生成新的对象，所以对于频繁改变内容的字符串避免使用 String，因为频繁的生成对象将会对系统性能产生影响。 线程安全。String 由于有 final 修饰，是 immutable 的，安全性是简单而纯粹的。StringBuilder 和 StringBuffer 的区别在于 StringBuilder 不保证同步，也就是说如果需要线程安全需要使用 StringBuffer，不需要同步的 StringBuilder 效率更高。 5. 移位运算符 # 运算符 说明 \u0026lt;\u0026lt; 左移运算符，x \u0026laquo; 1，相当于 x 乘以 2 (不溢出的情况下),低位补 0 \u0026gt;\u0026gt; 带符号右移，x \u0026raquo; 1，相当于 x 除以 2，正数高位补 0，负数高位补 1 \u0026gt;\u0026gt;\u0026gt; 无符号右移，忽略符号位，空位都以 0 补齐 6. 如何理解 Java 中的泛型是伪泛型？泛型中类型擦除？ # Java 泛型这个特性是从 JDK 1.5 才开始加入的，因此为了兼容之前的版本，Java 泛型的实现采取了“伪泛型”的策略，即 Java 在语法上支持泛型，但是在编译阶段会进行所谓的“类型擦除”（Type Erasure），将所有的泛型表示（尖括号中的内容）都替换为具体的类型（其对应的原生态类型），就像完全没有泛型一样。\n消除类型参数声明，即删除\u0026lt;\u0026gt;及其包围的部分。 根据类型参数的上下界推断并替换所有的类型参数为原生态类型：如果类型参数是无限制通配符或没有上下界限定则替换为 Object，如果存在上下界限定则根据子类替换原则取类型参数的最左边限定类型（即父类）。 为了保证类型安全，必要时插入强制类型转换代码。 自动产生“桥接方法”以保证擦除类型后的代码仍然具有泛型的“多态性”。 "},{"id":128,"href":"/posts/java-basic/ch01/","title":"Ch01-Java 之 基础","section":"Blog","content":" 1.数据类型 # 基本类型 所占大小 (bit) 最小范围 最大范围 包装类型 boolean 1 Boolean byte 8 -128 127 Byte char 16 0 65535 Char short 16 -32768 32767 Short int 32 -2147483648 2147483647 Integer float 32 3.402823e+38 1.401298e-45 Float long 64 -9223372036854774808 9223372036854774807 Long double 64 1.797693e+308 4.9000000e-324 Double 基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。\n2. 缓存池 # new Integer(123) 与 Integer.valueOf(123) 的区别在于：\nnew Integer(123) 每次都会新建一个对象 nteger.valueOf(123) 会使用缓存池中的对象，多次调用会取得同一个对象的引用。 3. 访问权限修饰符 # Java 的访问控制是停留在编译层的，也就是它不会在 .class 文件中留下任何的痕迹，只在编译的时候进行访问控制的检查。其实，通过反射的手段，是可以访问任何包下任何类中的成员。\n修饰符 同一个类 同一个包 子类 任何地方 public yes yes yes yes protected yes yes yes default yes yes private yes "},{"id":129,"href":"/posts/software-engineering/ch03/","title":"Ch03-equals","section":"Blog","content":"equals() 或者 ==\n特性 说明 代码示例 自反性 对于任何非 null 的引用值 x, x.equals(x) 必须返回 true x.equals(x) // true 对称性 对于任何非 null 的引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 必须返回 true x.equals(y) == y.equals(x) // true 传递性 对于任何非 null 的引用值 x,y 和 z，如果 x.equals(y) 返回 true，并且 y.equals(z) 返回 true，那么 x.equals(z) 返回 true x.equals(y) \u0026amp;\u0026amp; y.equals(z) =\u0026gt; x.equals(z) // true 一致性 对于任何非 null 的引用值 x 和 y，只要 equals 的比较操作在对象中所用的信息没有被修改，多次调用 x.equals(y) 就会一致地返回 true，或者一致地返回 false x.equals(y) == x.equals(y) // true, false 与 null 的比较 x.equals(null) 结果为 false x.equals(null); // false "},{"id":130,"href":"/posts/software-engineering/ch02/","title":"Ch02-重写与重载","section":"Blog","content":"重写与重载\n1. 重写 (Override) # 存在于继承体系中，指子类实现了一个与父类在方法声明上完全相同的一个方法。\n为了满足里式替换原则，重写有以下两个限制：\n子类方法的访问权限必须大于等于父类方法； 子类方法的返回类型必须是父类方法返回类型或为其子类型。（一般会用 override 注解修饰） 2. 重载 (Overload) # 存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同。应该注意的是，返回值不同，其它都相同不算是重载。\n"},{"id":131,"href":"/posts/software-engineering/ch01/","title":"Ch01-面向对象","section":"Blog","content":"面向对象有三大特性，封装、继承、多态。\n1. 封装 # 利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体。数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外接口使之与外部发生联系。用户无需知道对象内部的细节，但可以通过对象对外提供的接口来访问该对象。\n减少耦合：可以独立地开发、测试、优化、使用、理解和修改 减轻维护的负担：可以更容易被程序员理解，并且在调试的时候可以不影响其他模块 有效地调节性能：可以通过剖析确定哪些模块影响了系统的性能 提高软件的可重用性 降低了构建大型系统的风险：即使整个系统不可用，但是这些独立的模块却有可能是可用的 2. 继承 # 继承实现了 IS-A 关系，例如 Cat 和 Animal 就是一种 IS-A 关系，因此 Cat 可以继承自 Animal，从而获得 Animal 非 private 的属性和方法。 继承应该遵循里氏替换原则，子类对象必须能够替换掉所有父类对象。 Cat 可以当做 Animal 来使用，也就是说可以使用 Animal 引用 Cat 对象。父类引用指向子类对象称为 向上转型。\n3. 多态 # 多态分为编译时多态和运行时多态：\n编译时多态：主要指方法的重载 运行时多态：指程序中定义的对象引用所指向的具体类型在运行期间才确定 "},{"id":132,"href":"/about/","title":"About Me","section":"Introduction","content":" 联系方式 # 姓名: acronymor 手机: 176\u0026hellip; 邮箱: acronymor@acronymor.com 毕业院校: 工作经历 # 经历1 # 深信服科技股份有限公司（ 2019年04月 ~ 2022年03月 ）\n大数据开发工程师\n个人经历吧啦吧啦….\n经历2 # 深圳今日头条科技有限公司（ 2022年04月 ~ 2022年11月 ）\n后端开发工程师\n个人经历吧啦吧啦….\n个人技能 # Java/C++/Python/Shell Spark/Flink Hadoop/HBase/Elasticsearch 个人爱好 # 摄影 羽毛球 乒乓球 "},{"id":133,"href":"/contact/","title":"Contact","section":"Introduction","content":"Follow me, @acronymor\n"}]